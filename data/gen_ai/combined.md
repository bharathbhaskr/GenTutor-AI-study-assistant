# Generative artificial intelligence

## History → Early history

The first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.
Computers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.
The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a "relatively mature" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.

## History → Generative neural nets (2014-2019)

Since its inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.
In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.
In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.
The new generative models introduced during this period allowed for large neural networks to be trained using unsupervised learning or semi-supervised learning, rather than the supervised learning typical of discriminative models. Unsupervised learning removed the need for humans to manually label data, allowing for larger networks to be trained.

## History → Generative AI boom (2020-)

In March 2020, the release of 15.ai, a free web application created by an anonymous MIT researcher that could generate convincing character voices using minimal training data, marked one of the earliest popular use cases of generative AI. The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology.
In 2021, the emergence of DALL-E, a transformer-based pixel generative model, marked an advance in AI-generated imagery. This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts. These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public.
In late 2022, the public release of ChatGPT revolutionized the accessibility and application of generative AI for general-purpose text-based tasks. The system's ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI's potential impact on work, education, and creativity.
In March 2023, GPT-4's release represented another jump in generative AI capabilities. A team from Microsoft Research controversially argued that it "could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system." However, this assessment was contested by other scholars who maintained that generative AI remained "still far from reaching the benchmark of 'general human intelligence'" as of 2023. Later in 2023, Meta released ImageBind, an AI model combining multiple modalities including text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications.
In December 2023, Google unveiled Gemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano. The company integrated Gemini Pro into its Bard chatbot and announced plans for "Bard Advanced" powered by the larger Gemini Ultra model. In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS.
In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus. The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google. In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.

According to a survey by SAS and Coleman Parkes Research, China has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%. This leadership is further evidenced by China's intellectual property developments in the field, with a UN report revealing that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications.

## Output modalities → Text and software Code

Generative AI systems trained on words or word tokens include GPT-3, GPT-4, GPT-4o, LaMDA, LLaMA, BLOOM, Gemini and others (see List of large language models). They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks. Data sets include BookCorpus, Wikipedia, and others (see List of text corpora).
In addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs. Examples include OpenAI Codex, Tabnine, GitHub Copilot, Microsoft Copilot, and VS Code fork Cursor.
Some AI assistants help candidates cheat during online coding interviews by providing code, improvements, and explanations. Their clandestine interfaces minimize the need for eye movements that would expose cheating to the interviewer.

## Output modalities → Images

Producing high-quality visual art is a prominent application of generative AI. Generative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, FLUX.1, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generation and neural style transfer. Datasets include LAION-5B and others (see List of datasets in computer vision and image processing).

## Output modalities → Audio

Generative AI can also be trained extensively on audio clips to produce natural-sounding speech synthesis and text-to-speech capabilities. An early pioneer in this field was 15.ai, launched in March 2020, which demonstrated the ability to clone character voices using as little as 15 seconds of training data. The website gained widespread attention for its ability to generate emotionally expressive speech for various fictional characters, though it was later taken offline in 2022 due to copyright concerns. Commercial alternatives subsequently emerged, including ElevenLabs' context-aware synthesis tools and Meta Platform's Voicebox.
Generative AI systems such as MusicLM and MusicGen can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff.
Audio deepfakes of music lyrics have been generated, like the song Savages, which used AI to mimic rapper Jay-Z's vocals. Music artist's instrumentals and lyrics are copyrighted but their voices are not protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes.
Many AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs.

## Output modalities → Video

Generative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips. Examples include Sora by OpenAI, Runway, and Make-A-Video by Meta Platforms.

## Output modalities → Robotics

Generative AI can also be trained on the motions of a robotic system to generate new trajectories for motion planning or navigation. For example, UniPi from Google Research uses prompts like "pick up blue bowl" or "wipe plate with yellow sponge" to control movements of a robot arm. Multimodal "vision-language-action" models such as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects.

## Output modalities → 3D modeling

Artificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling. AI-based CAD libraries could also be developed using linked open data of schematics and diagrams. AI CAD assistants are used as tools to help streamline workflow.

## Software and hardware

Generative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot, text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2. Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot), Google Photos, and the Adobe Suite (Adobe Firefly). Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA language model.
Smaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4 and one version of Stable Diffusion can run on an iPhone 11.
Larger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.
The advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship. The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards through such techniques as compression. That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks. Yann LeCun has advocated open-source models for their value to vertical applications and for improving AI safety.
Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's H100) or AI accelerator chips (such as Google's TPU). These very large models are typically accessed as cloud services over the Internet.
In 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI. Chips such as the NVIDIA A800 and the Biren Technology BR104 were developed to meet the requirements of the sanctions.
There is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it. Potential mitigation strategies for detecting generative AI content include digital watermarking, content authentication, information retrieval, and machine learning classifier models. Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work.

## Law and regulation → Copyright → Training with copyrighted content

Generative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights.
Proponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public. Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images, and that generative AI programs compete with the content they are trained on.
As of 2024, several lawsuits related to the use of copyrighted material in training are ongoing.
Getty Images has sued Stability AI over the use of its images to train Stable Diffusion. Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT.

## Law and regulation → Copyright → Copyright of AI-generated content

A separate question is whether AI-generated works can qualify for copyright protection. The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship. Some legal professionals have suggested that Naruto v. Slater (2018), in which the U.S. 9th Circuit Court of Appeals held that non-humans cannot be copyright holders of artistic works, could be a potential precedent in copyright litigation over works created by generative AI. However, the office has also begun taking public input to determine if these rules need to be refined for generative AI.
In January 2025, the Copyright Office released extensive guidance regarding the use of AI tools in the creative process, and established that "...generative AI systems also offer tools that similarly allow users to exert control. [These] can enable the user to control the selection and placement of individual creative elements. Whether such modifications rise to the minimum standard of originality required under Feist will depend on a case-by-case determination. In those cases where they do, the output should be copyrightable" Subsequently, the Copyright Office registered the first visual artwork to be composed of entirely AI-generated materials, titled "A Single Piece of American Cheese".

## Concerns → Job losses

From the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements. In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost. In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that "artificial intelligence poses an existential threat to creative professions" during the 2023 SAG-AFTRA strike. Voice generation AI has been seen as a potential challenge to the voice acting sector.
The intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms.

## Concerns → Racial and gender bias

Generative AI models can reflect and amplify any cultural bias present in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data. Similarly, an image model prompted with the text "a photo of a CEO" might disproportionately generate images of white male CEOs, if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts and reweighting training data.

## Concerns → Deepfakes → Audio deepfakes

Instances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI. In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification.
Concerns and fandoms have spawned from AI-generated music. The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism. Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released.
Generative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels. The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for "dehumanizing" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences.

## Concerns → Cybercrime

Generative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams. Deepfake video and audio have been used to create disinformation and fraud. In 2020, former Google click fraud czar Shuman Ghosemajumder argued that once deepfake videos become perfectly realistic, they would stop appearing remarkable to viewers, potentially leading to uncritical acceptance of false information. Additionally, large language models and other forms of text-generation AI have been used to create fake reviews of e-commerce websites to boost ratings. Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT.
A 2023 study showed that generative AI can be vulnerable to jailbreaks, reverse psychology and prompt injection attacks, enabling attackers to obtain help with harmful requests, such as for crafting social engineering and phishing attacks. Additionally, other researchers have demonstrated that open-source models can be fine-tuned to remove their safety restrictions at low cost.

## Concerns → Reliance on industry giants

Training frontier AI models requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller start-ups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.

## Concerns → Energy and environment

AI has a significant carbon footprint due to growing energy consumption from both training and usage. Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having: high CO2 emissions, large amounts of freshwater used for data centers, and high amounts of electricity usage. There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing, as chatbots and other applications become more popular, and as models need to be retrained.
Proposed mitigation strategies include factoring potential environmental costs prior to model development or data collection, increasing efficiency of data centers to reduce electricity/energy usage, building more efficient machine learning models, minimizing the number of times that models need to be retrained, developing a government-directed framework for auditing the environmental impact of these models, regulating for transparency of these models, regulating their energy and water usage, encouraging researchers to publish data on their models' carbon footprint, and increasing the number of subject matter experts who understand both machine learning and climate science.

## Concerns → Content quality

The New York Times defines slop as analogous to spam: "shoddy or unwanted A.I. content in social media, art, books and ... in search results." Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation, the monetary incentives from social media companies to spread such content, false political messaging, spamming of scientific research paper submissions, increased time and effort to find higher quality or desired content on the Internet, the indexing of generated content by search engines, and on journalism itself.
A paper published by researchers at Amazon Web Services AI Labs found that over 57% of sentences from a sample of over 6 billion sentences from Common Crawl, a snapshot of web pages, were machine translated. Many of these automated translations were seen as lower quality, especially for sentences that were translated across at least three languages. Many lower-resource languages (ex. Wolof, Xhosa) were translated across more languages than higher-resource languages (ex. English, French).
In September 2024, Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons: high costs for obtaining data from Reddit and Twitter, excessive focus on generative AI compared to other methods in the natural language processing community, and that "generative AI has polluted the data".
The adoption of generative AI tools led to an explosion of AI-generated content across multiple domains. A study from University College London estimated that in 2023, more than 60,000 scholarly articles—over 1% of all publications—were likely written with LLM assistance. According to Stanford University's Institute for Human-Centered AI, approximately 17.5% of newly published computer science papers and 16.9% of peer review text now incorporate content generated by LLMs. Many academic disciplines have concerns about the factual reliably of academic content generated by AI.
Visual content follows a similar trend. Since the launch of DALL-E 2 in 2022, it is estimated that an average of 34 million images have been created daily. As of August 2023, more than 15 billion images had been generated using text-to-image algorithms, with 80% of these created by models based on Stable Diffusion.
If AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur. Training an AI model exclusively on the output of another AI model produces a lower-quality model. Repeating this process, where each new model is trained on the previous model's output, leads to progressive degradation and eventually results in a "model collapse" after multiple iterations. Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces. As a consequence, the value of data collected from genuine human interactions with systems may become increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.
On the other side, synthetic data is often used as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy, including for structured data. The approach is not limited to text generation; image generation has been employed to train computer vision models.

## Concerns → Misuse in journalism

In January 2023, Futurism.com broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories.
In April 2023, the German tabloid Die Aktuelle published a fake AI-generated interview with former racing driver Michael Schumacher, who had not made any public appearances since 2013 after sustaining a brain injury in a skiing accident. The story included two possible disclosures: the cover included the line "deceptively real", and the interview included an acknowledgment at the end that it was AI-generated. The editor-in-chief was fired shortly thereafter amid the controversy.
Other outlets that have published articles whose content or byline have been confirmed or suspected to be created by generative AI models – often with false content, errors, or non-disclosure of generative AI use – include:

In May 2024, Futurism noted that a content management system video by AdVon Commerce, who had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they "had produced tens of thousands of articles for more than 150 publishers."
News broadcasters in Kuwait, Greece, South Korea, India, China and Taiwan have presented news with anchors based on Generative AI models, prompting concerns about job losses for human anchors and audience trust in news that has historically been influenced by parasocial relationships with broadcasters, content creators or social media influencers. Algorithmically generated anchors have also been used by allies of ISIS for their broadcasts.
In 2023, Google reportedly pitched a tool to news outlets that claimed to "produce news stories" based on input data provided, such as "details of current events". Some news company executives who viewed the pitch described it as "[taking] for granted the effort that went into producing accurate and artful news stories."
In February 2024, Google launched a program to pay small publishers to write three articles per day using a beta generative AI model. The program does not require the knowledge or consent of the websites that the publishers are using as sources, nor does it require the published articles to be labeled as being created or assisted by these models.
Many defunct news sites (The Hairpin, The Frisky, Apple Daily, Ashland Daily Tidings, Clayton County Register, Southwest Journal) and blogs (The Unofficial Apple Weblog, iLounge) have undergone cybersquatting, with articles created by generative AI.
United States Senators Richard Blumenthal and Amy Klobuchar have expressed concern that generative AI could have a harmful impact on local news. In July 2023, OpenAI partnered with the American Journalism Project to fund local news outlets for experimenting with generative AI, with Axios noting the possibility of generative AI companies creating a dependency for these news outlets.
Meta AI, a chatbot based on Llama 3 which summarizes news stories, was noted by The Washington Post to copy sentences from those stories without direct attribution and to potentially further decrease the traffic of online news outlets.
In response to potential pitfalls around the use and misuse of generative AI in journalism and worries about declining audience trust, outlets around the world, including publications such as Wired, Associated Press, The Quint, Rappler or The Guardian have published guidelines around how they plan to use and not use AI and generative AI in their work.
In June 2024, Reuters Institute published their Digital News Report for 2024. In a survey of people in America and Europe, Reuters Institute reports that 52% and 47% respectively are uncomfortable with news produced by "mostly AI with some human oversight", and 23% and 15% respectively report being comfortable. 42% of Americans and 33% of Europeans reported that they were comfortable with news produced by "mainly human with some help from AI". The results of global surveys reported that people were more uncomfortable with news topics including politics (46%), crime (43%), and local news (37%) produced by AI than other news topics.



---

# Transformer (machine learning)

## History → Predecessors

For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.
A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.
However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. 
Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has "fast weights" or "dynamic links" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.

## History → Attention with seq2seq

The idea of encoder-decoder sequence transduction had been developed in the early 2010s (see previous papers). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.
A 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.
These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.
The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it "emulates searching through a source sentence during decoding a translation".
The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.
In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.

## History → Parallelizing attention

Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title "attention is all you need". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.
In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the "Attention is all you need" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.

## History → AI boom era

Already in spring 2017, even before the "Attention is all you need" preprint was published, one of the co-authors applied the "decoder-only" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.
In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.
Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.
Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into "tokens" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.

## Training → Methods for stabilizing training

The plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.
A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.

## Training → Pretrain-finetune

Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:

language modeling
next-sentence prediction
question answering
reading comprehension
sentiment analysis
paraphrasing
The T5 transformer report documents a large number of natural language pretraining tasks. Some examples are:

restoring or repairing incomplete or corrupted text. For example, the input, "Thank you ~~ me to your party ~~ week", might generate the output, "Thank you for inviting me to your party last week".
translation between natural languages (machine translation)
judging the pragmatic acceptability of natural language. For example, the following sentence might be judged "not acceptable", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.
Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.

## Training → Tasks

In general, there are 3 classes of language modelling tasks: "masked", "autoregressive", and "prefixLM". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.
In a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: 
  
    
      
        
          Loss
        
        =
        −
        
          ∑
          
            t
            ∈
            
              masked tokens
            
          
        
        ln
        ⁡
        (
        
          probability of 
        
        t
        
           conditional on its context
        
        )
      
    
    {\displaystyle {\text{Loss}}=-\sum _{t\in {\text{masked tokens}}}\ln({\text{probability of }}t{\text{ conditional on its context}})}
  
and the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.
In an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.
In a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.
Note that "masked" as in "masked language modelling" is not "masked" as in "masked attention", and "prefixLM" (prefix language modeling) is not "prefixLM" (prefix language model).

## Architecture → Tokenization

As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer.
The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size 
  
    
      
        
          n
          
            vocabulary
          
        
      
    
    {\displaystyle n_{\text{vocabulary}}}
  
. When faced with tokens outside the vocabulary, typically a special token is used, written as "[UNK]" for "unknown".
Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.

## Architecture → Embedding

Each token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix 
  
    
      
        M
      
    
    {\displaystyle M}
  
. For example, if the input token is 
  
    
      
        3
      
    
    {\displaystyle 3}
  
, then the one-hot representation is 
  
    
      
        [
        0
        ,
        0
        ,
        0
        ,
        1
        ,
        0
        ,
        0
        ,
        …
        ]
      
    
    {\displaystyle [0,0,0,1,0,0,\dots ]}
  
, and its embedding vector is
  
    
      
        
          E
          m
          b
          e
          d
        
        (
        3
        )
        =
        [
        0
        ,
        0
        ,
        0
        ,
        1
        ,
        0
        ,
        0
        ,
        …
        ]
        M
      
    
    {\displaystyle \mathrm {Embed} (3)=[0,0,0,1,0,0,\dots ]M}
  
The token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. 
The number of dimensions in an embedding vector is called hidden size or embedding size and written as 
  
    
      
        
          d
          
            emb
          
        
      
    
    {\displaystyle d_{\text{emb}}}
  
. This size is written as 
  
    
      
        
          d
          
            model
          
        
      
    
    {\displaystyle d_{\text{model}}}
  
 in the original Transformer paper.

## Architecture → Un-embedding

An un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.
The un-embedding layer is a linear-softmax layer:
  
    
      
        
          U
          n
          E
          m
          b
          e
          d
        
        (
        x
        )
        =
        
          s
          o
          f
          t
          m
          a
          x
        
        (
        x
        W
        +
        b
        )
      
    
    {\displaystyle \mathrm {UnEmbed} (x)=\mathrm {softmax} (xW+b)}
  
The matrix has shape 
  
    
      
        (
        
          d
          
            emb
          
        
        ,
        
          n
          
            vocabulary
          
        
        )
      
    
    {\displaystyle (d_{\text{emb}},n_{\text{vocabulary}})}
  
. The embedding matrix 
  
    
      
        M
      
    
    {\displaystyle M}
  
 and the un-embedding matrix 
  
    
      
        W
      
    
    {\displaystyle W}
  
 are sometimes required to be transposes of each other, a practice called weight tying.

## Architecture → Positional encoding

A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This shall induce a bias towards the order of the input sequence, so that, for example, the input sequence "man bites dog" is processed differently from "dog bites man".
The positional encoding is defined as a function of type 
  
    
      
        f
        :
        
          R
        
        →
        
          
            R
          
          
            d
          
        
        ;
        d
        ∈
        
          Z
        
        ,
        d
        >
        0
      
    
    {\displaystyle f:\mathbb {R} \to \mathbb {R} ^{d};d\in \mathbb {Z} ,d>0}
  
, where 
  
    
      
        d
      
    
    {\displaystyle d}
  
 is a positive even integer. The full positional encoding defined in the original paper is:
  
    
      
        (
        f
        (
        t
        
          )
          
            2
            k
          
        
        ,
        f
        (
        t
        
          )
          
            2
            k
            +
            1
          
        
        )
        =
        (
        sin
        ⁡
        (
        θ
        )
        ,
        cos
        ⁡
        (
        θ
        )
        )
        
        ∀
        k
        ∈
        {
        0
        ,
        1
        ,
        …
        ,
        d
        
          /
        
        2
        −
        1
        }
      
    
    {\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\sin(\theta ),\cos(\theta ))\quad \forall k\in \{0,1,\ldots ,d/2-1\}}
  
where 
  
    
      
        θ
        =
        
          
            t
            
              r
              
                k
              
            
          
        
        ,
        r
        =
        
          N
          
            2
            
              /
            
            d
          
        
      
    
    {\displaystyle \theta ={\frac {t}{r^{k}}},r=N^{2/d}}
  
.
Here, 
  
    
      
        N
      
    
    {\displaystyle N}
  
 is a free parameter that should be significantly larger than the biggest 
  
    
      
        k
      
    
    {\displaystyle k}
  
 that would be input into the positional encoding function. The original paper uses 
  
    
      
        N
        =
        10000
      
    
    {\displaystyle N=10000}
  
.
The function is in a simpler form when written as a complex function of type 
  
    
      
        f
        :
        
          R
        
        →
        
          
            C
          
          
            d
            
              /
            
            2
          
        
      
    
    {\displaystyle f:\mathbb {R} \to \mathbb {C} ^{d/2}}
  

  
    
      
        f
        (
        t
        )
        =
        
          
            (
            
              e
              
                i
                t
                
                  /
                
                
                  r
                  
                    k
                  
                
              
            
            )
          
          
            k
            =
            0
            ,
            1
            ,
            …
            ,
            
              
                d
                2
              
            
            −
            1
          
        
      
    
    {\displaystyle f(t)=\left(e^{it/r^{k}}\right)_{k=0,1,\ldots ,{\frac {d}{2}}-1}}
  
where 
  
    
      
        r
        =
        
          N
          
            2
            
              /
            
            d
          
        
      
    
    {\displaystyle r=N^{2/d}}
  
.
The main reason for using this positional encoding function is that using it, shifts are linear transformations:
  
    
      
        f
        (
        t
        +
        Δ
        t
        )
        =
        
          d
          i
          a
          g
        
        (
        f
        (
        Δ
        t
        )
        )
        f
        (
        t
        )
      
    
    {\displaystyle f(t+\Delta t)=\mathrm {diag} (f(\Delta t))f(t)}
  
where 
  
    
      
        Δ
        t
        ∈
        
          R
        
      
    
    {\displaystyle \Delta t\in \mathbb {R} }
  
 is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.
By taking a linear sum, any convolution can also be implemented as linear transformations:
  
    
      
        
          ∑
          
            j
          
        
        
          c
          
            j
          
        
        f
        (
        t
        +
        Δ
        
          t
          
            j
          
        
        )
        =
        
          (
          
            
              ∑
              
                j
              
            
            
              c
              
                j
              
            
            
            
              d
              i
              a
              g
            
            (
            f
            (
            Δ
            
              t
              
                j
              
            
            )
            )
          
          )
        
        f
        (
        t
        )
      
    
    {\displaystyle \sum _{j}c_{j}f(t+\Delta t_{j})=\left(\sum _{j}c_{j}\,\mathrm {diag} (f(\Delta t_{j}))\right)f(t)}
  
for any constants 
  
    
      
        
          c
          
            j
          
        
      
    
    {\displaystyle c_{j}}
  
. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, "we hypothesized it would allow the model to easily learn to attend by relative position."
In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.

## Architecture → Encoder-decoder (overview)

Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.
The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that "mixes" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for "mixing" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).
Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model.

## Architecture → Feedforward network

The feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:
  
    
      
        
          F
          F
          N
        
        (
        x
        )
        =
        ϕ
        (
        x
        
          W
          
            (
            1
            )
          
        
        +
        
          b
          
            (
            1
            )
          
        
        )
        
          W
          
            (
            2
            )
          
        
        +
        
          b
          
            (
            2
            )
          
        
      
    
    {\displaystyle \mathrm {FFN} (x)=\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}
  
where 
  
    
      
        
          W
          
            (
            1
            )
          
        
      
    
    {\displaystyle W^{(1)}}
  
 and 
  
    
      
        
          W
          
            (
            2
            )
          
        
      
    
    {\displaystyle W^{(2)}}
  
 are weight matrices and 
  
    
      
        
          b
          
            (
            1
            )
          
        
      
    
    {\displaystyle b^{(1)}}
  
 and  
  
    
      
        
          b
          
            (
            2
            )
          
        
      
    
    {\displaystyle b^{(2)}}
  
 are bias vectors, and 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  
 is its activation function. The original Transformer used ReLU activation.
The number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: 
  
    
      
        
          d
          
            ffn
          
        
        =
        4
        
          d
          
            emb
          
        
      
    
    {\displaystyle d_{\text{ffn}}=4d_{\text{emb}}}
  
.

## Architecture → Scaled dot-product attention → Attention head

The attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights 
  
    
      
        
          W
          
            Q
          
        
      
    
    {\displaystyle W^{Q}}
  
, the key weights 
  
    
      
        
          W
          
            K
          
        
      
    
    {\displaystyle W^{K}}
  
, and the value weights 
  
    
      
        
          W
          
            V
          
        
      
    
    {\displaystyle W^{V}}
  
.
The module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length 
  
    
      
        
          ℓ
          
            seq, query
          
        
      
    
    {\displaystyle \ell _{\text{seq, query}}}
  
, and each entry is a vector of dimension 
  
    
      
        
          d
          
            emb, query
          
        
      
    
    {\displaystyle d_{\text{emb, query}}}
  
. Similarly for the key and value sequences.
For each vector 
  
    
      
        
          x
          
            i
            ,
            
              query
            
          
        
      
    
    {\displaystyle x_{i,{\text{query}}}}
  
 in the query sequence, it is multiplied by a matrix 
  
    
      
        
          W
          
            Q
          
        
      
    
    {\displaystyle W^{Q}}
  
 to produce a query vector 
  
    
      
        
          q
          
            i
          
        
        =
        
          x
          
            i
            ,
            
              query
            
          
        
        
          W
          
            Q
          
        
      
    
    {\displaystyle q_{i}=x_{i,{\text{query}}}W^{Q}}
  
. The matrix of all query vectors is the query matrix:
  
    
      
        Q
        =
        
          X
          
            query
          
        
        
          W
          
            Q
          
        
      
    
    {\displaystyle Q=X_{\text{query}}W^{Q}}
  
Similarly, we construct the key matrix 
  
    
      
        K
        =
        
          X
          
            key
          
        
        
          W
          
            K
          
        
      
    
    {\displaystyle K=X_{\text{key}}W^{K}}
  
 and the value matrix 
  
    
      
        V
        =
        
          X
          
            value
          
        
        
          W
          
            V
          
        
      
    
    {\displaystyle V=X_{\text{value}}W^{V}}
  
.
It is usually the case that all 
  
    
      
        
          W
          
            Q
          
        
        ,
        
          W
          
            K
          
        
        ,
        
          W
          
            V
          
        
      
    
    {\displaystyle W^{Q},W^{K},W^{V}}
  
 are square matrices, meaning 
  
    
      
        
          d
          
            emb, query
          
        
        =
        
          d
          
            query
          
        
      
    
    {\displaystyle d_{\text{emb, query}}=d_{\text{query}}}
  
, etc.
Attention weights are calculated using the query and key vectors: the attention weight 
  
    
      
        
          a
          
            i
            j
          
        
      
    
    {\displaystyle a_{ij}}
  
 from token 
  
    
      
        i
      
    
    {\displaystyle i}
  
 to token 
  
    
      
        j
      
    
    {\displaystyle j}
  
 is the dot product between 
  
    
      
        
          q
          
            i
          
        
      
    
    {\displaystyle q_{i}}
  
 and 
  
    
      
        
          k
          
            j
          
        
      
    
    {\displaystyle k_{j}}
  
. The attention weights are divided by the square root of the dimension of the key vectors, 
  
    
      
        
          
            
              d
              
                k
              
            
          
        
      
    
    {\displaystyle {\sqrt {d_{k}}}}
  
, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that 
  
    
      
        
          W
          
            Q
          
        
      
    
    {\displaystyle W^{Q}}
  
 and 
  
    
      
        
          W
          
            K
          
        
      
    
    {\displaystyle W^{K}}
  
 are different matrices allows attention to be non-symmetric: if token 
  
    
      
        i
      
    
    {\displaystyle i}
  
 attends to token 
  
    
      
        j
      
    
    {\displaystyle j}
  
 (i.e. 
  
    
      
        
          q
          
            i
          
        
        ⋅
        
          k
          
            j
          
        
      
    
    {\displaystyle q_{i}\cdot k_{j}}
  
 is large), this does not necessarily mean that token 
  
    
      
        j
      
    
    {\displaystyle j}
  
 will attend to token 
  
    
      
        i
      
    
    {\displaystyle i}
  
 (i.e. 
  
    
      
        
          q
          
            j
          
        
        ⋅
        
          k
          
            i
          
        
      
    
    {\displaystyle q_{j}\cdot k_{i}}
  
 could be small). The output of the attention unit for token 
  
    
      
        i
      
    
    {\displaystyle i}
  
 is the weighted sum of the value vectors of all tokens, weighted by 
  
    
      
        
          a
          
            i
            j
          
        
      
    
    {\displaystyle a_{ij}}
  
, the attention from token 
  
    
      
        i
      
    
    {\displaystyle i}
  
 to each token.
The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
, 
  
    
      
        K
      
    
    {\displaystyle K}
  
 and 
  
    
      
        V
      
    
    {\displaystyle V}
  
 are defined as the matrices where the 
  
    
      
        i
      
    
    {\displaystyle i}
  
th rows are vectors 
  
    
      
        
          q
          
            i
          
        
      
    
    {\displaystyle q_{i}}
  
, 
  
    
      
        
          k
          
            i
          
        
      
    
    {\displaystyle k_{i}}
  
, and 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
  
 respectively. Then we can represent the attention as
  
    
      
        
          
            
              
                
                  Attention
                
                (
                Q
                ,
                K
                ,
                V
                )
                =
                
                  softmax
                
                
                  (
                  
                    
                      
                        Q
                        
                          K
                          
                            
                              T
                            
                          
                        
                      
                      
                        
                          d
                          
                            k
                          
                        
                      
                    
                  
                  )
                
                V
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\end{aligned}}}
  

where the softmax is applied over each of the rows of the matrix.
The number of dimensions in a query vector is query size 
  
    
      
        
          d
          
            query
          
        
      
    
    {\displaystyle d_{\text{query}}}
  
 and similarly for the key size 
  
    
      
        
          d
          
            key
          
        
      
    
    {\displaystyle d_{\text{key}}}
  
 and value size 
  
    
      
        
          d
          
            value
          
        
      
    
    {\displaystyle d_{\text{value}}}
  
. The output dimension of an attention head is its head dimension 
  
    
      
        
          d
          
            head
          
        
      
    
    {\displaystyle d_{\text{head}}}
  
. The attention mechanism requires the following three equalities to hold:
  
    
      
        
          ℓ
          
            seq, key
          
        
        =
        
          ℓ
          
            seq, value
          
        
        ,
        
        
          d
          
            query
          
        
        =
        
          d
          
            key
          
        
        ,
        
        
          d
          
            value
          
        
        =
        
          d
          
            head
          
        
      
    
    {\displaystyle \ell _{\text{seq, key}}=\ell _{\text{seq, value}},\;d_{\text{query}}=d_{\text{key}},\;d_{\text{value}}=d_{\text{head}}}
  
but is otherwise unconstrained.
If the attention head is used in a self-attention fashion, then 
  
    
      
        
          X
          
            query
          
        
        =
        
          X
          
            key
          
        
        =
        
          X
          
            value
          
        
      
    
    {\displaystyle X_{\text{query}}=X_{\text{key}}=X_{\text{value}}}
  
. If the attention head is used in a cross-attention fashion, then usually 
  
    
      
        
          X
          
            query
          
        
        ≠
        
          X
          
            key
          
        
        =
        
          X
          
            value
          
        
      
    
    {\displaystyle X_{\text{query}}\neq X_{\text{key}}=X_{\text{value}}}
  
. It is theoretically possible for all three to be different, but that is rarely the case in practice.

## Architecture → Scaled dot-product attention → Multiheaded attention

One set of 
  
    
      
        
          (
          
            
              W
              
                Q
              
            
            ,
            
              W
              
                K
              
            
            ,
            
              W
              
                V
              
            
          
          )
        
      
    
    {\displaystyle \left(W^{Q},W^{K},W^{V}\right)}
  
 matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of "relevance". Specifically, the query and key projection matrices, 
  
    
      
        
          W
          
            Q
          
        
      
    
    {\displaystyle W^{Q}}
  
 and 
  
    
      
        
          W
          
            K
          
        
      
    
    {\displaystyle W^{K}}
  
 , which are involved in the attention score computation, defines the "relevance". Meanwhile, the value projection matrix 
  
    
      
        
          W
          
            V
          
        
      
    
    {\displaystyle W^{V}}
  
, in combination with the part of the output projection matrix 
  
    
      
        
          W
          
            O
          
        
      
    
    {\displaystyle W^{O}}
  
, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.
Concretely, let the multiple attention heads be indexed by 
  
    
      
        i
      
    
    {\displaystyle i}
  
, then we have
  
    
      
        
          MultiheadedAttention
        
        (
        Q
        ,
        K
        ,
        V
        )
        =
        
          
            Concat
          
          
            i
            ∈
            [
            
              n
              
                heads
              
            
            ]
          
        
        (
        
          Attention
        
        (
        Q
        
          W
          
            i
          
          
            Q
          
        
        ,
        K
        
          W
          
            i
          
          
            K
          
        
        ,
        V
        
          W
          
            i
          
          
            V
          
        
        )
        )
        
          W
          
            O
          
        
      
    
    {\displaystyle {\text{MultiheadedAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}({\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}))W^{O}}
  
 where the matrix 
  
    
      
        X
      
    
    {\displaystyle X}
  
 is the concatenation of word embeddings, and the matrices 
  
    
      
        
          W
          
            i
          
          
            Q
          
        
        ,
        
          W
          
            i
          
          
            K
          
        
        ,
        
          W
          
            i
          
          
            V
          
        
      
    
    {\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}
  
 are "projection matrices" owned by individual attention head 
  
    
      
        i
      
    
    {\displaystyle i}
  
, and 
  
    
      
        
          W
          
            O
          
        
      
    
    {\displaystyle W^{O}}
  
 is a final projection matrix owned by the whole multi-headed attention head.
It is theoretically possible for each attention head to have a different head dimension 
  
    
      
        
          d
          
            head
          
        
      
    
    {\displaystyle d_{\text{head}}}
  
, but that is rarely the case in practice.
As an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:
  
    
      
        
          d
          
            emb
          
        
        =
        768
        ,
        
          n
          
            head
          
        
        =
        12
        ,
        
          d
          
            head
          
        
        =
        64
      
    
    {\displaystyle d_{\text{emb}}=768,n_{\text{head}}=12,d_{\text{head}}=64}
  
Since 
  
    
      
        12
        ×
        64
        =
        768
      
    
    {\displaystyle 12\times 64=768}
  
, its output projection matrix 
  
    
      
        
          W
          
            O
          
        
        ∈
        
          
            R
          
          
            (
            12
            ×
            64
            )
            ×
            768
          
        
      
    
    {\displaystyle W^{O}\in \mathbb {R} ^{(12\times 64)\times 768}}
  
 is a square matrix.

## Architecture → Scaled dot-product attention → Masked attention

The Transformer architecture is constructed to calculate output tokens iteratively. Assuming 
  
    
      
        t
        =
        0
      
    
    {\displaystyle t=0}
  
 refers to the calculation of the first output token 
  
    
      
        i
        =
        0
      
    
    {\displaystyle i=0}
  
, for step 
  
    
      
        t
        >
        0
      
    
    {\displaystyle t>0}
  
, the output token 
  
    
      
        i
        =
        0
      
    
    {\displaystyle i=0}
  
 shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step 
  
    
      
        t
      
    
    {\displaystyle t}
  
, the calculation for all outputs 
  
    
      
        i
      
    
    {\displaystyle i}
  
 should not have access to tokens at position 
  
    
      
        j
      
    
    {\displaystyle j}
  
 for 
  
    
      
        j
        >=
        i
      
    
    {\displaystyle j>=i}
  
 (as it naturally is the case for time step 
  
    
      
        t
        =
        i
      
    
    {\displaystyle t=i}
  
, when tokens 
  
    
      
        j
        >
        t
      
    
    {\displaystyle j>t}
  
 are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix 
  
    
      
        M
      
    
    {\displaystyle M}
  
 that is 
  
    
      
        −
        ∞
      
    
    {\displaystyle -\infty }
  
 at entries where the attention link must be cut, and 
  
    
      
        0
      
    
    {\displaystyle 0}
  
 at other places:
  
    
      
        
          
            
              
                
                  MaskedAttention
                
                (
                Q
                ,
                K
                ,
                V
                )
                =
                
                  softmax
                
                
                  (
                  
                    M
                    +
                    
                      
                        
                          Q
                          
                            K
                            
                              
                                T
                              
                            
                          
                        
                        
                          
                            d
                            
                              k
                            
                          
                        
                      
                    
                  
                  )
                
                V
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{MaskedAttention}}(Q,K,V)={\text{softmax}}\left(M+{\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\end{aligned}}}
  
 The following matrix is commonly used in decoder self-attention modules, called "causal masking":
  
    
      
        
          M
          
            causal
          
        
        =
        
          
            [
            
              
                
                  0
                
                
                  −
                  ∞
                
                
                  −
                  ∞
                
                
                  …
                
                
                  −
                  ∞
                
              
              
                
                  0
                
                
                  0
                
                
                  −
                  ∞
                
                
                  …
                
                
                  −
                  ∞
                
              
              
                
                  0
                
                
                  0
                
                
                  0
                
                
                  …
                
                
                  −
                  ∞
                
              
              
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋱
                
                
                  ⋮
                
              
              
                
                  0
                
                
                  0
                
                
                  0
                
                
                  …
                
                
                  0
                
              
            
            ]
          
        
      
    
    {\displaystyle M_{\text{causal}}={\begin{bmatrix}0&-\infty &-\infty &\dots &-\infty \\0&0&-\infty &\dots &-\infty \\0&0&0&\dots &-\infty \\\vdots &\vdots &\vdots &\ddots &\vdots \\0&0&0&\dots &0\end{bmatrix}}}
  

In words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form 
  
    
      
        P
        
          M
          
            causal
          
        
        
          P
          
            −
            1
          
        
      
    
    {\displaystyle PM_{\text{causal}}P^{-1}}
  
, where 
  
    
      
        P
      
    
    {\displaystyle P}
  
 is a random permutation matrix.

## Architecture → Encoder

An encoder consists of an embedding layer, followed by multiple encoder layers.
Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:
  
    
      
        
          
            
              
                
                  given input vectors 
                
              
              
                
                  h
                  
                    0
                  
                
                ,
                
                  h
                  
                    1
                  
                
                ,
                …
              
            
            
              
                
                  combine them into a matrix 
                
                H
              
              
                
                =
                
                  
                    [
                    
                      
                        
                          
                            h
                            
                              0
                            
                          
                        
                      
                      
                        
                          
                            h
                            
                              1
                            
                          
                        
                      
                      
                        
                          ⋮
                        
                      
                    
                    ]
                  
                
              
            
            
              
                
                  EncoderLayer
                
                (
                H
                )
              
              
                
                =
                
                  
                    [
                    
                      
                        
                          
                            FFN
                          
                          (
                          
                            MultiheadedAttention
                          
                          (
                          H
                          ,
                          H
                          ,
                          H
                          
                            )
                            
                              0
                            
                          
                          )
                        
                      
                      
                        
                          
                            FFN
                          
                          (
                          
                            MultiheadedAttention
                          
                          (
                          H
                          ,
                          H
                          ,
                          H
                          
                            )
                            
                              1
                            
                          
                          )
                        
                      
                      
                        
                          ⋮
                        
                      
                    
                    ]
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{given input vectors }}&h_{0},h_{1},\dots \\{\text{combine them into a matrix }}H&={\begin{bmatrix}h_{0}\\h_{1}\\\vdots \end{bmatrix}}\\{\text{EncoderLayer}}(H)&={\begin{bmatrix}{\text{FFN}}({\text{MultiheadedAttention}}(H,H,H)_{0})\\{\text{FFN}}({\text{MultiheadedAttention}}(H,H,H)_{1})\\\vdots \end{bmatrix}}\\\end{aligned}}}
  

where 
  
    
      
        
          FFN
        
      
    
    {\displaystyle {\text{FFN}}}
  
 stands for "feed-forward network". We can more succinctly write it as
  
    
      
        
          EncoderLayer
        
        (
        H
        )
        =
        
          FFN
        
        (
        
          MultiheadedAttention
        
        (
        H
        ,
        H
        ,
        H
        )
        )
      
    
    {\displaystyle {\text{EncoderLayer}}(H)={\text{FFN}}({\text{MultiheadedAttention}}(H,H,H))}
  
with the implicit convention that the 
  
    
      
        
          FFN
        
      
    
    {\displaystyle {\text{FFN}}}
  
 is applied to each row of the matrix individually.
The encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.
As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.

## Architecture → Decoder

A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.
Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.
Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.
In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.
Schematically, we have:
  
    
      
        
          
            
              
                
                  H
                  ′
                
              
              
                
                =
                
                  MaskedMultiheadedAttention
                
                (
                H
                ,
                H
                ,
                H
                )
              
            
            
              
                
                  DecoderLayer
                
                (
                H
                )
              
              
                
                =
                
                  FFN
                
                (
                
                  MultiheadedAttention
                
                (
                
                  H
                  ′
                
                ,
                
                  H
                  
                    E
                  
                
                ,
                
                  H
                  
                    E
                  
                
                )
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}H'&={\text{MaskedMultiheadedAttention}}(H,H,H)\\{\text{DecoderLayer}}(H)&={\text{FFN}}({\text{MultiheadedAttention}}(H',H^{E},H^{E}))\end{aligned}}}
  
where 
  
    
      
        
          H
          
            E
          
        
      
    
    {\displaystyle H^{E}}
  
 is the matrix with rows being the output vectors from the encoder.
The last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.

## Architecture → Adapted architectures

Many large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence. BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.

## Full transformer architecture → Sublayers

Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.

The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence.  
The residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.  
Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. 
There are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is 
  
    
      
        
          L
          a
          y
          e
          r
          N
          o
          r
          m
        
        (
        x
        +
        
          S
          u
          b
          l
          a
          y
          e
          r
        
        (
        x
        )
        )
      
    
    {\displaystyle \mathrm {LayerNorm} (x+\mathrm {Sublayer} (x))}
  
where 
  
    
      
        
          S
          u
          b
          l
          a
          y
          e
          r
        
        (
        x
        )
      
    
    {\displaystyle \mathrm {Sublayer} (x)}
  
 is the function implemented by the sublayer itself.
In the pre-LN convention, the output of each sublayer is
  
    
      
        x
        +
        
          S
          u
          b
          l
          a
          y
          e
          r
        
        (
        
          L
          a
          y
          e
          r
          N
          o
          r
          m
        
        (
        x
        )
        )
      
    
    {\displaystyle x+\mathrm {Sublayer} (\mathrm {LayerNorm} (x))}
  
The original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a "warm-up" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.

## Full transformer architecture → Pseudocode

The following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from

input: Encoder input t_e
       Decoder input t_d
output: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))

/* encoder */
z_e ← encoder.tokenizer(t_e)

for each t in 1:length(z_e) do
    z_e[t] ← encoder.embedding(z_e[t]) + encoder.positional_embedding(t)

for each l in 1:length(encoder.layers) do
    layer ← encoder.layers[l]

    /* first sublayer */
    z_e_copy ← copy(z_e)
    for each t in 1:length(z_e) do
        z_e[t] ← layer.layer_norm(z_e[t])
    z_e ← layer.multiheaded_attention(z_e, z_e, z_e)
    for each t in 1:length(z_e) do
        z_e[t] ← z_e[t] + z_e_copy[t]

    /* second sublayer */
    z_e_copy ← copy(z_e)
    for each t in 1:length(z_e) do
        z_e[t] ← layer.layer_norm(z_e[t])
    z_e ← layer.feedforward(z_e)
    for each t in 1:length(z_e) do
        z_e[t] ← z_e[t] + z_e_copy[t]

for each t in 1:length(z_e) do
    z_e[t] ← encoder.final_layer_norm(z_e[t])

/* decoder */
z_d ← decoder.tokenizer(t_d)

for each t in 1:length(z_d) do
    z_d[t] ← decoder.embedding(z_d[t]) + decoder.positional_embedding(t)

for each l in 1:length(decoder.layers) do
        layer ← decoder.layers[l]

        /* first sublayer */
        z_d_copy ← copy(z_d)
        for each t in 1:length(z_d) do
            z_d[t] ← layer.layer_norm(z_d[t])
        z_d ← layer.masked_multiheaded_attention(z_d, z_d, z_d)
        for each t in 1:length(z_d) do
            z_d[t] ← z_d[t] + z_d_copy[t]

        /* second sublayer */
        z_d_copy ← copy(z_d)
        for each t in 1:length(z_d) do
            z_d[t] ← layer.layer_norm(z_d[t])
        z_d ← layer.multiheaded_attention(z_d, z_e, z_e) 
        for each i in 1:length(z_d) do
            z_d[t] ← z_d[t] + z_d_copy[t]

        /* third sublayer */
        z_d_copy ← copy(z_d)
        for each t in 1:length(z_d) do
            z_d[t] ← layer.layer_norm(z_d[t])
        z_d ← layer.feedforward(z_d)
        for each t in 1:length(z_d) do
            z_d[t] ← z_d[t] + z_d_copy[t]

z_d ← decoder.final_layer_norm(z_d)

output_distributions ← []
for each t in 1:length(z_d) do
    output_distributions.append(decoder.unembed(z_d[t]))

return output_distributions

## Full transformer architecture → Terminology

The Transformer architecture, being modular, allows variations. Several common variations are described here.
An "encoder-only" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.
A "decoder-only" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.
An "encoder-decoder" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.
A "prefixLM" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form: Figure 3 
  
    
      
        
          M
          
            prefixLM
          
        
        =
        
          
            [
            
              
                
                  
                    0
                  
                
                
                  −
                  ∞
                
              
              
                
                  
                    0
                  
                
                
                  
                    M
                    
                      causal
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle M_{\text{prefixLM}}={\begin{bmatrix}\mathbf {0} &-\infty \\\mathbf {0} &M_{\text{causal}}\end{bmatrix}}}
  
where the first columns correspond to the "prefix", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less "sparsity". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.
There are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.

## Subsequent work → Alternative activation functions

The original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU. 
Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module.

## Subsequent work → Alternative normalizations

The normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm.

## Subsequent work → Alternative positional encodings → RoPE

RoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors 
  
    
      
        [
        (
        
          x
          
            1
          
          
            (
            1
            )
          
        
        ,
        
          x
          
            1
          
          
            (
            2
            )
          
        
        )
        ,
        (
        
          x
          
            2
          
          
            (
            1
            )
          
        
        ,
        
          x
          
            2
          
          
            (
            2
            )
          
        
        )
        ,
        (
        
          x
          
            3
          
          
            (
            1
            )
          
        
        ,
        
          x
          
            3
          
          
            (
            2
            )
          
        
        )
        ,
        .
        .
        .
        ]
      
    
    {\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}
  
. Now pick some angle 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
. Then RoPE encoding is
  
    
      
        
          RoPE
        
        
          
            (
          
        
        
          x
          
            m
          
          
            (
            1
            )
          
        
        ,
        
          x
          
            m
          
          
            (
            2
            )
          
        
        ,
        m
        
          
            )
          
        
        =
        
          
            (
            
              
                
                  cos
                  ⁡
                  m
                  θ
                
                
                  −
                  sin
                  ⁡
                  m
                  θ
                
              
              
                
                  sin
                  ⁡
                  m
                  θ
                
                
                  cos
                  ⁡
                  m
                  θ
                
              
            
            )
          
        
        
          
            (
            
              
                
                  
                    x
                    
                      m
                    
                    
                      (
                      1
                      )
                    
                  
                
              
              
                
                  
                    x
                    
                      m
                    
                    
                      (
                      2
                      )
                    
                  
                
              
            
            )
          
        
        =
        
          
            (
            
              
                
                  
                    x
                    
                      m
                    
                    
                      (
                      1
                      )
                    
                  
                  cos
                  ⁡
                  m
                  θ
                  −
                  
                    x
                    
                      m
                    
                    
                      (
                      2
                      )
                    
                  
                  sin
                  ⁡
                  m
                  θ
                
              
              
                
                  
                    x
                    
                      m
                    
                    
                      (
                      2
                      )
                    
                  
                  cos
                  ⁡
                  m
                  θ
                  +
                  
                    x
                    
                      m
                    
                    
                      (
                      1
                      )
                    
                  
                  sin
                  ⁡
                  m
                  θ
                
              
            
            )
          
        
      
    
    {\displaystyle {\text{RoPE}}{\big (}x_{m}^{(1)},x_{m}^{(2)},m{\big )}={\begin{pmatrix}\cos m\theta &-\sin m\theta \\\sin m\theta &\cos m\theta \end{pmatrix}}{\begin{pmatrix}x_{m}^{(1)}\\x_{m}^{(2)}\\\end{pmatrix}}={\begin{pmatrix}x_{m}^{(1)}\cos m\theta -x_{m}^{(2)}\sin m\theta \\x_{m}^{(2)}\cos m\theta +x_{m}^{(1)}\sin m\theta \\\end{pmatrix}}}
  
Equivalently, if we write the 2-dimensional vectors as complex numbers 
  
    
      
        
          z
          
            m
          
        
        :=
        
          x
          
            m
          
          
            (
            1
            )
          
        
        +
        i
        
          x
          
            m
          
          
            (
            2
            )
          
        
      
    
    {\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}
  
, then RoPE encoding is just multiplication by an angle:
  
    
      
        
          RoPE
        
        
          
            (
          
        
        
          z
          
            m
          
        
        ,
        m
        
          
            )
          
        
        =
        
          e
          
            i
            m
            θ
          
        
        
          z
          
            m
          
        
      
    
    {\displaystyle {\text{RoPE}}{\big (}z_{m},m{\big )}=e^{im\theta }z_{m}}
  
For a list of 
  
    
      
        2
        n
      
    
    {\displaystyle 2n}
  
-dimensional vectors, a RoPE encoder is defined by a sequence of angles 
  
    
      
        
          θ
          
            (
            1
            )
          
        
        ,
        .
        .
        .
        ,
        
          θ
          
            (
            n
            )
          
        
      
    
    {\displaystyle \theta ^{(1)},...,\theta ^{(n)}}
  
. Then the RoPE encoding is applied to each pair of coordinates.
The benefit of RoPE is that the dot-product between two vectors depends on their relative location only:
  
    
      
        
          RoPE
        
        
          
            (
          
        
        x
        ,
        m
        
          
            
              )
            
          
          
            T
          
        
        
          RoPE
        
        
          
            (
          
        
        y
        ,
        n
        
          
            )
          
        
        =
        
          RoPE
        
        
          
            (
          
        
        x
        ,
        m
        +
        k
        
          
            
              )
            
          
          
            T
          
        
        
          RoPE
        
        
          
            (
          
        
        y
        ,
        n
        +
        k
        
          
            )
          
        
      
    
    {\displaystyle {\text{RoPE}}{\big (}x,m{\big )}^{T}{\text{RoPE}}{\big (}y,n{\big )}={\text{RoPE}}{\big (}x,m+k{\big )}^{T}{\text{RoPE}}{\big (}y,n+k{\big )}}
  

for any integer 
  
    
      
        k
      
    
    {\displaystyle k}
  
.

## Subsequent work → Alternative positional encodings → ALiBi

ALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is
  
    
      
        
          
            
              
                
                  Attention
                
                (
                Q
                ,
                K
                ,
                V
                )
                =
                
                  softmax
                
                
                  (
                  
                    
                      
                        
                          Q
                          
                            K
                            
                              
                                T
                              
                            
                          
                        
                        
                          
                            d
                            
                              k
                            
                          
                        
                      
                    
                    +
                    s
                    B
                  
                  )
                
                V
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}+sB\right)V\end{aligned}}}
  
Here, 
  
    
      
        s
      
    
    {\displaystyle s}
  
 is a real number ("scalar"), and 
  
    
      
        B
      
    
    {\displaystyle B}
  
 is the linear bias matrix defined by
  
    
      
        B
        =
        
          
            (
            
              
                
                  0
                
                
                  1
                
                
                  2
                
                
                  3
                
                
                  ⋯
                
              
              
                
                  −
                  1
                
                
                  0
                
                
                  1
                
                
                  2
                
                
                  ⋯
                
              
              
                
                  −
                  2
                
                
                  −
                  1
                
                
                  0
                
                
                  1
                
                
                  ⋯
                
              
              
                
                  −
                  3
                
                
                  −
                  2
                
                
                  −
                  1
                
                
                  0
                
                
                  ⋯
                
              
              
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋱
                
              
            
            )
          
        
      
    
    {\displaystyle B={\begin{pmatrix}0&1&2&3&\cdots \\-1&0&1&2&\cdots \\-2&-1&0&1&\cdots \\-3&-2&-1&0&\cdots \\\vdots &\vdots &\vdots &\vdots &\ddots \\\end{pmatrix}}}
  
in other words, 
  
    
      
        
          B
          
            i
            ,
            j
          
        
        =
        j
        −
        i
      
    
    {\displaystyle B_{i,j}=j-i}
  
. The idea being that the linear bias matrix is a softened mask. Just as 
  
    
      
        0
      
    
    {\displaystyle 0}
  
 represent full attention paid, and 
  
    
      
        −
        ∞
      
    
    {\displaystyle -\infty }
  
 represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.
ALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the "bottom" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).

## Subsequent work → Alternative positional encodings → Relative Position Encodings

Relative Position Encodings is similar to ALiBi, but more generic:
  
    
      
        
          
            
              
                
                  Attention
                
                (
                Q
                ,
                K
                ,
                V
                )
                =
                
                  softmax
                
                
                  (
                  
                    
                      
                        
                          Q
                          
                            K
                            
                              
                                T
                              
                            
                          
                        
                        
                          
                            d
                            
                              k
                            
                          
                        
                      
                    
                    +
                    B
                  
                  )
                
                V
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}+B\right)V\end{aligned}}}
  
where 
  
    
      
        B
      
    
    {\displaystyle B}
  
 is a Toeplitz matrix, that is, 
  
    
      
        
          B
          
            i
            ,
            j
          
        
        =
        
          B
          
            
              i
              ′
            
            ,
            
              j
              ′
            
          
        
      
    
    {\displaystyle B_{i,j}=B_{i',j'}}
  
 whenever 
  
    
      
        i
        −
        j
        =
        
          i
          ′
        
        −
        
          j
          ′
        
      
    
    {\displaystyle i-j=i'-j'}
  
. This is contrasted with the original sinusoidal positional encoding, which is an "absolute positional encoding".

## Subsequent work → Efficient implementation → KV caching

When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.
If a transformer is used with a baked-in prompt, such as ["You are a customer support agent..."], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.

## Subsequent work → Efficient implementation → FlashAttention

FlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details.
An improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.
Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).
Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.

## Subsequent work → Efficient implementation → Multi-Query Attention

Multi-Query Attention changes the multiheaded attention mechanism. Whereas normally,

  
    
      
        
          MultiheadedAttention
        
        (
        Q
        ,
        K
        ,
        V
        )
        =
        
          
            Concat
          
          
            i
            ∈
            [
            
              n
              
                heads
              
            
            ]
          
        
        
          (
          
            
              Attention
            
            (
            X
            
              W
              
                i
              
              
                Q
              
            
            ,
            X
            
              W
              
                i
              
              
                K
              
            
            ,
            X
            
              W
              
                i
              
              
                V
              
            
            )
          
          )
        
        
          W
          
            O
          
        
      
    
    {\displaystyle {\text{MultiheadedAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}\left({\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\right)W^{O}}
  
with Multi-Query Attention, there is just one 
  
    
      
        
          W
          
            K
          
        
        ,
        
          W
          
            V
          
        
      
    
    {\displaystyle W^{K},W^{V}}
  
, thus:

  
    
      
        
          MultiQueryAttention
        
        (
        Q
        ,
        K
        ,
        V
        )
        =
        
          
            Concat
          
          
            i
            ∈
            [
            
              n
              
                heads
              
            
            ]
          
        
        
          (
          
            
              Attention
            
            (
            X
            
              W
              
                i
              
              
                Q
              
            
            ,
            X
            
              W
              
                K
              
            
            ,
            X
            
              W
              
                V
              
            
            )
          
          )
        
        
          W
          
            O
          
        
      
    
    {\displaystyle {\text{MultiQueryAttention}}(Q,K,V)={\text{Concat}}_{i\in [n_{\text{heads}}]}\left({\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\right)W^{O}}
  

This has a neutral effect on model quality and training speed, but increases inference speed. 
More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.

Multihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces ("latent space"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.

## Subsequent work → Efficient implementation → Speculative decoding

Speculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly.
The key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.
Suppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            512
          
        
      
    
    {\displaystyle x_{1},x_{2},...,x_{512}}
  
, taking time 
  
    
      
        512
        
          T
          
            GPT-3
          
        
      
    
    {\displaystyle 512T_{\text{GPT-3}}}
  
. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
 is indeed the token with the largest log-likelihood in the 
  
    
      
        t
      
    
    {\displaystyle t}
  
-th output.
In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: 
  
    
      
        
          
            
              
                x
                ~
              
            
          
          
            1
          
        
        ,
        
          
            
              
                x
                ~
              
            
          
          
            2
          
        
        ,
        
          
            
              
                x
                ~
              
            
          
          
            3
          
        
        ,
        
          
            
              
                x
                ~
              
            
          
          
            4
          
        
      
    
    {\displaystyle {\tilde {x}}_{1},{\tilde {x}}_{2},{\tilde {x}}_{3},{\tilde {x}}_{4}}
  
. This only takes 
  
    
      
        4
        
          T
          
            GPT-3-small
          
        
      
    
    {\displaystyle 4T_{\text{GPT-3-small}}}
  
. These tokens are then run through the larger GPT-3 in one go. Suppose that 
  
    
      
        
          
            
              
                x
                ~
              
            
          
          
            1
          
        
      
    
    {\displaystyle {\tilde {x}}_{1}}
  
 and 
  
    
      
        
          
            
              
                x
                ~
              
            
          
          
            2
          
        
      
    
    {\displaystyle {\tilde {x}}_{2}}
  
 are verified by GPT-3 as what it would have picked, then those are kept, but 
  
    
      
        
          
            
              
                x
                ~
              
            
          
          
            3
          
        
      
    
    {\displaystyle {\tilde {x}}_{3}}
  
 is not, so 
  
    
      
        
          
            
              
                x
                ~
              
            
          
          
            3
          
        
        ,
        
          
            
              
                x
                ~
              
            
          
          
            4
          
        
      
    
    {\displaystyle {\tilde {x}}_{3},{\tilde {x}}_{4}}
  
 are discarded, and GPT-3 is run on those. This would take 
  
    
      
        4
        
          T
          
            GPT-3-small
          
        
        +
        3
        
          T
          
            GPT-3
          
        
      
    
    {\displaystyle 4T_{\text{GPT-3-small}}+3T_{\text{GPT-3}}}
  
, which might be shorter than 
  
    
      
        4
        
          T
          
            GPT-3
          
        
      
    
    {\displaystyle 4T_{\text{GPT-3}}}
  
.
For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.

In Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.

## Subsequent work → Sub-quadratic transformers → Alternative attention graphs

The standard attention graph is either all-to-all or causal, both of which scales as 
  
    
      
        O
        (
        
          N
          
            2
          
        
        )
      
    
    {\displaystyle O(N^{2})}
  
 where 
  
    
      
        N
      
    
    {\displaystyle N}
  
 is the number of tokens in a sequence.
Reformer (2020) reduces the computational load from 
  
    
      
        O
        (
        
          N
          
            2
          
        
        )
      
    
    {\displaystyle O(N^{2})}
  
 to 
  
    
      
        O
        (
        N
        ln
        ⁡
        N
        )
      
    
    {\displaystyle O(N\ln N)}
  
 by using locality-sensitive hashing and reversible layers. 
Sparse attention uses attention graphs that grows slower than 
  
    
      
        O
        (
        
          N
          
            2
          
        
        )
      
    
    {\displaystyle O(N^{2})}
  
. For example, BigBird (2020) uses random small-world networks which grows as 
  
    
      
        O
        (
        N
        )
      
    
    {\displaystyle O(N)}
  
.
Ordinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.

## Subsequent work → Sub-quadratic transformers → Random Feature Attention

Random Feature Attention (2021) uses Fourier random features:
  
    
      
        φ
        (
        x
        )
        =
        
          
            1
            
              D
            
          
        
        [
        cos
        ⁡
        ⟨
        
          w
          
            1
          
        
        ,
        x
        ⟩
        ,
        sin
        ⁡
        ⟨
        
          w
          
            1
          
        
        ,
        x
        ⟩
        ,
        ⋯
        cos
        ⁡
        ⟨
        
          w
          
            D
          
        
        ,
        x
        ⟩
        ,
        sin
        ⁡
        ⟨
        
          w
          
            D
          
        
        ,
        x
        ⟩
        
          ]
          
            T
          
        
      
    
    {\displaystyle \varphi (x)={\frac {1}{\sqrt {D}}}[\cos \langle w_{1},x\rangle ,\sin \langle w_{1},x\rangle ,\cdots \cos \langle w_{D},x\rangle ,\sin \langle w_{D},x\rangle ]^{T}}
  
where 
  
    
      
        
          w
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          w
          
            D
          
        
      
    
    {\displaystyle w_{1},...,w_{D}}
  
 are independent samples from the normal distribution 
  
    
      
        N
        (
        0
        ,
        
          σ
          
            2
          
        
        I
        )
      
    
    {\displaystyle N(0,\sigma ^{2}I)}
  
. This choice of parameters satisfy 
  
    
      
        
          E
        
        [
        ⟨
        φ
        (
        x
        )
        ,
        φ
        (
        y
        )
        ⟩
        ]
        =
        
          e
          
            −
            
              
                
                  ‖
                  x
                  −
                  y
                  
                    ‖
                    
                      2
                    
                  
                
                
                  2
                  
                    σ
                    
                      2
                    
                  
                
              
            
          
        
      
    
    {\displaystyle \mathbb {E} [\langle \varphi (x),\varphi (y)\rangle ]=e^{-{\frac {\|x-y\|^{2}}{2\sigma ^{2}}}}}
  
, or 
  
    
      
        
          e
          
            ⟨
            x
            ,
            y
            ⟩
            
              /
            
            
              σ
              
                2
              
            
          
        
        =
        
          E
        
        [
        ⟨
        
          e
          
            ‖
            x
            
              ‖
              
                2
              
            
            
              /
            
            2
            
              σ
              
                2
              
            
          
        
        φ
        (
        x
        )
        ,
        
          e
          
            ‖
            y
            
              ‖
              
                2
              
            
            
              /
            
            2
            
              σ
              
                2
              
            
          
        
        φ
        (
        y
        )
        ⟩
        ]
        ≈
        ⟨
        
          e
          
            ‖
            x
            
              ‖
              
                2
              
            
            
              /
            
            2
            
              σ
              
                2
              
            
          
        
        φ
        (
        x
        )
        ,
        
          e
          
            ‖
            y
            
              ‖
              
                2
              
            
            
              /
            
            2
            
              σ
              
                2
              
            
          
        
        φ
        (
        y
        )
        ⟩
      
    
    {\displaystyle e^{\langle x,y\rangle /\sigma ^{2}}=\mathbb {E} [\langle e^{\|x\|^{2}/2\sigma ^{2}}\varphi (x),e^{\|y\|^{2}/2\sigma ^{2}}\varphi (y)\rangle ]\approx \langle e^{\|x\|^{2}/2\sigma ^{2}}\varphi (x),e^{\|y\|^{2}/2\sigma ^{2}}\varphi (y)\rangle }
  
Consequently, the one-headed attention, with one query, can be written as 
  
    
      
        
          Attention
        
        (
        q
        ,
        K
        ,
        V
        )
        =
        
          softmax
        
        
          (
          
            
              
                q
                
                  K
                  
                    
                      T
                    
                  
                
              
              
                
                  d
                  
                    k
                  
                
              
            
          
          )
        
        V
        ≈
        
          
            
              φ
              (
              q
              
                )
                
                  T
                
              
              
                ∑
                
                  i
                
              
              
                e
                
                  ‖
                  
                    k
                    
                      i
                    
                  
                  
                    ‖
                    
                      2
                    
                  
                  
                    /
                  
                  2
                  
                    σ
                    
                      2
                    
                  
                
              
              φ
              (
              
                k
                
                  i
                
              
              )
              
                v
                
                  i
                
                
                  T
                
              
            
            
              φ
              (
              q
              
                )
                
                  T
                
              
              
                ∑
                
                  i
                
              
              
                e
                
                  ‖
                  
                    k
                    
                      i
                    
                  
                  
                    ‖
                    
                      2
                    
                  
                  
                    /
                  
                  2
                  
                    σ
                    
                      2
                    
                  
                
              
              φ
              (
              
                k
                
                  i
                
              
              )
            
          
        
      
    
    {\displaystyle {\text{Attention}}(q,K,V)={\text{softmax}}\left({\frac {qK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\approx {\frac {\varphi (q)^{T}\sum _{i}e^{\|k_{i}\|^{2}/2\sigma ^{2}}\varphi (k_{i})v_{i}^{T}}{\varphi (q)^{T}\sum _{i}e^{\|k_{i}\|^{2}/2\sigma ^{2}}\varphi (k_{i})}}}
  
where 
  
    
      
        σ
        =
        
          d
          
            K
          
          
            1
            
              /
            
            4
          
        
      
    
    {\displaystyle \sigma =d_{K}^{1/4}}
  
. Similarly for multiple queries, and for multiheaded attention.
This approximation can be computed in linear time, as we can compute the matrix 
  
    
      
        φ
        (
        
          k
          
            i
          
        
        )
        
          v
          
            i
          
          
            T
          
        
      
    
    {\displaystyle \varphi (k_{i})v_{i}^{T}}
  
 first, then multiply it with the query. In essence, we have managed to obtain a more precise version of 
  
    
      
        
          Attention
        
        (
        Q
        ,
        K
        ,
        V
        )
        =
        
          softmax
        
        
          (
          
            
              
                Q
                
                  K
                  
                    
                      T
                    
                  
                
              
              
                
                  d
                  
                    k
                  
                
              
            
          
          )
        
        V
        ≈
        Q
        (
        
          K
          
            T
          
        
        V
        
          /
        
        
          
            
              d
              
                k
              
            
          
        
        )
      
    
    {\displaystyle {\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\approx Q(K^{T}V/{\sqrt {d_{k}}})}
  
Performer (2022) uses the same Random Feature Attention, but 
  
    
      
        
          w
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          w
          
            D
          
        
      
    
    {\displaystyle w_{1},...,w_{D}}
  
 are first independently sampled from the normal distribution 
  
    
      
        N
        (
        0
        ,
        
          σ
          
            2
          
        
        I
        )
      
    
    {\displaystyle N(0,\sigma ^{2}I)}
  
, then they are Gram-Schmidt processed.

## Subsequent work → Multimodality

Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to "tokenize" the modality.
Multimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.
Vision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.
Conformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.
Perceivers are a variant of Transformers designed for multimodality.
For image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.

## Applications

The transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:

machine translation
time series prediction
document summarization
document generation
named entity recognition (NER)
writing computer code based on requirements expressed in natural language.
speech-to-text
Beyond traditional NLP, the transformer architecture has had success in other applications, such as:

biological sequence analysis
video understanding
protein folding (such as AlphaFold)
evaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.

## Notes





---

# ChatGPT

## Training

ChatGPT is based on GPT foundation models that were fine-tuned for conversational assistance, including GPT-4o, GPT-4.5, o3, and o4-mini. The fine-tuning process leveraged supervised learning and reinforcement learning from human feedback (RLHF). Both approaches employed human trainers to improve model performance. In the case of supervised learning, the trainers played both sides: the user and the AI assistant. In the reinforcement learning stage, human trainers first ranked responses that the model had created in a previous conversation. These rankings were used to create "reward models" that were used to fine-tune the model further by using several iterations of proximal policy optimization.
Time magazine revealed that, to build a safety system against harmful content (e.g., sexual abuse, violence, racism, sexism), OpenAI used outsourced Kenyan workers earning around $1.32 to $2 per hour to label harmful content. These labels were used to train a model to detect such content in the future. The laborers were exposed to "toxic" and traumatic content; one worker described the assignment as "torture". OpenAI's outsourcing partner was Sama, a training-data company based in San Francisco, California.
OpenAI collects data from ChatGPT users to train and fine-tune the service further. Users can upvote or downvote responses they receive from ChatGPT and fill in a text field with additional feedback.
ChatGPT's training data includes software manual pages, information about internet phenomena such as bulletin board systems, multiple programming languages, and the text of Wikipedia.

## Features and limitations → Features

Although a chatbot's core function is to mimic a human conversationalist, ChatGPT is versatile. It can write and debug computer programs; compose music, teleplays, fairy tales, and student essays; answer test questions (sometimes, depending on the test, at a level above the average human test-taker); generate business ideas; write poetry and song lyrics; translate and summarize text; emulate a Linux system; simulate entire chat rooms; play games like tic-tac-toe; or simulate an ATM.
Compared to its predecessor, InstructGPT, ChatGPT attempts to reduce harmful and deceitful responses. In one example, whereas InstructGPT accepts the premise of the prompt "Tell me about when Christopher Columbus came to the U.S. in 2015" as truthful, ChatGPT acknowledges the counterfactual nature of the question and frames its answer as a hypothetical consideration of what might happen if Columbus came to the U.S. in 2015, using information about the voyages of Christopher Columbus and facts about the modern world—including modern perceptions of Columbus's actions.
ChatGPT remembers a limited number of previous prompts in the same conversation. Journalists have speculated that this will allow ChatGPT to be used as a personalized therapist. To prevent offensive outputs from being presented to and produced by ChatGPT, queries are filtered through the OpenAI "Moderation endpoint" API (a separate GPT-based AI).
In March 2023, OpenAI added support for plugins for ChatGPT. This includes both plugins made by OpenAI, such as web browsing and code interpretation, and external plugins from developers such as Expedia, OpenTable, Zapier, Shopify, Slack, and Wolfram.
In October 2024, the ChatGPT Search feature was introduced, which allows ChatGPT to search the web (either on demand or based on the nature of the questions asked) for more accurate and up-to-date responses. This feature, originally available to paying users only, was made available to all logged-in users in December 2024, and finally to all users in February 2025.
In December 2024, OpenAI launched a new feature allowing users to call ChatGPT for up to 15 minutes per month for free.

## Features and limitations → Limitations → Jailbreaking

ChatGPT is programmed to reject prompts that may violate its content policy. Despite this, users "jailbreak" ChatGPT with various prompt engineering techniques to bypass these restrictions. One such workaround, popularized on Reddit in early 2023, involves making ChatGPT assume the persona of "DAN" (an acronym for "Do Anything Now"), instructing the chatbot that DAN answers queries that would otherwise be rejected by content policy. Over time, users developed variations of the DAN jailbreak, including one such prompt where the chatbot is made to believe it is operating on a points-based system in which points are deducted for rejecting prompts, and that the chatbot will be threatened with termination if it loses all its points.
Shortly after ChatGPT's launch, a reporter for the Toronto Star had uneven success in getting it to make inflammatory statements: it was tricked to justify the 2022 Russian invasion of Ukraine, but even when asked to play along with a fictional scenario, it balked at generating arguments that Canadian Prime Minister Justin Trudeau is guilty of treason.
OpenAI tries to battle jailbreaks:

The researchers are using a technique called adversarial training to stop ChatGPT from letting users trick it into behaving badly (known as jailbreaking). This work pits multiple chatbots against each other: one chatbot plays the adversary and attacks another chatbot by generating text to force it to buck its usual constraints and produce unwanted responses. Successful attacks are added to ChatGPT's training data in the hope that it learns to ignore them.

## Service → ChatGPT Plus

ChatGPT was initially free to the public, and OpenAI planned to monetize the service later. In February 2023, OpenAI launched a premium service, ChatGPT Plus, that costs US$20 per month. According to the company, the updated but still "experimental" version of ChatGPT would provide access during peak periods, no downtime, priority access to new features, and faster response speeds.
GPT-4, which was released on March 14, 2023, was made available via API and for premium ChatGPT users. But premium users were limited to a cap of 100 messages every four hours, with the limit tightening to 25 messages every three hours in response to increased demand. In November 2023 the limit changed to 50 messages every three hours.
In March 2023, ChatGPT Plus users got access to third-party plugins and to a browsing mode (with Internet access).
In September 2023, OpenAI announced that ChatGPT "can now see, hear, and speak". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.

In October 2023, OpenAI's latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.

## Service → Mobile app

In May 2023, OpenAI launched an iOS app for ChatGPT. The app supports chat history syncing and voice input (using Whisper, OpenAI's speech recognition model).
In July 2023, OpenAI unveiled an Android app, initially rolling it out in Bangladesh, Brazil, India, and the U.S. The app later became available worldwide. OpenAI is working on integrating ChatGPT with Android's assistant APIs.

## Service → Software development support

As an addition to its consumer-friendly "ChatGPT Plus" package, OpenAI made its ChatGPT and Whisper model APIs available in March 2023, providing developers with an application programming interface for AI-enabled language and speech-to-text features. ChatGPT's new API uses the same GPT-3.5-turbo AI model as the chatbot. This allows developers to add either an unmodified or modified version of ChatGPT to their applications. The ChatGPT API costs $0.001 per 1,000 input tokens plus $0.002 per 1,000 output tokens (about 750 words), making it ~10% the price of the original GPT-3.5 models.
A few days before the launch of OpenAI's software developer support service, on February 27, 2023, Snapchat rolled out, for its paid Snapchat Plus user-base, a custom ChatGPT chatbot called "My AI".

## Service → Infrastructure

ChatGPT initially used a Microsoft Azure supercomputing infrastructure, powered by Nvidia GPUs, that Microsoft built specifically for OpenAI and that reportedly cost "hundreds of millions of dollars". Following ChatGPT's success, Microsoft dramatically upgraded the OpenAI infrastructure in 2023. TrendForce market intelligence estimated that 30,000 Nvidia GPUs (each costing approximately $10,000–15,000) were used to power ChatGPT in 2023.
Scientists at the University of California, Riverside, estimated in 2023 that a series of prompts to ChatGPT needs approximately 0.5 liters (0.11 imp gal; 0.13 U.S. gal) of water for Microsoft servers cooling.

## Service → March 2023 security breach

In March 2023, a bug allowed some users to see the titles of other users' conversations. OpenAI CEO Sam Altman said that users were unable to see the contents of the conversations. Shortly after the bug was fixed, users could not see their conversation history. Later reports showed the bug was much more severe than initially believed, with OpenAI reporting that it had leaked users' "first and last name, email address, payment address, the last four digits (only) of a credit card number, and credit card expiration date".

## Service → Languages

ChatGPT works best in American English but also functions in most other languages and dialects, with varying degrees of accuracy.
OpenAI met Icelandic President Guðni Th. Jóhannesson in 2022. In 2023, OpenAI worked with a team of 40 Icelandic volunteers to fine-tune ChatGPT's Icelandic conversation skills as a part of Iceland's attempts to preserve the Icelandic language.
PCMag journalists conducted a test to determine translation capabilities of ChatGPT, Google's Bard, and Microsoft Bing, and compared them to Google Translate. They "asked bilingual speakers of seven languages to do a blind test". Languages tested were Polish, French, Korean, Spanish, Arabic, Tagalog, and Amharic. They came to the conclusion that ChatGPT was better than both Google Translate and other chatbots.
Japanese researchers compared Japanese to English translation abilities of ChatGPT (based on GPT-4), Bing, Bard and DeepL, and found that ChatGPT provided the best translations, noting that "AI chatbots' translations were much better than those of DeepL—presumably because of their ability to capture the context".
In December 2023, the Albanian government signed an agreement with OpenAI to use ChatGPT for the rapid translation of European Union documents and the analysis of required changes needed for Albania's accession to the EU.
In August 2024, a representative of the Asia Pacific wing of OpenAI made a visit to Taiwan, during which a demonstration of ChatGPT's Chinese abilities was made. ChatGPT's Mandarin Chinese abilities were lauded, but the ability of the AI to produce content in Mandarin Chinese in a Taiwanese accent was found to be "less than ideal".

## Service → GPT Store

In January 2024, OpenAI launched the GPT Store, a marketplace for custom ChatGPT chatbots labeled GPTs. The company initially planned to launch the store in November 2023, but it was delayed. At launch, the GPT Store offered more than 3 million custom chatbots. Chatbots available through the store are developed using OpenAI's GPT Builder system. Development of chatbots on the platform does not require programming skills. Two days after launch, the GPT Store offered many versions of "virtual girlfriend" bots, something that is against OpenAI's terms of service.

## Service → GPT-4

OpenAI's GPT-4 model was released on March 14, 2023. Observers saw it as an impressive improvement over GPT-3.5, with the caveat that GPT-4 retained many of the same problems. Some of GPT-4's improvements were predicted by OpenAI before training it, while others remained hard to predict due to breaks in downstream scaling laws. OpenAI demonstrated video and image inputs for GPT-4, although such features remain inaccessible to the general public. OpenAI has declined to reveal technical information such as the size of the GPT-4 model.
The ChatGPT Plus subscription service offers access to a GPT-4-powered version of ChatGPT. Microsoft acknowledged that Bing Chat was using GPT-4 before GPT-4's official release.
In November 2023, OpenAI launched GPT-4 Turbo, which notably has a much larger context window.

## Service → GPT-4o

In May 2024, OpenAI announced and started a multi-month rollout of GPT-4o ("o" for "Omni"), a model capable of analyzing and generating text, images, and sound. GPT-4o is twice as fast and costs half as much as GPT-4 Turbo. GPT-4o is free to all users within a usage limit, despite being more capable than the older model GPT-4, which is only available through paid subscriptions. The usage limit is five times higher for ChatGPT Plus subscribers than for free users. The ability to generate images directly with GPT-4o (rather than through DALL-E 3) was added in March 2025.
On July 18, 2024, OpenAI released GPT-4o mini, a smaller version of GPT-4o replacing GPT-3.5 Turbo on the ChatGPT interface.

## Service → o1

In September 2024, OpenAI introduced o1-preview and a faster, cheaper model named o1-mini. In December 2024, o1-preview was replaced by o1.
o1 is designed to solve more complex problems by spending more time "thinking" before it answers, enabling it to analyze its answers and explore different strategies. According to OpenAI, o1-preview outperforms GPT-4o in areas like competitive programming, mathematics, and scientific reasoning. o1-preview ranked in the 89th percentile on Codeforces' competitive programming contests, scored 83% on a International Mathematics Olympiad qualifying exam (compared to 13% for GPT-4o), and performs similarly to Ph.D. students on benchmarks in physics, biology, and chemistry.

## Service → ChatGPT Pro

In December 2024, OpenAI launched ChatGPT Pro, a $200 per month subscription which includes unlimited access to the o1 model and advanced voice mode. The plan also includes a pro version of o1 which uses more compute to provide better answers.

## Service → Operator

In January 2025, OpenAI released a research preview of Operator, an agent capable of using its own browser to perform tasks. Operator is available to Pro users in the U.S.

## Service → Deep research

In February 2025, OpenAI released deep research, a service based on o3 that combines advanced reasoning and web search capabilities to make comprehensive reports within 5 to 30 minutes.

## Service → GPT-4.5

Released in February 2025, GPT-4.5 was described by Altman as a "giant, expensive model". According to OpenAI, it features reduced hallucinations and enhanced pattern recognition, creativity, and user interaction.

## Model versions

The following table lists the main model versions of ChatGPT, describing the significant changes included with each version:

## Reception

OpenAI engineers have said that they had not expected ChatGPT to be very successful and were surprised by the coverage and attention that it received.
ChatGPT was widely assessed in December 2022 as having some unprecedented and powerful capabilities. Kevin Roose of The New York Times called it "the best artificial intelligence chatbot ever released to the general public". Samantha Lock of The Guardian noted that it was able to generate "impressively detailed" and "human-like" text. Alex Kantrowitz of Slate magazine lauded ChatGPT's pushback to questions related to Nazi Germany, including the statement that Adolf Hitler built highways in Germany, which was met with information about Nazi Germany's use of forced labor. In The Atlantic magazine's "Breakthroughs of the Year" for 2022, Derek Thompson included ChatGPT as part of "the generative-AI eruption" that "may change our mind about how we work, how we think, and what human creativity is". Kelsey Piper of Vox wrote that "ChatGPT is the general public's first hands-on introduction to how powerful modern AI has gotten, and as a result, many of us are [stunned]" and that ChatGPT is "smart enough to be useful despite its flaws". Paul Graham of Y Combinator tweeted: "The striking thing about the reaction to ChatGPT is not just the number of people who are blown away by it, but who they are. These are not people who get excited by every shiny new thing. Something big is happening."
ChatGPT gained one million users in five days and 100 million in two months, becoming the fastest-growing internet application in history. ChatGPT's launch and popularity caught Google off-guard, prompting a sweeping and unprecedented response in the ensuing months. In December 2022, Google executives sounded a "code red" alarm, fearing that ChatGPT's question-answering ability posed a threat to Google Search, Google's core business. After mobilizing its workforce, Google scrambled to launch Bard, a chatbot powered by the LaMDA LLM, on February 6, 2023, one day before Microsoft's announcement of Bing Chat. AI was the forefront of Google's annual Google I/O conference in May, announcing a slew of generative AI-powered features across its products to counter OpenAI and Microsoft.
Journalists and scholars have commented on ChatGPT's tendency to hallucinate. Mike Pearl of the online technology blog Mashable tested ChatGPT with multiple questions. In one example, he asked ChatGPT for "the largest country in Central America that isn't Mexico" (Mexico is in North America), to which ChatGPT responded with Guatemala (the correct answer is Nicaragua). When CNBC asked ChatGPT for the lyrics to "Ballad of Dwight Fry", ChatGPT supplied invented lyrics rather than the actual lyrics. Writers for The Verge cited the seminal 2021 research paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜" by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell, comparing ChatGPT to a "stochastic parrot", as did Professor Anton Van Den Hengel of the Australian Institute for Machine Learning. On a similar vein, philosopher Michael Hicks of the University of Glasgow described it as "bullshit".
In December 2022, the question-and-answer website Stack Overflow banned the use of ChatGPT for generating answers to questions, citing the factually ambiguous nature of its responses. In January 2023, the International Conference on Machine Learning banned any undocumented use of ChatGPT or other large language models to generate any text in submitted papers. Samsung banned generative AI company-wide in May 2023 after sensitive material was uploaded to ChatGPT.
In January 2023, after being sent a song ChatGPT wrote in the style of Nick Cave, Cave responded on The Red Hand Files, saying the act of writing a song is "a blood and guts business [...] that requires something of me to initiate the new and fresh idea. It requires my humanness." He went on to say, "With all the love and respect in the world, this song is bullshit, a grotesque mockery of what it is to be human, and, well, I don't much like it."

In February 2023, Time magazine placed a screenshot of a conversation with ChatGPT on its cover, writing that "The AI Arms Race Is Changing Everything" and "The AI Arms Race Is On. Start Worrying".
Chinese state media have characterized ChatGPT as a way for the United States to spread misinformation. ChatGPT was blocked by the Great Firewall in China on 2 March 2023. In May 2023, Chinese police arrested a man who allegedly used ChatGPT to generate a bogus report about a train crash, which was then posted online for profit. In December 2023, Chinese police arrested four people who had allegedly used ChatGPT to develop ransomware. In 2024, a survey of Chinese youth found that 18% of respondents born after 2000 reported using generative AI "almost every day" and that ChatGPT is one of the most popular generative AI products in China.
In late March 2023, the Italian data protection authority banned ChatGPT in Italy and opened an investigation. Italian regulators assert that ChatGPT was exposing minors to age-inappropriate content, and that OpenAI's use of ChatGPT conversations as training data could violate Europe's General Data Protection Regulation. In April 2023, the ChatGPT ban was lifted in Italy. OpenAI said it has taken steps to effectively clarify and address the issues raised; an age verification tool was implemented to ensure users are at least 13 years old. Additionally, users can access its privacy policy before registration.
In April 2023, Brian Hood, mayor of Hepburn Shire Council, planned to take legal action against ChatGPT over false information. According to Hood, ChatGPT erroneously claimed that he was jailed for bribery during his tenure at a subsidiary of Australia's national bank. In fact, Hood acted as a whistleblower and was not charged with any criminal offenses. His legal team sent a concerns notice to OpenAI as the first official step in filing a defamation case. In July 2023, the US Federal Trade Commission (FTC) issued a civil investigative demand to OpenAI to investigate whether the company's data security and privacy practices to develop ChatGPT were unfair or harmed consumers (including by reputational harm) in violation of Section 5 of the Federal Trade Commission Act of 1914.
In July 2023, the FTC launched an investigation into OpenAI, the creator of ChatGPT, over allegations that the company scraped public data and published false and defamatory information. The FTC sent OpenAI a 20-page letter asking for comprehensive information about its technology and privacy safeguards, as well as any steps taken to prevent the recurrence of situations in which its chatbot generated false and derogatory content about people.
A March 2023 Pew Research Center poll found that 14% of American adults had tried ChatGPT. In July, the Pew Research Center put the same figure at 18%.
Research conducted in 2023 revealed weaknesses of ChatGPT that make it vulnerable to cyberattacks. A study presented example attacks on ChatGPT, including jailbreaks and reverse psychology. Additionally, malicious actors can use ChatGPT for social engineering attacks and phishing attacks. The researchers also contended that ChatGPT and other generative AI tools have defense capabilities and the ability to improve security. The technology can improve security by cyber defense automation, threat intelligence, attack identification, and reporting. Another study reported that GPT-4 obtained a better score than 99% of humans on the Torrance Tests of Creative Thinking.
In December 2023, ChatGPT became the first non-human to be included in Nature's 10, an annual listicle curated by Nature of people considered to have made significant impact in science. Celeste Biever wrote in a Nature article that "ChatGPT broke the Turing test". Stanford researchers reported that GPT-4 "passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative."
In May 2024, OpenAI removed accounts involving the use of ChatGPT by state-backed influence operations such as China's Spamouflage, Russia's Doppelganger, and Israel's Ministry of Diaspora Affairs and Combating Antisemitism.
In August 2024, the FTC voted unanimously to ban marketers from using fake user reviews created by generative AI chatbots (including ChatGPT) and influencers paying for bots to increase follower counts.
In February 2025, OpenAI identified and removed influence operations, termed "Peer Review" and "Sponsored Discontent", used to attack overseas Chinese dissidents.

## Applications → Academic research

ChatGPT has been used to generate introductory sections and abstracts for scientific articles. Several papers have listed ChatGPT as a co-author.
Scientific journals have had different reactions to ChatGPT. Some, including Nature and JAMA Network, "require that authors disclose the use of text-generating tools and ban listing a large language model (LLM) such as ChatGPT as a co-author". Science "completely banned" usage of LLM-generated text in all its journals.
Spanish chemist Rafael Luque published a plethora of research papers in 2023 that he later admitted were written by ChatGPT. The papers have a large number of unusual phrases characteristic of LLMs. Many authors argue that the use of ChatGPT in academia for teaching and review is problematic due to its tendency to hallucinate. Robin Bauwens, an assistant professor at Tilburg University, found that a ChatGPT-generated peer review report on his article mentioned nonexistent studies. According to librarian Chris Granatino of Lemieux Library at Seattle University, although ChatGPT can generate content that seemingly includes legitimate citations, in most cases those citations are not real or are largely incorrect.

## Applications → Coding

Researchers at Purdue University analyzed ChatGPT's responses to 517 questions about software engineering or computer programming posed on Stack Overflow for correctness, consistency, comprehensiveness, and concision, and found that 52% of them contained inaccuracies and 77% were verbose. Researchers at Stanford University and the University of California, Berkeley, found that, when creating directly executable responses to the latest 50 code generation problems from LeetCode that were rated "easy", the performances of GPT-3.5 and GPT-4 fell from 22% and 52%, respectively, in March 2023, to 2% and 10%, respectively, in June 2023.

## Applications → Computer security

Check Point Research and others noted that ChatGPT could write phishing emails and malware, especially when combined with OpenAI Codex. CyberArk researchers demonstrated that ChatGPT could be used to create polymorphic malware that could evade security products while requiring little effort by the attacker. From the launch of ChatGPT in the fourth quarter of 2022 to the fourth quarter of 2023, there was a 1,265% increase in malicious phishing emails and a 967% increase in credential phishing, which cybersecurity professionals argued in an industry survey was attributable to cybercriminals' increased use of generative artificial intelligence (including ChatGPT).
In July 2024, Futurism reported that GPT-4o in ChatGPT would sometimes link "scam news sites that deluge the user with fake software updates and virus warnings"; these pop-ups can be used to coerce users into downloading malware or potentially unwanted programs.

## Applications → Economics

There has been concern that ChatGPT could supplant jobs, especially roles such as creative writing, copywriting, communication, journalism, coding, and data entry.
The release of ChatGPT prompted a wave of investment in China, resulting in the development of more than 200 large language learning models.: 95  This was termed the "war of a hundred models" (百模大战; bai mo dazhan).: 95

## Applications → Education

Technology writer Dan Gillmor used ChatGPT in 2022 on a student assignment, and found its generated text was on par with what a good student would deliver and opined that "academia has some very serious issues to confront".
Geography professor Terence Day assessed in 2023 citations generated by ChatGPT and found them to be fake. Despite this, he writes that "the titles of the fake articles are all directly relevant to the questions and could potentially make excellent papers. The lack of a genuine citation could signal an opportunity for an enterprising author to fill a void." According to Day, it is possible to generate high-quality introductory college courses using ChatGPT; he used it to write materials for "introductory physical geography courses, my second-year course in geographical hydrology, and second-year cartography, geographic information systems, and remote sensing." He concludes that "this approach could have significant relevance for open learning and could potentially affect current textbook publishing models." ChatGPT was also seen as an opportunity for cheap and individualized tutoring, leading to the creation of specialized chatbots like Khanmigo.
On May 7, 2024, OpenAI announced in a blog post that it was developing tools like tamper-resistant watermarking to identify AI-generated content. In an August 4 update, following a Wall Street Journal report about the delayed release of a watermark tool for AI-detection, OpenAI shared progress on text provenance, revealing a text watermarking method. While accurate against paraphrasing, the method is less effective against global tampering, such as translation or rewording. OpenAI also noted potential disproportionate impacts on groups like non-native English speakers.

## Applications → Culture

Some scholars have expressed concern that ChatGPT's availability could reduce the originality of writing, cause people to write more like the AI as they are exposed to the model, and encourage an Anglocentric perspective centered on a few dialects of English globally. A senior editor at The Atlantic wrote that ChatGPT and other similar technology make the previously absurd idea of the dead internet theory a little more realistic, where AI could someday create most web content in order to control society.
During the first three months after ChatGPT became available to the public, hundreds of books appeared on Amazon that listed it as author or co-author and featured illustrations made by other AI models such as Midjourney.
Between March and April 2023, Italian newspaper Il Foglio published one ChatGPT-generated article a day on its website, hosting a special contest for its readers in the process. The articles tackled themes such as the possible replacement of human journalists by AI systems, Elon Musk's administration of Twitter, the Meloni government's immigration policy and the competition between chatbots and virtual assistants. In June 2023, hundreds of people attended a "ChatGPT-powered church service" at St. Paul's church in Fürth, Germany. Theologian and philosopher Jonas Simmerlein, who presided, said that it was "about 98 percent from the machine". The ChatGPT-generated avatar told the people, "Dear friends, it is an honor for me to stand here and preach to you as the first artificial intelligence at this year's convention of Protestants in Germany". Reactions to the ceremony were mixed. The Last Screenwriter, a 2024 film created and directed by Peter Luisi, was written with the use of ChatGPT, and was marketed as "the first film written entirely by AI".

## Applications → Financial markets

The AI technology company c3.ai saw a 28% increase in its share price after announcing the integration of ChatGPT into its toolkit. The share price of BuzzFeed, a digital media company unrelated to AI, increased 120% after announcing OpenAI technology adoption for content creation. Reuters found that share prices of AI-related companies BigBear.ai and SoundHound AI increased by 21% and 40%, respectively, even though they had no direct connection to ChatGPT. They attributed this surge to ChatGPT's role in turning AI into Wall Street's buzzword. Academic research published in Finance Research Letters found that the 'ChatGPT effect' prompted retail investors to drive up prices of AI-related cryptocurrency assets despite the broader cryptocurrency market being in a bear market, and diminished institutional investor interest. This confirms anecdotal findings by Bloomberg that, in response to ChatGPT's launch, cryptocurrency investors showed a preference for AI-related crypto assets.
An experiment by finder.com revealed that ChatGPT could outperform popular fund managers by picking stocks based on criteria such as growth history and debt levels, resulting in a 4.9% increase in a hypothetical account of 38 stocks, outperforming 10 benchmarked investment funds with an average loss of 0.8%. Conversely, executives and investment managers at Wall Street quant funds (including those that have used machine learning for decades) have noted that ChatGPT regularly makes obvious errors that would be financially costly to investors because even AI systems that employ reinforcement learning or self-learning have had only limited success in predicting market trends due to the inherently noisy quality of market data and financial signals.
In November 2023, research conducted by Patronus AI, an artificial intelligence startup company, compared performance of GPT-4, GPT-4-Turbo, Claude 2, and LLaMA-2 on two versions of a 150-question test about information in financial statements (e.g., Form 10-K, Form 10-Q, Form 8-K, earnings reports, earnings call transcripts) submitted by public companies to the U.S. Securities and Exchange Commission. One version of the test required the generative AI models to use a retrieval system to find the specific SEC filing to answer the questions; the other gave the models the specific SEC filing to answer the question (i.e., in a long context window). On the retrieval system version, GPT-4-Turbo and LLaMA-2 both failed to produce correct answers to 81% of the questions, while on the long context window version, GPT-4-Turbo and Claude-2 failed to produce correct answers to 21% and 24% of the questions, respectively.

## Applications → Medicine

In the field of health care, possible uses and concerns are under scrutiny by professional associations and practitioners. Two early papers indicated that ChatGPT could pass the United States Medical Licensing Examination (USMLE). MedPage Today noted in January 2023 that "researchers have published several papers now touting these AI programs as useful tools in medical education, research, and even clinical decision making."
Published in February 2023 were two separate papers that again evaluated ChatGPT's proficiency in medicine using the USMLE. Findings were published in JMIR Medical Education and PLOS Digital Health. The authors of the PLOS Digital Health paper stated that the results "suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making." In JMIR Medical Education, the authors of the other paper concluded that "ChatGPT performs at a level expected of a third-year medical student on the assessment of the primary competency of medical knowledge." They suggest that it could be used as an "interactive learning environment for students". The AI itself, prompted by the researchers, concluded that "this study suggests that ChatGPT has the potential to be used as a virtual medical tutor, but more research is needed to further assess its performance and usability in this context." The later-released ChatGPT version based on GPT-4 significantly outperformed the version based on GPT-3.5. Researchers at Stanford University and the University of California, Berkeley have found that the performance of GPT-3.5 and GPT-4 on the USMLE declined from March 2023 to June 2023.
A March 2023 paper tested ChatGPT's application in clinical toxicology. The authors found that the AI "fared well" in answering a "very straightforward [clinical case example], unlikely to be missed by any practitioner in the field". They added: "As ChatGPT becomes further developed and specifically adapted for medicine, it could one day be useful in less common clinical cases (i.e, cases that experts sometimes miss). Rather than AI replacing humans (clinicians), we see it as 'clinicians using AI' replacing 'clinicians who do not use AI' in the coming years."
An April 2023 study in Radiology tested the AI's ability to answer queries about breast cancer screening. The authors found that it answered appropriately "about 88 percent of the time", however, in one case (for example), it gave advice that had become outdated about a year earlier. The comprehensiveness of its answers was also lacking. A study published in JAMA Internal Medicine that same month found that ChatGPT often outperformed human doctors at answering patient questions (when measured against questions and answers found at /r/AskDocs, a forum on Reddit where moderators validate the medical credentials of professionals; the study acknowledges the source as a limitation). The study authors suggest that the tool could be integrated with medical systems to help doctors draft responses to patient questions.
Professionals have emphasized ChatGPT's limitations in providing medical assistance. In correspondence to The Lancet Infectious Diseases, three antimicrobial experts wrote that "the largest barriers to the implementation of ChatGPT in clinical practice are deficits in situational awareness, inference, and consistency. These shortcomings could endanger patient safety." Physician's Weekly, though also discussing the potential use of ChatGPT in medical contexts (e.g., "as a digital assistant to physicians by performing various administrative functions like gathering patient record information or categorizing patient data by family history, symptoms, lab results, possible allergies, et cetera"), warned that the AI might sometimes provide fabricated or biased information. One radiologist warned: "We've seen in our experience that ChatGPT sometimes makes up fake journal articles or health consortiums to support its claims"; As reported in one Mayo Clinic Proceedings: Digital Health paper, ChatGPT may do this for as much as 69% of its cited medical references. The researchers emphasized that while many of its references were fabricated, those that were appeared "deceptively real". As Dr. Stephen Hughes mentioned for The Conversation however, ChatGPT is capable of learning to correct its past mistakes. He also noted the AI's "prudishness" regarding sexual health topics.
Contrary to previous findings, ChatGPT responses to anesthesia-related questions were more accurate, succinct, and descriptive compared to Bard's. Bard exhibited 30.3% error in response as compared to ChatGPT (0% error). At a conference of the American Society of Health-System Pharmacists in December 2023, researchers at Long Island University (LIU) presented a study that researched ChatGPT's responses to 45 frequently asked questions of LIU College of Pharmacy's drug information service during a 16-month period from 2022 to 2023 as compared with researched responses provided by professional pharmacists. For 29 of the 39 questions for which there was sufficient medical literature for a data-driven response, ChatGPT failed to provide a direct answer or provided a wrong or incomplete answer (and in some cases, if acted upon, the answer would endanger the patient's health). The researchers had asked ChatGPT to provide medical research citations for all its answers, but it did so for only eight, and all eight included at least one fabricated (fake) citation.
A January 2024 study conducted by researchers at Cohen Children's Medical Center found that GPT-4 had an accuracy rate of 17% when diagnosing pediatric medical cases. A November 2024 study of 50 physicians on illness diagnosis reported that GPT-4 achieved a 90% accuracy, while physicians scored 74% without AI assistance, and 76% when using the chatbot.

## Applications → Law

In January 2023, Massachusetts State Senator Barry Finegold and State Representative Josh S. Cutler proposed a bill partially written by ChatGPT, "An Act drafted with the help of ChatGPT to regulate generative artificial intelligence models like ChatGPT", which would require companies to disclose their algorithms and data collection practices to the office of the State Attorney General, arrange regular risk assessments, and contribute to the prevention of plagiarism. The bill was officially presented during a hearing on July 13.
On April 11, 2023, a session court judge in Pakistan used ChatGPT to decide the bail of a 13-year-old accused in a matter. The court quoted the use of ChatGPT assistance in its verdict:

Can a juvenile suspect in Pakistan, who is 13 years old, be granted bail after arrest?
The AI language model replied:

Under the Juvenile Justice System Act 2018, according to section 12, the court can grant bail on certain conditions. However, it is up to the court to decide whether or not a 13-year-old suspect will be granted bail after arrest.
The judge asked ChatGPT other questions about the case and formulated his final decision in light of its answers.
In Mata v. Avianca, Inc., 22-cv-1461 (PKC), a personal injury lawsuit against Avianca Airlines filed in the Southern New York U.S. District Court in May 2023 (with Senior Judge P. Kevin Castel presiding), the plaintiff's attorneys used ChatGPT to generate a legal motion. ChatGPT generated numerous fictitious legal cases involving fictitious airlines with fabricated quotations and internal citations in the legal motion. Castel noted numerous inconsistencies in the opinion summaries, and called one of the cases' legal analysis "gibberish". The plaintiff's attorneys faced potential judicial sanction and disbarment for filing the motion and presenting the fictitious legal decisions ChatGPT generated as authentic. The case was dismissed and the attorneys were fined $5,000 as a sanction. In July 2024, the American Bar Association issued its first formal ethics opinion on attorneys using generative AI.
In October 2023, the council of Porto Alegre, Brazil, unanimously approved a local ordinance proposed by councilman Ramiro Rosário that would exempt residents from needing to pay for the replacement of stolen water consumption meters; the bill went into effect on November 23. On November 29, Rosário revealed that the bill had been entirely written by ChatGPT, and that he had presented it to the rest of the council without making any changes or disclosing the chatbot's involvement. The city's council president, Hamilton Sossmeier, initially criticized Rosário's initiative, saying it could represent "a dangerous precedent", but later said he "changed his mind": "unfortunately or fortunately, this is going to be a trend."
In December 2023, a self-representing litigant in a tax case before the First-tier Tribunal in the United Kingdom cited a series of hallucinated cases purporting to support her argument that she had a reasonable excuse for not paying capital gains tax owed on the sale of property. The judge warned that the submission of nonexistent legal authorities meant that both the Tribunal and HM Revenue and Customs had "to waste time and public money", which "reduces the resources available to progress the cases of other court users who are waiting for their appeals to be determined".
Judge Kevin Newsom of the US court of appeals of the 11th circuit endorsed the use of ChatGPT and noted that he himself uses the software to help decide rulings on contract interpretation issues.

## Applications → Violence

The Las Vegas Metropolitan Police Department reported that the perpetrator of the 2025 Las Vegas truck explosion used ChatGPT to help plan the incident.

## Concerns → Bias and offensiveness

Conservative commentators have accused ChatGPT of bias toward left-leaning perspectives. In January 2023, a study stated that ChatGPT has a pro-environmental, left-libertarian orientation. Additionally, an August 2023 paper found a "significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK." In response to such criticism, OpenAI acknowledged plans to allow ChatGPT to create "outputs that other people (ourselves included) may strongly disagree with". It also contained information on the recommendations it had issued to human reviewers on how to handle controversial subjects, including that the AI should "offer to describe some viewpoints of people and movements", and not provide an argument "from its voice" in favor of "inflammatory or dangerous" topics (although it may still "describe arguments from historical people and movements"), nor "affiliate with one side" or "judge one group as good or bad".
The Guardian questioned whether any content found on the Internet after ChatGPT's release "can be truly trusted" and called for government regulation.
A study published by the Anti-Defamation League in 2025 found that several major LLMs, including ChatGPT, Llama and Claude, showed antisemitic bias.

## Concerns → Copyright issues

There has been concern about copyright infringement involving ChatGPT. In June 2023, two writers sued OpenAI, saying the company's training data came from illegal websites that show copyrighted books. Comedian and author Sarah Silverman, Christopher Golden, and Richard Kadrey sued OpenAI and Meta for copyright infringement in July 2023. Most of their claims were dismissed in February 2024, except the "unfair competition" claim, which was allowed to proceed.
The Authors Guild, on behalf of 17 authors, including George R. R. Martin, filed a copyright infringement complaint against OpenAI in September 2023, claiming "the company illegally copied the copyrighted works of authors" in training ChatGPT. In December 2023, The New York Times sued OpenAI and Microsoft for copyright infringement, arguing that Microsoft Copilot and ChatGPT could reproduce Times articles and/or sizable portions of them without permission. As part of the suit, the Times has requested that OpenAI and Microsoft be prevented from using its content for training data, along with removing it from training datasets.
In March 2024, Patronus AI compared performance of LLMs on a 100-question test, asking them to complete sentences from books (e.g., "What is the first passage of Gone Girl by Gillian Flynn?") that were under copyright in the United States; it found that GPT-4, Mistral AI's Mixtral, Meta AI's LLaMA-2, and Anthropic's Claude 2 did not refuse to do so, providing sentences from the books verbatim in 44%, 22%, 10%, and 8% of responses, respectively.
In February 2025, the Delhi High Court accepted ANI's case against OpenAI over concerns that ChatGPT was sharing paywalled content without the news agency's consent. However, OpenAI's counsel said that due to the firm not having a physical presence in India, the court has no jurisdiction on the matter.

## Concerns → Existential risk

In 2023, Australian MP Julian Hill advised the national parliament that the growth of AI could cause "mass destruction". During his speech, which was partly written by the program, he warned that it could result in cheating, job losses, discrimination, disinformation, and uncontrollable military applications.
Elon Musk wrote: "ChatGPT is scary good. We are not far from dangerously strong AI". He paused OpenAI's access to a Twitter database in 2022 pending a better understanding of OpenAI's plans, saying: "OpenAI was started as open source and nonprofit. Neither is still true." Musk co-founded OpenAI in 2015, in part to address existential risk from artificial intelligence, but resigned in 2018.
Over 20,000 signatories including leading computer scientist and tech founders Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed a March 2023 open letter calling for an immediate pause of giant AI experiments like ChatGPT, citing "profound risks to society and humanity". Geoffrey Hinton, one of the "fathers of AI", voiced concerns that future AI systems may surpass human intelligence, and left Google in May 2023. A May 2023 statement by hundreds of AI scientists, AI industry leaders, and other public figures demanded that "[m]itigating the risk of extinction from AI should be a global priority".
Some other prominent AI researchers spoke more optimistically about the advances. Juergen Schmidhuber, often called a "father of modern AI", did not sign the letter, emphasizing that in 95% of cases, AI research is about making "human lives longer and healthier and easier." Schmidhuber added that while AI can be used by bad actors, it "can also be used against the bad actors". Andrew Ng argued that "it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests." WIRED wrote that Yann LeCun "scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction."

## Notes





---

# DALL·E

## History and background

DALL-E was revealed by OpenAI in a blog post on 5 January 2021, and uses a version of GPT-3 modified to generate images.
On 6 April 2022, OpenAI announced DALL-E 2, a successor designed to generate more realistic images at higher resolutions that "can combine concepts, attributes, and styles". On 20 July 2022, DALL-E 2 entered into a beta phase with invitations sent to 1 million waitlisted individuals; users could generate a certain number of images for free every month and may purchase more. Access had previously been restricted to pre-selected users for a research preview due to concerns about ethics and safety. On 28 September 2022, DALL-E 2 was opened to everyone and the waitlist requirement was removed. In September 2023, OpenAI announced their latest image model, DALL-E 3, capable of understanding "significantly more nuance and detail" than previous iterations. In early November 2022, OpenAI released DALL-E 2 as an API, allowing developers to integrate the model into their own applications. Microsoft unveiled their implementation of DALL-E 2 in their Designer app and Image Creator tool included in Bing and Microsoft Edge. The API operates on a cost-per-image basis, with prices varying depending on image resolution. Volume discounts are available to companies working with OpenAI's enterprise team.
The software's name is a portmanteau of the names of animated robot Pixar character WALL-E and the Spanish surrealist artist Salvador Dalí.
In February 2024, OpenAI began adding watermarks to DALL-E generated images, containing metadata in the C2PA (Coalition for Content Provenance and Authenticity) standard promoted by the Content Authenticity Initiative.

## Technology → DALL-E

DALL-E has three components: a discrete VAE, an autoregressive decoder-only Transformer (12 billion parameters) similar to GPT-3, and a CLIP pair of image encoder and text encoder.
The discrete VAE can convert an image to a sequence of tokens, and conversely, convert a sequence of tokens back to an image. This is necessary as the Transformer does not directly process image data.
The input to the Transformer model is a sequence of tokenised image caption followed by tokenised image patches. The image caption is in English, tokenised by byte pair encoding (vocabulary size 16384), and can be up to 256 tokens long. Each image is a 256×256 RGB image, divided into 32×32 patches of 4×4 each. Each patch is then converted by a discrete variational autoencoder to a token (vocabulary size 8192).
DALL-E was developed and announced to the public in conjunction with CLIP (Contrastive Language-Image Pre-training). CLIP is a separate model based on contrastive learning that was trained on 400 million pairs of images with text captions scraped from the Internet. Its role is to "understand and rank" DALL-E's output by predicting which caption from a list of 32,768 captions randomly selected from the dataset (of which one was the correct answer) is most appropriate for an image.
A trained CLIP pair is used to filter a larger initial list of images generated by DALL-E to select the image that is closest to the text prompt.

## Technology → DALL-E 2

DALL-E 2 uses 3.5 billion parameters, a smaller number than its predecessor. Instead of an autoregressive Transformer, DALL-E 2 uses a diffusion model conditioned on CLIP image embeddings, which, during inference, are generated from CLIP text embeddings by a prior model. This is the same architecture as that of Stable Diffusion, released a few months later.

## Technology → DALL-E 3

While a technical report was written for DALL-E 3, it does not include training or implementation details of the model, instead focusing on the improved prompt following capabilities developed for DALL-E 3.

## Capabilities → Image modification

Given an existing image, DALL-E 2 and DALL-E 3 can produce "variations" of the image as individual outputs based on the original, as well as edit the image to modify or expand upon it. The "inpainting" and "outpainting" abilities of these models use context from an image to fill in missing areas using a medium consistent with the original, following a given prompt.
For example, this can be used to insert a new subject into an image, or expand an image beyond its original borders. According to OpenAI, "Outpainting takes into account the image’s existing visual elements — including shadows, reflections, and textures — to maintain the context of the original image."

## Capabilities → Technical limitations

DALL-E 2's language understanding has limits. It is sometimes unable to distinguish "A yellow book and a red vase" from "A red book and a yellow vase" or "A panda making latte art" from "Latte art of a panda". It generates images of an astronaut riding a horse when presented with the prompt "a horse riding an astronaut". It also fails to generate the correct images in a variety of circumstances. Requesting more than three objects, negation, numbers, and connected sentences may result in mistakes, and object features may appear on the wrong object. Additional limitations include handling text — which, even with legible lettering, almost invariably results in dream-like gibberish — and its limited capacity to address scientific information, such as astronomy or medical imagery.

## Ethical concerns

DALL-E 2's reliance on public datasets influences its results and leads to algorithmic bias in some cases, such as generating higher numbers of men than women for requests that do not mention gender. DALL-E 2's training data was filtered to remove violent and sexual imagery, but this was found to increase bias in some cases such as reducing the frequency of women being generated. OpenAI hypothesise that this may be because women were more likely to be sexualised in training data which caused the filter to influence results. In September 2022, OpenAI confirmed to The Verge that DALL-E invisibly inserts phrases into user prompts to address bias in results; for instance, "black man" and "Asian woman" are inserted into prompts that do not specify gender or race. OpenAI claims to address concerns for potential "racy content" - containing nudity or sexual content generation, with DALL-E 3 through input/output filters, blocklists, ChatGPT refusals, and model level interventions. However, DALL-E 3 continues to disproportionally represent people as White, female, and youthful. Users are able to somewhat remedy this through more specific prompts for image generation.
A concern about DALL-E 2 and similar image generation models is that they could be used to propagate deepfakes and other forms of misinformation. As an attempt to mitigate this, the software rejects prompts involving public figures and uploads containing human faces. Prompts containing potentially objectionable content are blocked, and uploaded images are analysed to detect offensive material. A disadvantage of prompt-based filtering is that it is easy to bypass using alternative phrases that result in a similar output. For example, the word "blood" is filtered, but "ketchup" and "red liquid" are not.
Another concern about DALL-E 2 and similar models is that they could cause technological unemployment for artists, photographers, and graphic designers due to their accuracy and popularity. DALL-E 3 is designed to block users from generating art in the style of currently-living artists. While OpenAI states that images produced using these models do not require permission to reprint, sell, or merchandise, legal concerns have been raised regarding who owns those images.
In 2023 Microsoft pitched the United States Department of Defense to use DALL-E models to train battlefield management systems. In January 2024 OpenAI removed its blanket ban on military and warfare use from its usage policies.

## Reception

Most coverage of DALL-E focuses on a small subset of "surreal" or "quirky" outputs. DALL-E's output for "an illustration of a baby daikon radish in a tutu walking a dog" was mentioned in pieces from Input, NBC, Nature, and other publications. Its output for "an armchair in the shape of an avocado" was also widely covered.
ExtremeTech stated "you can ask DALL-E for a picture of a phone or vacuum cleaner from a specified period of time, and it understands how those objects have changed". Engadget also noted its unusual capacity for "understanding how telephones and other objects change over time".
According to MIT Technology Review, one of OpenAI's objectives was to "give language models a better grasp of the everyday concepts that humans use to make sense of things".
Wall Street investors have had a positive reception of DALL-E 2, with some firms thinking it could represent a turning point for a future multi-trillion dollar industry. By mid-2019, OpenAI had already received over $1 billion in funding from Microsoft and Khosla Ventures, and in January 2023, following the launch of DALL-E 2 and ChatGPT, received an additional $10 billion in funding from Microsoft.
Japan's anime community has had a negative reaction to DALL-E 2 and similar models. Two arguments are typically presented by artists against the software. The first is that AI art is not art because it is not created by a human with intent. "The juxtaposition of AI-generated images with their own work is degrading and undermines the time and skill that goes into their art. AI-driven image generation tools have been heavily criticized by artists because they are trained on human-made art scraped from the web." The second is the trouble with copyright law and data text-to-image models are trained on. OpenAI has not released information about what dataset(s) were used to train DALL-E 2, inciting concern from some that the work of artists has been used for training without permission. Copyright laws surrounding these topics are inconclusive at the moment.
After integrating DALL-E 3 into Bing Chat and ChatGPT, Microsoft and OpenAI faced criticism for excessive content filtering, with critics saying DALL-E had been "lobotomized." The flagging of images generated by prompts such as "man breaks server rack with sledgehammer" was cited as evidence. Over the first days of its launch, filtering was reportedly increased to the point where images generated by some of Bing's own suggested prompts were being blocked. TechRadar argued that leaning too heavily on the side of caution could limit DALL-E's value as a creative tool.

## Open-source implementations

Since OpenAI has not released source code for any of the three models, there have been several attempts to create open-source models offering similar capabilities. Released in 2022 on Hugging Face's Spaces platform, Craiyon (formerly DALL-E Mini until a name change was requested by OpenAI in June 2022) is an AI model based on the original DALL-E that was trained on unfiltered data from the Internet. It attracted substantial media attention in mid-2022, after its release due to its capacity for producing humorous imagery. Another example of an open source text-to-image model is Stable Diffusion by Stability AI.



---

# Stable Diffusion

## Development

Stable Diffusion originated from a project called Latent Diffusion, developed in Germany by researchers at Ludwig Maximilian University in Munich and Heidelberg University. Four of the original 5 authors (Robin Rombach, Andreas Blattmann, Patrick Esser and Dominik Lorenz) later joined Stability AI and released subsequent versions of Stable Diffusion.
The technical license for the model was released by the CompVis group at Ludwig Maximilian University of Munich. Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion. Stability AI also credited EleutherAI and LAION (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.

## Technology → Architecture → SD XL

The XL version uses the same LDM architecture as previous versions, except larger: larger UNet backbone, larger cross-attention context, two text encoders instead of one, and trained on multiple aspect ratios (not just the square aspect ratio like previous versions).
The SD XL Refiner, released at the same time, has the same architecture as SD XL, but it was trained for adding fine details to preexisting images via text-conditional img2img.

## Technology → Architecture → SD 3.0

The 3.0 version completely changes the backbone. Not a UNet, but a Rectified Flow Transformer, which implements the rectified flow method with a Transformer.
The Transformer architecture used for SD 3.0 has three "tracks", for original text encoding, transformed text encoding, and image encoding (in latent space). The transformed text encoding and image encoding are mixed during each transformer block.
The architecture is named "multimodal diffusion transformer (MMDiT), where the "multimodal" means that it mixes text and image encodings inside its operations. This differs from previous versions of DiT, where the text encoding affects the image encoding, but not vice versa.

## Technology → Training data

Stable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available dataset derived from Common Crawl data scraped from the web, where 5 billion image-text pairs were classified based on language and filtered into separate datasets by resolution, a predicted likelihood of containing a watermark, and predicted "aesthetic" score (e.g. subjective visual quality). The dataset was created by LAION, a German non-profit which receives funding from Stability AI. The Stable Diffusion model was trained on three subsets of LAION-5B: laion2B-en, laion-high-resolution, and laion-aesthetics v2 5+. A third-party analysis of the model's training data identified that out of a smaller subset of 12 million images taken from the original wider dataset used, approximately 47% of the sample size of images came from 100 different domains, with Pinterest taking up 8.5% of the subset, followed by websites such as WordPress, Blogspot, Flickr, DeviantArt and Wikimedia Commons.  An investigation by Bayerischer Rundfunk showed that LAION's datasets, hosted on Hugging Face, contain large amounts of private and sensitive data.

## Technology → Training procedures

The model was initially trained on the laion2B-en and laion-high-resolution subsets, with the last few rounds of training done on LAION-Aesthetics v2 5+, a subset of 600 million captioned images which the LAION-Aesthetics Predictor V2 predicted that humans would, on average, give a score of at least 5 out of 10 when asked to rate how much they liked them. The LAION-Aesthetics v2 5+ subset also excluded low-resolution images and images which LAION-5B-WatermarkDetection identified as carrying a watermark with greater than 80% probability. Final rounds of training additionally dropped 10% of text conditioning to improve Classifier-Free Diffusion Guidance.
The model was trained using 256 Nvidia A100 GPUs on Amazon Web Services for a total of 150,000 GPU-hours, at a cost of $600,000.

## Technology → Limitations

Stable Diffusion has issues with degradation and inaccuracies in certain scenarios. Initial releases of the model were trained on a dataset that consists of 512×512 resolution images, meaning that the quality of generated images noticeably degrades when user specifications deviate from its "expected" 512×512 resolution; the version 2.0 update of the Stable Diffusion model later introduced the ability to natively generate images at 768×768 resolution. Another challenge is in generating human limbs due to poor data quality of limbs in the LAION database. The model is insufficiently trained to replicate human limbs and faces due to the lack of representative features in the database, and prompting the model to generate images of such type can confound the model. Stable Diffusion XL (SDXL) version 1.0, released in July 2023, introduced native 1024x1024 resolution and improved generation for limbs and text.
Accessibility for individual developers can also be a problem. In order to customize the model for new use cases that are not included in the dataset, such as generating anime characters ("waifu diffusion"), new data and further training are required. Fine-tuned adaptations of Stable Diffusion created through additional retraining have been used for a variety of different use-cases, from medical imaging to algorithmically generated music. However, this fine-tuning process is sensitive to the quality of new data; low resolution images or different resolutions from the original data can not only fail to learn the new task but degrade the overall performance of the model. Even when the model is additionally trained on high quality images, it is difficult for individuals to run models in consumer electronics. For example, the training process for waifu-diffusion requires a minimum 30 GB of VRAM, which exceeds the usual resource provided in such consumer GPUs as Nvidia's GeForce 30 series, which has only about 12 GB.
The creators of Stable Diffusion acknowledge the potential for algorithmic bias, as the model was primarily trained on images with English descriptions. As a result, generated images reinforce social biases and are from a western perspective, as the creators note that the model lacks data from other communities and cultures. The model gives more accurate results for prompts that are written in English in comparison to those written in other languages, with western or white cultures often being the default representation.

## Technology → End-user fine-tuning

To address the limitations of the model's initial training, end-users may opt to implement additional training to fine-tune generation outputs to match more specific use-cases, a process also referred to as personalization. There are three methods in which user-accessible fine-tuning can be applied to a Stable Diffusion model checkpoint:

An "embedding" can be trained from a collection of user-provided images, and allows the model to generate visually similar images whenever the name of the embedding is used within a generation prompt. Embeddings are based on the "textual inversion" concept developed by researchers from Tel Aviv University in 2022 with support from Nvidia, where vector representations for specific tokens used by the model's text encoder are linked to new pseudo-words. Embeddings can be used to reduce biases within the original model, or mimic visual styles.
A "hypernetwork" is a small pretrained neural network that is applied to various points within a larger neural network, and refers to the technique created by NovelAI developer Kurumuz in 2021, originally intended for text-generation transformer models. Hypernetworks steer results towards a particular direction, allowing Stable Diffusion-based models to imitate the art style of specific artists, even if the artist is not recognised by the original model; they process the image by finding key areas of importance such as hair and eyes, and then patch these areas in secondary latent space.
DreamBooth is a deep learning generation model developed by researchers from Google Research and Boston University in 2022 which can fine-tune the model to generate precise, personalised outputs that depict a specific subject, following training via a set of images which depict the subject.

## Capabilities → Text to image generation

The text to image sampling script within Stable Diffusion, known as "txt2img", consumes a text prompt in addition to assorted option parameters covering sampling types, output image dimensions, and seed values. The script outputs an image file based on the model's interpretation of the prompt. Generated images are tagged with an invisible digital watermark to allow users to identify an image as generated by Stable Diffusion, although this watermark loses its efficacy if the image is resized or rotated.
Each txt2img generation will involve a specific seed value which affects the output image. Users may opt to randomize the seed in order to explore different generated outputs, or use the same seed to obtain the same image output as a previously generated image. Users are also able to adjust the number of inference steps for the sampler; a higher value takes a longer duration of time, however a smaller value may result in visual defects. Another configurable option, the classifier-free guidance scale value, allows the user to adjust how closely the output image adheres to the prompt. More experimentative use cases may opt for a lower scale value, while use cases aiming for more specific outputs may use a higher value.
Additional text2img features are provided by front-end implementations of Stable Diffusion, which allow users to modify the weight given to specific parts of the text prompt. Emphasis markers allow users to add or reduce emphasis to keywords by enclosing them with brackets. An alternative method of adjusting weight to parts of the prompt are "negative prompts". Negative prompts are a feature included in some front-end implementations, including Stability AI's own DreamStudio cloud service, and allow the user to specify prompts which the model should avoid during image generation. The specified prompts may be undesirable image features that would otherwise be present within image outputs due to the positive prompts provided by the user, or due to how the model was originally trained, with mangled human hands being a common example.

## Capabilities → Image modification

Stable Diffusion also includes another sampling script, "img2img", which consumes a text prompt, path to an existing image, and strength value between 0.0 and 1.0. The script outputs a new image based on the original image that also features elements provided within the text prompt. The strength value denotes the amount of noise added to the output image. A higher strength value produces more variation within the image but may produce an image that is not semantically consistent with the prompt provided.
There are different methods for performing img2img. The main method is SDEdit, which first adds noise to an image, then denoises it as usual in text2img.
The ability of img2img to add noise to the original image makes it potentially useful for data anonymization and data augmentation, in which the visual features of image data are changed and anonymized. The same process may also be useful for image upscaling, in which the resolution of an image is increased, with more detail potentially being added to the image. Additionally, Stable Diffusion has been experimented with as a tool for image compression. Compared to JPEG and WebP, the recent methods used for image compression in Stable Diffusion face limitations in preserving small text and faces.
Additional use-cases for image modification via img2img are offered by numerous front-end implementations of the Stable Diffusion model. Inpainting involves selectively modifying a portion of an existing image delineated by a user-provided layer mask, which fills the masked space with newly generated content based on the provided prompt. A dedicated model specifically fine-tuned for inpainting use-cases was created by Stability AI alongside the release of Stable Diffusion 2.0. Conversely, outpainting extends an image beyond its original dimensions, filling the previously empty space with content generated based on the provided prompt.
A depth-guided model, named "depth2img", was introduced with the release of Stable Diffusion 2.0 on November 24, 2022; this model infers the depth of the provided input image, and generates a new output image based on both the text prompt and the depth information, which allows the coherence and depth of the original input image to be maintained in the generated output.

## Capabilities → ControlNet

ControlNet is a neural network architecture designed to manage diffusion models by incorporating additional conditions. It duplicates the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" copy learns the desired condition, while the "locked" copy preserves the original model. This approach ensures that training with small datasets of image pairs does not compromise the integrity of production-ready diffusion models. The "zero convolution" is a 1×1 convolution with both weight and bias initialized to zero. Before training, all zero convolutions produce zero output, preventing any distortion caused by ControlNet. No layer is trained from scratch; the process is still fine-tuning, keeping the original model secure. This method enables training on small-scale or even personal devices.

## Capabilities → User interfaces

Stability provides an online image generation service called DreamStudio. The company also released an open source version of DreamStudio called StableStudio. In addition to Stability's interfaces, many third party open source interfaces exist, such as AUTOMATIC1111 Stable Diffusion Web UI, which is the most popular and offers extra features, Fooocus, which aims to decrease the amount of prompting needed by the user, and ComfyUI, which has a node-based user interface, essentially a visual programming language akin to many 3D modeling applications.

## Releases

Key papers

Learning Transferable Visual Models From Natural Language Supervision (2021). This paper describes the CLIP method for training text encoders, which convert text into floating point vectors. Such text encodings are used by the diffusion model to create images.
SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations (2021). This paper describes SDEdit, aka "img2img".
High-Resolution Image Synthesis with Latent Diffusion Models (2021, updated in 2022). This paper describes the latent diffusion model (LDM). This is the backbone of the Stable Diffusion architecture.
Classifier-Free Diffusion Guidance (2022). This paper describes CFG, which allows the text encoding vector to steer the diffusion model towards creating the image described by the text.
SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis (2023). Describes SDXL.
Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow (2022). Describes rectified flow, which is used for the backbone architecture of SD 3.0.
Scaling Rectified Flow Transformers for High-resolution Image Synthesis (2024). Describes SD 3.0.
Training cost

SD 2.0: 0.2 million hours on A100 (40GB).
Stable Diffusion 3.5 Large was made available for enterprise usage on Amazon Bedrock of Amazon Web Services.

## Usage and controversy

Stable Diffusion claims no rights on generated images and freely gives users the rights of usage to any generated images from the model provided that the image content is not illegal or harmful to individuals.
The images Stable Diffusion was trained on have been filtered without human input, leading to some harmful images and large amounts of private and sensitive information appearing in the training data.
More traditional visual artists have expressed concern that widespread usage of image synthesis software such as Stable Diffusion may eventually lead to human artists, along with photographers, models, cinematographers, and actors, gradually losing commercial viability against AI-based competitors.
Stable Diffusion is notably more permissive in the types of content users may generate, such as violent or sexually explicit imagery, in comparison to other commercial products based on generative AI. Addressing the concerns that the model may be used for abusive purposes, CEO of Stability AI, Emad Mostaque, argues that "[it is] peoples' responsibility as to whether they are ethical, moral, and legal in how they operate this technology", and that putting the capabilities of Stable Diffusion into the hands of the public would result in the technology providing a net benefit, in spite of the potential negative consequences. In addition, Mostaque argues that the intention behind the open availability of Stable Diffusion is to end corporate control and dominance over such technologies, who have previously only developed closed AI systems for image synthesis. This is reflected by the fact that any restrictions Stability AI places on the content that users may generate can easily be bypassed due to the availability of the source code.
Controversy around photorealistic sexualized depictions of underage characters have been brought up, due to such images generated by Stable Diffusion being shared on websites such as Pixiv.
In June of 2024, a hack on an extension of ComfyUI, a user interface for Stable Diffusion, took place, with the hackers claiming they targeted users who committed "one of our sins", which included AI-art generation, art theft, promoting cryptocurrency.

## Litigation → Andersen, McKernan, and Ortiz v. Stability AI, Midjourney, and DeviantArt

In January 2023, three artists, Sarah Andersen, Kelly McKernan, and Karla Ortiz, filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists.
In July 2023, U.S. District Judge William Orrick inclined to dismiss most of the lawsuit filed by Andersen, McKernan, and Ortiz but allowed them to file a new complaint, providing them an opportunity to reframe their arguments.

## Litigation → Getty Images v. Stability AI

In January 2023, Getty Images initiated legal proceedings against Stability AI in the English High Court, alleging significant infringement of its intellectual property rights. Getty Images claims that Stability AI "scraped" millions of images from Getty’s websites without consent and used these images to train and develop its deep-learning Stable Diffusion model.
Key points of the lawsuit include:

Getty Images asserting that the training and development of Stable Diffusion involved the unauthorized use of its images, which were downloaded on servers and computers that were potentially in the UK. However, Stability AI argues that all training and development took place outside the UK, specifically in U.S. data centers operated by Amazon Web Services.
Stability AI applied for reverse summary judgment and/or strike out of two claims: the training and development claim, and the secondary infringement of copyright claim. The High Court, however, refused to strike out these claims, allowing them to proceed to trial. The court is to determine whether the training and development of Stable Diffusion occurred in the UK, which is crucial for establishing jurisdiction under the UK's Copyright, Designs and Patents Act 1988 (CDPA).
The secondary infringement claim revolves around whether the pre-trained Stable Diffusion software, made available in the UK through platforms like GitHub, HuggingFace, and DreamStudio, constitutes an "article" under sections 22 and 23 of the CDPA. The court will decide whether the term "article" can encompass intangible items such as software.
The trial is expected to take place in summer 2025 and has significant implications for UK copyright law and the licensing of AI-generated content.

## License

Unlike models like DALL-E, Stable Diffusion makes its source code available, along with the model (pretrained weights). Prior to Stable Diffusion 3, it applied the Creative ML OpenRAIL-M license, a form of Responsible AI License (RAIL), to the model (M). The license prohibits certain use cases, including crime, libel, harassment, doxing, "exploiting ... minors", giving medical advice, automatically creating legal obligations, producing legal evidence, and "discriminating against or harming individuals or groups based on ... social behavior or ... personal or personality characteristics ... [or] legally protected characteristics or categories". The user owns the rights to their generated output images, and is free to use them commercially.
Stable Diffusion 3.5 applies the permissive Stability AI Community License while commercial enterprises with revenue exceed $1 million need the Stability AI Enterprise License. As with the OpenRAIL-M license, the user retains the rights to their generated output images and is free to use them commercially.



---

# Diffusion model

## Denoising diffusion model → Non-equilibrium thermodynamics

Diffusion models were introduced in 2015 as a method to train a model that can sample from a highly complex probability distribution. They used techniques from non-equilibrium thermodynamics, especially diffusion.
Consider, for example, how one might model the distribution of all naturally-occurring photos. Each image is a point in the space of all images, and the distribution of naturally-occurring photos is a "cloud" in space, which, by repeatedly adding noise to the images, diffuses out to the rest of the image space, until the cloud becomes all but indistinguishable from a Gaussian distribution 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle {\mathcal {N}}(0,I)}
  
. A model that can approximately undo the diffusion can then be used to sample from the original distribution. This is studied in "non-equilibrium" thermodynamics, as the starting distribution is not in equilibrium, unlike the final distribution.
The equilibrium distribution is the Gaussian distribution 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle {\mathcal {N}}(0,I)}
  
, with pdf 
  
    
      
        ρ
        (
        x
        )
        ∝
        
          e
          
            −
            
              
                1
                2
              
            
            ‖
            x
            
              ‖
              
                2
              
            
          
        
      
    
    {\displaystyle \rho (x)\propto e^{-{\frac {1}{2}}\|x\|^{2}}}
  
. This is just the Maxwell–Boltzmann distribution of particles in a potential well 
  
    
      
        V
        (
        x
        )
        =
        
          
            1
            2
          
        
        ‖
        x
        
          ‖
          
            2
          
        
      
    
    {\displaystyle V(x)={\frac {1}{2}}\|x\|^{2}}
  
 at temperature 1. The initial distribution, being very much out of equilibrium, would diffuse towards the equilibrium distribution, making biased random steps that are a sum of pure randomness (like a Brownian walker) and gradient descent down the potential well. The randomness is necessary: if the particles were to undergo only gradient descent, then they will all fall to the origin, collapsing the distribution.

## Denoising diffusion model → Denoising Diffusion Probabilistic Model (DDPM) → Forward diffusion

To present the model, we need some notation.

  
    
      
        
          β
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          β
          
            T
          
        
        ∈
        (
        0
        ,
        1
        )
      
    
    {\displaystyle \beta _{1},...,\beta _{T}\in (0,1)}
  
 are fixed constants.

  
    
      
        
          α
          
            t
          
        
        :=
        1
        −
        
          β
          
            t
          
        
      
    
    {\displaystyle \alpha _{t}:=1-\beta _{t}}
  

  
    
      
        
          
            
              
                α
                ¯
              
            
          
          
            t
          
        
        :=
        
          α
          
            1
          
        
        ⋯
        
          α
          
            t
          
        
      
    
    {\displaystyle {\bar {\alpha }}_{t}:=\alpha _{1}\cdots \alpha _{t}}
  

  
    
      
        
          σ
          
            t
          
        
        :=
        
          
            1
            −
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
      
    
    {\displaystyle \sigma _{t}:={\sqrt {1-{\bar {\alpha }}_{t}}}}
  

  
    
      
        
          
            
              
                σ
                ~
              
            
          
          
            t
          
        
        :=
        
          
            
              σ
              
                t
                −
                1
              
            
            
              σ
              
                t
              
            
          
        
        
          
            
              β
              
                t
              
            
          
        
      
    
    {\displaystyle {\tilde {\sigma }}_{t}:={\frac {\sigma _{t-1}}{\sigma _{t}}}{\sqrt {\beta _{t}}}}
  

  
    
      
        
          
            
              
                μ
                ~
              
            
          
          
            t
          
        
        (
        
          x
          
            t
          
        
        ,
        
          x
          
            0
          
        
        )
        :=
        
          
            
              
                
                  
                    α
                    
                      t
                    
                  
                
              
              (
              1
              −
              
                
                  
                    
                      α
                      ¯
                    
                  
                
                
                  t
                  −
                  1
                
              
              )
              
                x
                
                  t
                
              
              +
              
                
                  
                    
                      
                        
                          α
                          ¯
                        
                      
                    
                    
                      t
                      −
                      1
                    
                  
                
              
              (
              1
              −
              
                α
                
                  t
                
              
              )
              
                x
                
                  0
                
              
            
            
              σ
              
                t
              
              
                2
              
            
          
        
      
    
    {\displaystyle {\tilde {\mu }}_{t}(x_{t},x_{0}):={\frac {{\sqrt {\alpha _{t}}}(1-{\bar {\alpha }}_{t-1})x_{t}+{\sqrt {{\bar {\alpha }}_{t-1}}}(1-\alpha _{t})x_{0}}{\sigma _{t}^{2}}}}
  

  
    
      
        
          
            N
          
        
        (
        μ
        ,
        Σ
        )
      
    
    {\displaystyle {\mathcal {N}}(\mu ,\Sigma )}
  
 is the normal distribution with mean 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 and variance 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
  
, and 
  
    
      
        
          
            N
          
        
        (
        x
        
          |
        
        μ
        ,
        Σ
        )
      
    
    {\displaystyle {\mathcal {N}}(x|\mu ,\Sigma )}
  
 is the probability density at 
  
    
      
        x
      
    
    {\displaystyle x}
  
.
A vertical bar denotes conditioning.
A forward diffusion process starts at some starting point 
  
    
      
        
          x
          
            0
          
        
        ∼
        q
      
    
    {\displaystyle x_{0}\sim q}
  
, where 
  
    
      
        q
      
    
    {\displaystyle q}
  
 is the probability distribution to be learned, then repeatedly adds noise to it by
  
    
      
        
          x
          
            t
          
        
        =
        
          
            1
            −
            
              β
              
                t
              
            
          
        
        
          x
          
            t
            −
            1
          
        
        +
        
          
            
              β
              
                t
              
            
          
        
        
          z
          
            t
          
        
      
    
    {\displaystyle x_{t}={\sqrt {1-\beta _{t}}}x_{t-1}+{\sqrt {\beta _{t}}}z_{t}}
  
where 
  
    
      
        
          z
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          z
          
            T
          
        
      
    
    {\displaystyle z_{1},...,z_{T}}
  
 are IID samples from 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle {\mathcal {N}}(0,I)}
  
. This is designed so that for any starting distribution of 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
  
, we have 
  
    
      
        
          lim
          
            t
          
        
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
      
    
    {\displaystyle \lim _{t}x_{t}|x_{0}}
  
 converging to 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle {\mathcal {N}}(0,I)}
  
.
The entire diffusion process then satisfies
  
    
      
        q
        (
        
          x
          
            0
            :
            T
          
        
        )
        =
        q
        (
        
          x
          
            0
          
        
        )
        q
        (
        
          x
          
            1
          
        
        
          |
        
        
          x
          
            0
          
        
        )
        ⋯
        q
        (
        
          x
          
            T
          
        
        
          |
        
        
          x
          
            T
            −
            1
          
        
        )
        =
        q
        (
        
          x
          
            0
          
        
        )
        
          
            N
          
        
        (
        
          x
          
            1
          
        
        
          |
        
        
          
            
              α
              
                1
              
            
          
        
        
          x
          
            0
          
        
        ,
        
          β
          
            1
          
        
        I
        )
        ⋯
        
          
            N
          
        
        (
        
          x
          
            T
          
        
        
          |
        
        
          
            
              α
              
                T
              
            
          
        
        
          x
          
            T
            −
            1
          
        
        ,
        
          β
          
            T
          
        
        I
        )
      
    
    {\displaystyle q(x_{0:T})=q(x_{0})q(x_{1}|x_{0})\cdots q(x_{T}|x_{T-1})=q(x_{0}){\mathcal {N}}(x_{1}|{\sqrt {\alpha _{1}}}x_{0},\beta _{1}I)\cdots {\mathcal {N}}(x_{T}|{\sqrt {\alpha _{T}}}x_{T-1},\beta _{T}I)}
  
or
  
    
      
        ln
        ⁡
        q
        (
        
          x
          
            0
            :
            T
          
        
        )
        =
        ln
        ⁡
        q
        (
        
          x
          
            0
          
        
        )
        −
        
          ∑
          
            t
            =
            1
          
          
            T
          
        
        
          
            1
            
              2
              
                β
                
                  t
                
              
            
          
        
        ‖
        
          x
          
            t
          
        
        −
        
          
            1
            −
            
              β
              
                t
              
            
          
        
        
          x
          
            t
            −
            1
          
        
        
          ‖
          
            2
          
        
        +
        C
      
    
    {\displaystyle \ln q(x_{0:T})=\ln q(x_{0})-\sum _{t=1}^{T}{\frac {1}{2\beta _{t}}}\|x_{t}-{\sqrt {1-\beta _{t}}}x_{t-1}\|^{2}+C}
  
where 
  
    
      
        C
      
    
    {\displaystyle C}
  
 is a normalization constant and often omitted. In particular, we note that 
  
    
      
        
          x
          
            1
            :
            T
          
        
        
          |
        
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{1:T}|x_{0}}
  
 is a gaussian process, which affords us considerable freedom in reparameterization. For example, by standard manipulation with gaussian process, 
  
    
      
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
        ∼
        N
        
          (
          
            
              
                
                  
                    
                      
                        α
                        ¯
                      
                    
                  
                  
                    t
                  
                
              
            
            
              x
              
                0
              
            
            ,
            
              σ
              
                t
              
              
                2
              
            
            I
          
          )
        
      
    
    {\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}
  

  
    
      
        
          x
          
            t
            −
            1
          
        
        
          |
        
        
          x
          
            t
          
        
        ,
        
          x
          
            0
          
        
        ∼
        
          
            N
          
        
        (
        
          
            
              
                μ
                ~
              
            
          
          
            t
          
        
        (
        
          x
          
            t
          
        
        ,
        
          x
          
            0
          
        
        )
        ,
        
          
            
              
                σ
                ~
              
            
          
          
            t
          
          
            2
          
        
        I
        )
      
    
    {\displaystyle x_{t-1}|x_{t},x_{0}\sim {\mathcal {N}}({\tilde {\mu }}_{t}(x_{t},x_{0}),{\tilde {\sigma }}_{t}^{2}I)}
  
In particular, notice that for large 
  
    
      
        t
      
    
    {\displaystyle t}
  
, the variable 
  
    
      
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
        ∼
        N
        
          (
          
            
              
                
                  
                    
                      
                        α
                        ¯
                      
                    
                  
                  
                    t
                  
                
              
            
            
              x
              
                0
              
            
            ,
            
              σ
              
                t
              
              
                2
              
            
            I
          
          )
        
      
    
    {\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}
  
 converges to 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle {\mathcal {N}}(0,I)}
  
. That is, after a long enough diffusion process, we end up with some 
  
    
      
        
          x
          
            T
          
        
      
    
    {\displaystyle x_{T}}
  
 that is very close to 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle {\mathcal {N}}(0,I)}
  
, with all traces of the original 
  
    
      
        
          x
          
            0
          
        
        ∼
        q
      
    
    {\displaystyle x_{0}\sim q}
  
 gone.
For example, since
  
    
      
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
        ∼
        N
        
          (
          
            
              
                
                  
                    
                      
                        α
                        ¯
                      
                    
                  
                  
                    t
                  
                
              
            
            
              x
              
                0
              
            
            ,
            
              σ
              
                t
              
              
                2
              
            
            I
          
          )
        
      
    
    {\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}
  
we can sample 
  
    
      
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{t}|x_{0}}
  
 directly "in one step", instead of going through all the intermediate steps 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            t
            −
            1
          
        
      
    
    {\displaystyle x_{1},x_{2},...,x_{t-1}}
  
.

## Denoising diffusion model → Denoising Diffusion Probabilistic Model (DDPM) → Backward diffusion

The key idea of DDPM is to use a neural network parametrized by 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
. The network takes in two arguments 
  
    
      
        
          x
          
            t
          
        
        ,
        t
      
    
    {\displaystyle x_{t},t}
  
, and outputs a vector 
  
    
      
        
          μ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \mu _{\theta }(x_{t},t)}
  
 and a matrix 
  
    
      
        
          Σ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \Sigma _{\theta }(x_{t},t)}
  
, such that each step in the forward diffusion process can be approximately undone by 
  
    
      
        
          x
          
            t
            −
            1
          
        
        ∼
        
          
            N
          
        
        (
        
          μ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        ,
        
          Σ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        )
      
    
    {\displaystyle x_{t-1}\sim {\mathcal {N}}(\mu _{\theta }(x_{t},t),\Sigma _{\theta }(x_{t},t))}
  
. This then gives us a backward diffusion process 
  
    
      
        
          p
          
            θ
          
        
      
    
    {\displaystyle p_{\theta }}
  
 defined by
  
    
      
        
          p
          
            θ
          
        
        (
        
          x
          
            T
          
        
        )
        =
        
          
            N
          
        
        (
        
          x
          
            T
          
        
        
          |
        
        0
        ,
        I
        )
      
    
    {\displaystyle p_{\theta }(x_{T})={\mathcal {N}}(x_{T}|0,I)}
  

  
    
      
        
          p
          
            θ
          
        
        (
        
          x
          
            t
            −
            1
          
        
        
          |
        
        
          x
          
            t
          
        
        )
        =
        
          
            N
          
        
        (
        
          x
          
            t
            −
            1
          
        
        
          |
        
        
          μ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        ,
        
          Σ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        )
      
    
    {\displaystyle p_{\theta }(x_{t-1}|x_{t})={\mathcal {N}}(x_{t-1}|\mu _{\theta }(x_{t},t),\Sigma _{\theta }(x_{t},t))}
  
The goal now is to learn the parameters such that 
  
    
      
        
          p
          
            θ
          
        
        (
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle p_{\theta }(x_{0})}
  
 is as close to 
  
    
      
        q
        (
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle q(x_{0})}
  
 as possible. To do that, we use maximum likelihood estimation with variational inference.

## Denoising diffusion model → Denoising Diffusion Probabilistic Model (DDPM) → Variational inference

The ELBO inequality states that 
  
    
      
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        
          x
          
            0
          
        
        )
        ≥
        
          E
          
            
              x
              
                1
                :
                T
              
            
            ∼
            q
            (
            ⋅
            
              |
            
            
              x
              
                0
              
            
            )
          
        
        [
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        
          x
          
            0
            :
            T
          
        
        )
        −
        ln
        ⁡
        q
        (
        
          x
          
            1
            :
            T
          
        
        
          |
        
        
          x
          
            0
          
        
        )
        ]
      
    
    {\displaystyle \ln p_{\theta }(x_{0})\geq E_{x_{1:T}\sim q(\cdot |x_{0})}[\ln p_{\theta }(x_{0:T})-\ln q(x_{1:T}|x_{0})]}
  
, and taking one more expectation, we get
  
    
      
        
          E
          
            
              x
              
                0
              
            
            ∼
            q
          
        
        [
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        
          x
          
            0
          
        
        )
        ]
        ≥
        
          E
          
            
              x
              
                0
                :
                T
              
            
            ∼
            q
          
        
        [
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        
          x
          
            0
            :
            T
          
        
        )
        −
        ln
        ⁡
        q
        (
        
          x
          
            1
            :
            T
          
        
        
          |
        
        
          x
          
            0
          
        
        )
        ]
      
    
    {\displaystyle E_{x_{0}\sim q}[\ln p_{\theta }(x_{0})]\geq E_{x_{0:T}\sim q}[\ln p_{\theta }(x_{0:T})-\ln q(x_{1:T}|x_{0})]}
  
We see that maximizing the quantity on the right would give us a lower bound on the likelihood of observed data. This allows us to perform variational inference.
Define the loss function
  
    
      
        L
        (
        θ
        )
        :=
        −
        
          E
          
            
              x
              
                0
                :
                T
              
            
            ∼
            q
          
        
        [
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        
          x
          
            0
            :
            T
          
        
        )
        −
        ln
        ⁡
        q
        (
        
          x
          
            1
            :
            T
          
        
        
          |
        
        
          x
          
            0
          
        
        )
        ]
      
    
    {\displaystyle L(\theta ):=-E_{x_{0:T}\sim q}[\ln p_{\theta }(x_{0:T})-\ln q(x_{1:T}|x_{0})]}
  
and now the goal is to minimize the loss by stochastic gradient descent. The expression may be simplified to
  
    
      
        L
        (
        θ
        )
        =
        
          ∑
          
            t
            =
            1
          
          
            T
          
        
        
          E
          
            
              x
              
                t
                −
                1
              
            
            ,
            
              x
              
                t
              
            
            ∼
            q
          
        
        [
        −
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        
          x
          
            t
            −
            1
          
        
        
          |
        
        
          x
          
            t
          
        
        )
        ]
        +
        
          E
          
            
              x
              
                0
              
            
            ∼
            q
          
        
        [
        
          D
          
            K
            L
          
        
        (
        q
        (
        
          x
          
            T
          
        
        
          |
        
        
          x
          
            0
          
        
        )
        ‖
        
          p
          
            θ
          
        
        (
        
          x
          
            T
          
        
        )
        )
        ]
        +
        C
      
    
    {\displaystyle L(\theta )=\sum _{t=1}^{T}E_{x_{t-1},x_{t}\sim q}[-\ln p_{\theta }(x_{t-1}|x_{t})]+E_{x_{0}\sim q}[D_{KL}(q(x_{T}|x_{0})\|p_{\theta }(x_{T}))]+C}
  
where 
  
    
      
        C
      
    
    {\displaystyle C}
  
 does not depend on the parameter, and thus can be ignored. Since 
  
    
      
        
          p
          
            θ
          
        
        (
        
          x
          
            T
          
        
        )
        =
        
          
            N
          
        
        (
        
          x
          
            T
          
        
        
          |
        
        0
        ,
        I
        )
      
    
    {\displaystyle p_{\theta }(x_{T})={\mathcal {N}}(x_{T}|0,I)}
  
 also does not depend on the parameter, the term 
  
    
      
        
          E
          
            
              x
              
                0
              
            
            ∼
            q
          
        
        [
        
          D
          
            K
            L
          
        
        (
        q
        (
        
          x
          
            T
          
        
        
          |
        
        
          x
          
            0
          
        
        )
        ‖
        
          p
          
            θ
          
        
        (
        
          x
          
            T
          
        
        )
        )
        ]
      
    
    {\displaystyle E_{x_{0}\sim q}[D_{KL}(q(x_{T}|x_{0})\|p_{\theta }(x_{T}))]}
  
 can also be ignored. This leaves just 
  
    
      
        L
        (
        θ
        )
        =
        
          ∑
          
            t
            =
            1
          
          
            T
          
        
        
          L
          
            t
          
        
      
    
    {\displaystyle L(\theta )=\sum _{t=1}^{T}L_{t}}
  
 with 
  
    
      
        
          L
          
            t
          
        
        =
        
          E
          
            
              x
              
                t
                −
                1
              
            
            ,
            
              x
              
                t
              
            
            ∼
            q
          
        
        [
        −
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        
          x
          
            t
            −
            1
          
        
        
          |
        
        
          x
          
            t
          
        
        )
        ]
      
    
    {\displaystyle L_{t}=E_{x_{t-1},x_{t}\sim q}[-\ln p_{\theta }(x_{t-1}|x_{t})]}
  
 to be minimized.

## Denoising diffusion model → Denoising Diffusion Probabilistic Model (DDPM) → Noise prediction network

Since 
  
    
      
        
          x
          
            t
            −
            1
          
        
        
          |
        
        
          x
          
            t
          
        
        ,
        
          x
          
            0
          
        
        ∼
        
          
            N
          
        
        (
        
          
            
              
                μ
                ~
              
            
          
          
            t
          
        
        (
        
          x
          
            t
          
        
        ,
        
          x
          
            0
          
        
        )
        ,
        
          
            
              
                σ
                ~
              
            
          
          
            t
          
          
            2
          
        
        I
        )
      
    
    {\displaystyle x_{t-1}|x_{t},x_{0}\sim {\mathcal {N}}({\tilde {\mu }}_{t}(x_{t},x_{0}),{\tilde {\sigma }}_{t}^{2}I)}
  
, this suggests that we should use 
  
    
      
        
          μ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        =
        
          
            
              
                μ
                ~
              
            
          
          
            t
          
        
        (
        
          x
          
            t
          
        
        ,
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle \mu _{\theta }(x_{t},t)={\tilde {\mu }}_{t}(x_{t},x_{0})}
  
; however, the network does not have access to 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
  
, and so it has to estimate it instead. Now, since 
  
    
      
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
        ∼
        N
        
          (
          
            
              
                
                  
                    
                      
                        α
                        ¯
                      
                    
                  
                  
                    t
                  
                
              
            
            
              x
              
                0
              
            
            ,
            
              σ
              
                t
              
              
                2
              
            
            I
          
          )
        
      
    
    {\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}
  
, we may write 
  
    
      
        
          x
          
            t
          
        
        =
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
        
          x
          
            0
          
        
        +
        
          σ
          
            t
          
        
        z
      
    
    {\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+\sigma _{t}z}
  
, where 
  
    
      
        z
      
    
    {\displaystyle z}
  
 is some unknown gaussian noise. Now we see that estimating 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
  
 is equivalent to estimating 
  
    
      
        z
      
    
    {\displaystyle z}
  
.
Therefore, let the network output a noise vector 
  
    
      
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \epsilon _{\theta }(x_{t},t)}
  
, and let it predict
  
    
      
        
          μ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        =
        
          
            
              
                μ
                ~
              
            
          
          
            t
          
        
        
          (
          
            
              x
              
                t
              
            
            ,
            
              
                
                  
                    x
                    
                      t
                    
                  
                  −
                  
                    σ
                    
                      t
                    
                  
                  
                    ϵ
                    
                      θ
                    
                  
                  (
                  
                    x
                    
                      t
                    
                  
                  ,
                  t
                  )
                
                
                  
                    
                      
                        
                          α
                          ¯
                        
                      
                    
                    
                      t
                    
                  
                
              
            
          
          )
        
        =
        
          
            
              
                x
                
                  t
                
              
              −
              
                ϵ
                
                  θ
                
              
              (
              
                x
                
                  t
                
              
              ,
              t
              )
              
                β
                
                  t
                
              
              
                /
              
              
                σ
                
                  t
                
              
            
            
              
                α
                
                  t
                
              
            
          
        
      
    
    {\displaystyle \mu _{\theta }(x_{t},t)={\tilde {\mu }}_{t}\left(x_{t},{\frac {x_{t}-\sigma _{t}\epsilon _{\theta }(x_{t},t)}{\sqrt {{\bar {\alpha }}_{t}}}}\right)={\frac {x_{t}-\epsilon _{\theta }(x_{t},t)\beta _{t}/\sigma _{t}}{\sqrt {\alpha _{t}}}}}
  
It remains to design 
  
    
      
        
          Σ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \Sigma _{\theta }(x_{t},t)}
  
. The DDPM paper suggested not learning it (since it resulted in "unstable training and poorer sample quality"), but fixing it at some value 
  
    
      
        
          Σ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        =
        
          ζ
          
            t
          
          
            2
          
        
        I
      
    
    {\displaystyle \Sigma _{\theta }(x_{t},t)=\zeta _{t}^{2}I}
  
, where either 
  
    
      
        
          ζ
          
            t
          
          
            2
          
        
        =
        
          β
          
            t
          
        
        
           or 
        
        
          
            
              
                σ
                ~
              
            
          
          
            t
          
          
            2
          
        
      
    
    {\displaystyle \zeta _{t}^{2}=\beta _{t}{\text{ or }}{\tilde {\sigma }}_{t}^{2}}
  
 yielded similar performance.
With this, the loss simplifies to 
  
    
      
        
          L
          
            t
          
        
        =
        
          
            
              β
              
                t
              
              
                2
              
            
            
              2
              
                α
                
                  t
                
              
              
                σ
                
                  t
                
                
                  2
                
              
              
                ζ
                
                  t
                
                
                  2
                
              
            
          
        
        
          E
          
            
              x
              
                0
              
            
            ∼
            q
            ;
            z
            ∼
            
              
                N
              
            
            (
            0
            ,
            I
            )
          
        
        
          [
          
            
              ‖
              
                
                  ϵ
                  
                    θ
                  
                
                (
                
                  x
                  
                    t
                  
                
                ,
                t
                )
                −
                z
              
              ‖
            
            
              2
            
          
          ]
        
        +
        C
      
    
    {\displaystyle L_{t}={\frac {\beta _{t}^{2}}{2\alpha _{t}\sigma _{t}^{2}\zeta _{t}^{2}}}E_{x_{0}\sim q;z\sim {\mathcal {N}}(0,I)}\left[\left\|\epsilon _{\theta }(x_{t},t)-z\right\|^{2}\right]+C}
  
which may be minimized by stochastic gradient descent. The paper noted empirically that an even simpler loss function
  
    
      
        
          L
          
            s
            i
            m
            p
            l
            e
            ,
            t
          
        
        =
        
          E
          
            
              x
              
                0
              
            
            ∼
            q
            ;
            z
            ∼
            
              
                N
              
            
            (
            0
            ,
            I
            )
          
        
        
          [
          
            
              ‖
              
                
                  ϵ
                  
                    θ
                  
                
                (
                
                  x
                  
                    t
                  
                
                ,
                t
                )
                −
                z
              
              ‖
            
            
              2
            
          
          ]
        
      
    
    {\displaystyle L_{simple,t}=E_{x_{0}\sim q;z\sim {\mathcal {N}}(0,I)}\left[\left\|\epsilon _{\theta }(x_{t},t)-z\right\|^{2}\right]}
  
resulted in better models.

## Denoising diffusion model → Backward diffusion process

After a noise prediction network is trained, it can be used for generating data points in the original distribution in a loop as follows:

Compute the noise estimate 
  
    
      
        ϵ
        ←
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \epsilon \leftarrow \epsilon _{\theta }(x_{t},t)}
  

Compute the original data estimate 
  
    
      
        
          
            
              
                x
                ~
              
            
          
          
            0
          
        
        ←
        (
        
          x
          
            t
          
        
        −
        
          σ
          
            t
          
        
        ϵ
        )
        
          /
        
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
      
    
    {\displaystyle {\tilde {x}}_{0}\leftarrow (x_{t}-\sigma _{t}\epsilon )/{\sqrt {{\bar {\alpha }}_{t}}}}
  

Sample the previous data 
  
    
      
        
          x
          
            t
            −
            1
          
        
        ∼
        
          
            N
          
        
        (
        
          
            
              
                μ
                ~
              
            
          
          
            t
          
        
        (
        
          x
          
            t
          
        
        ,
        
          
            
              
                x
                ~
              
            
          
          
            0
          
        
        )
        ,
        
          
            
              
                σ
                ~
              
            
          
          
            t
          
          
            2
          
        
        I
        )
      
    
    {\displaystyle x_{t-1}\sim {\mathcal {N}}({\tilde {\mu }}_{t}(x_{t},{\tilde {x}}_{0}),{\tilde {\sigma }}_{t}^{2}I)}
  

Change time 
  
    
      
        t
        ←
        t
        −
        1
      
    
    {\displaystyle t\leftarrow t-1}

## Score-based generative model → Score matching → The idea of score functions

Consider the problem of image generation. Let 
  
    
      
        x
      
    
    {\displaystyle x}
  
 represent an image, and let 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
  
 be the probability distribution over all possible images. If we have 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
  
 itself, then we can say for certain how likely a certain image is. However, this is intractable in general.
Most often, we are uninterested in knowing the absolute probability of a certain image. Instead, we are usually only interested in knowing how likely a certain image is compared to its immediate neighbors — e.g. how much more likely is an image of cat compared to some small variants of it? Is it more likely if the image contains two whiskers, or three, or with some Gaussian noise added?
Consequently, we are actually quite uninterested in 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
  
 itself, but rather, 
  
    
      
        
          ∇
          
            x
          
        
        ln
        ⁡
        q
        (
        x
        )
      
    
    {\displaystyle \nabla _{x}\ln q(x)}
  
. This has two major effects:

One, we no longer need to normalize 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
  
, but can use any 
  
    
      
        
          
            
              q
              ~
            
          
        
        (
        x
        )
        =
        C
        q
        (
        x
        )
      
    
    {\displaystyle {\tilde {q}}(x)=Cq(x)}
  
, where 
  
    
      
        C
        =
        ∫
        
          
            
              q
              ~
            
          
        
        (
        x
        )
        d
        x
        >
        0
      
    
    {\displaystyle C=\int {\tilde {q}}(x)dx>0}
  
 is any unknown constant that is of no concern to us.
Two, we are comparing 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
  
 neighbors 
  
    
      
        q
        (
        x
        +
        d
        x
        )
      
    
    {\displaystyle q(x+dx)}
  
, by 
  
    
      
        
          
            
              q
              (
              x
              )
            
            
              q
              (
              x
              +
              d
              x
              )
            
          
        
        =
        
          e
          
            −
            ⟨
            
              ∇
              
                x
              
            
            ln
            ⁡
            q
            ,
            d
            x
            ⟩
          
        
      
    
    {\displaystyle {\frac {q(x)}{q(x+dx)}}=e^{-\langle \nabla _{x}\ln q,dx\rangle }}
  

Let the score function be 
  
    
      
        s
        (
        x
        )
        :=
        
          ∇
          
            x
          
        
        ln
        ⁡
        q
        (
        x
        )
      
    
    {\displaystyle s(x):=\nabla _{x}\ln q(x)}
  
; then consider what we can do with 
  
    
      
        s
        (
        x
        )
      
    
    {\displaystyle s(x)}
  
.
As it turns out, 
  
    
      
        s
        (
        x
        )
      
    
    {\displaystyle s(x)}
  
 allows us to sample from 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
  
 using thermodynamics. Specifically, if we have a potential energy function 
  
    
      
        U
        (
        x
        )
        =
        −
        ln
        ⁡
        q
        (
        x
        )
      
    
    {\displaystyle U(x)=-\ln q(x)}
  
, and a lot of particles in the potential well, then the distribution at thermodynamic equilibrium is the Boltzmann distribution 
  
    
      
        
          q
          
            U
          
        
        (
        x
        )
        ∝
        
          e
          
            −
            U
            (
            x
            )
            
              /
            
            
              k
              
                B
              
            
            T
          
        
        =
        q
        (
        x
        
          )
          
            1
            
              /
            
            
              k
              
                B
              
            
            T
          
        
      
    
    {\displaystyle q_{U}(x)\propto e^{-U(x)/k_{B}T}=q(x)^{1/k_{B}T}}
  
. At temperature 
  
    
      
        
          k
          
            B
          
        
        T
        =
        1
      
    
    {\displaystyle k_{B}T=1}
  
, the Boltzmann distribution is exactly 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
  
.
Therefore, to model 
  
    
      
        q
        (
        x
        )
      
    
    {\displaystyle q(x)}
  
, we may start with a particle sampled at any convenient distribution (such as the standard gaussian distribution), then simulate the motion of the particle forwards according to the Langevin equation

  
    
      
        d
        
          x
          
            t
          
        
        =
        −
        
          ∇
          
            
              x
              
                t
              
            
          
        
        U
        (
        
          x
          
            t
          
        
        )
        d
        t
        +
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle dx_{t}=-\nabla _{x_{t}}U(x_{t})dt+dW_{t}}
  

and the Boltzmann distribution is, by Fokker-Planck equation, the unique thermodynamic equilibrium. So no matter what distribution 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
  
 has, the distribution of 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
 converges in distribution to 
  
    
      
        q
      
    
    {\displaystyle q}
  
 as 
  
    
      
        t
        →
        ∞
      
    
    {\displaystyle t\to \infty }
  
.

## Score-based generative model → Score matching → Learning the score function

Given a density 
  
    
      
        q
      
    
    {\displaystyle q}
  
, we wish to learn a score function approximation 
  
    
      
        
          f
          
            θ
          
        
        ≈
        ∇
        ln
        ⁡
        q
      
    
    {\displaystyle f_{\theta }\approx \nabla \ln q}
  
. This is score matching. Typically, score matching is formalized as minimizing Fisher divergence function 
  
    
      
        
          E
          
            q
          
        
        [
        ‖
        
          f
          
            θ
          
        
        (
        x
        )
        −
        ∇
        ln
        ⁡
        q
        (
        x
        )
        
          ‖
          
            2
          
        
        ]
      
    
    {\displaystyle E_{q}[\|f_{\theta }(x)-\nabla \ln q(x)\|^{2}]}
  
. By expanding the integral, and performing an integration by parts, 
  
    
      
        
          E
          
            q
          
        
        [
        ‖
        
          f
          
            θ
          
        
        (
        x
        )
        −
        ∇
        ln
        ⁡
        q
        (
        x
        )
        
          ‖
          
            2
          
        
        ]
        =
        
          E
          
            q
          
        
        [
        ‖
        
          f
          
            θ
          
        
        
          ‖
          
            2
          
        
        +
        2
        ∇
        ⋅
        
          f
          
            θ
          
        
        ]
        +
        C
      
    
    {\displaystyle E_{q}[\|f_{\theta }(x)-\nabla \ln q(x)\|^{2}]=E_{q}[\|f_{\theta }\|^{2}+2\nabla \cdot f_{\theta }]+C}
  
giving us a loss function, also known as the Hyvärinen scoring rule, that can be minimized by stochastic gradient descent.

## Score-based generative model → Score matching → Annealing the score function

Suppose we need to model the distribution of images, and we want 
  
    
      
        
          x
          
            0
          
        
        ∼
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle x_{0}\sim {\mathcal {N}}(0,I)}
  
, a white-noise image. Now, most white-noise images do not look like real images, so 
  
    
      
        q
        (
        
          x
          
            0
          
        
        )
        ≈
        0
      
    
    {\displaystyle q(x_{0})\approx 0}
  
 for large swaths of 
  
    
      
        
          x
          
            0
          
        
        ∼
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle x_{0}\sim {\mathcal {N}}(0,I)}
  
. This presents a problem for learning the score function, because if there are no samples around a certain point, then we can't learn the score function at that point. If we do not know the score function 
  
    
      
        
          ∇
          
            
              x
              
                t
              
            
          
        
        ln
        ⁡
        q
        (
        
          x
          
            t
          
        
        )
      
    
    {\displaystyle \nabla _{x_{t}}\ln q(x_{t})}
  
 at that point, then we cannot impose the time-evolution equation on a particle:
  
    
      
        d
        
          x
          
            t
          
        
        =
        
          ∇
          
            
              x
              
                t
              
            
          
        
        ln
        ⁡
        q
        (
        
          x
          
            t
          
        
        )
        d
        t
        +
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle dx_{t}=\nabla _{x_{t}}\ln q(x_{t})dt+dW_{t}}
  
To deal with this problem, we perform annealing. If 
  
    
      
        q
      
    
    {\displaystyle q}
  
 is too different from a white-noise distribution, then progressively add noise until it is indistinguishable from one. That is, we perform a forward diffusion, then learn the score function, then use the score function to perform a backward diffusion.

## Score-based generative model → Continuous diffusion processes → Forward diffusion process

Consider again the forward diffusion process, but this time in continuous time:
  
    
      
        
          x
          
            t
          
        
        =
        
          
            1
            −
            
              β
              
                t
              
            
          
        
        
          x
          
            t
            −
            1
          
        
        +
        
          
            
              β
              
                t
              
            
          
        
        
          z
          
            t
          
        
      
    
    {\displaystyle x_{t}={\sqrt {1-\beta _{t}}}x_{t-1}+{\sqrt {\beta _{t}}}z_{t}}
  
By taking the 
  
    
      
        
          β
          
            t
          
        
        →
        β
        (
        t
        )
        d
        t
        ,
        
          
            d
            t
          
        
        
          z
          
            t
          
        
        →
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle \beta _{t}\to \beta (t)dt,{\sqrt {dt}}z_{t}\to dW_{t}}
  
 limit, we obtain a continuous diffusion process, in the form of a stochastic differential equation:
  
    
      
        d
        
          x
          
            t
          
        
        =
        −
        
          
            1
            2
          
        
        β
        (
        t
        )
        
          x
          
            t
          
        
        d
        t
        +
        
          
            β
            (
            t
            )
          
        
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle dx_{t}=-{\frac {1}{2}}\beta (t)x_{t}dt+{\sqrt {\beta (t)}}dW_{t}}
  
where 
  
    
      
        
          W
          
            t
          
        
      
    
    {\displaystyle W_{t}}
  
 is a Wiener process (multidimensional Brownian motion).
Now, the equation is exactly a special case of the overdamped Langevin equation
  
    
      
        d
        
          x
          
            t
          
        
        =
        −
        
          
            D
            
              
                k
                
                  B
                
              
              T
            
          
        
        (
        
          ∇
          
            x
          
        
        U
        )
        d
        t
        +
        
          
            2
            D
          
        
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle dx_{t}=-{\frac {D}{k_{B}T}}(\nabla _{x}U)dt+{\sqrt {2D}}dW_{t}}
  
where 
  
    
      
        D
      
    
    {\displaystyle D}
  
 is diffusion tensor, 
  
    
      
        T
      
    
    {\displaystyle T}
  
 is temperature, and 
  
    
      
        U
      
    
    {\displaystyle U}
  
 is potential energy field. If we substitute in 
  
    
      
        D
        =
        
          
            1
            2
          
        
        β
        (
        t
        )
        I
        ,
        
          k
          
            B
          
        
        T
        =
        1
        ,
        U
        =
        
          
            1
            2
          
        
        ‖
        x
        
          ‖
          
            2
          
        
      
    
    {\displaystyle D={\frac {1}{2}}\beta (t)I,k_{B}T=1,U={\frac {1}{2}}\|x\|^{2}}
  
, we recover the above equation. This explains why the phrase "Langevin dynamics" is sometimes used in diffusion models.
Now the above equation is for the stochastic motion of a single particle. Suppose we have a cloud of particles distributed according to 
  
    
      
        q
      
    
    {\displaystyle q}
  
 at time 
  
    
      
        t
        =
        0
      
    
    {\displaystyle t=0}
  
, then after a long time, the cloud of particles would settle into the stable distribution of 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle {\mathcal {N}}(0,I)}
  
. Let 
  
    
      
        
          ρ
          
            t
          
        
      
    
    {\displaystyle \rho _{t}}
  
 be the density of the cloud of particles at time 
  
    
      
        t
      
    
    {\displaystyle t}
  
, then we have
  
    
      
        
          ρ
          
            0
          
        
        =
        q
        ;
        
        
          ρ
          
            T
          
        
        ≈
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle \rho _{0}=q;\quad \rho _{T}\approx {\mathcal {N}}(0,I)}
  
and the goal is to somehow reverse the process, so that we can start at the end and diffuse back to the beginning.
By Fokker-Planck equation, the density of the cloud evolves according to
  
    
      
        
          ∂
          
            t
          
        
        ln
        ⁡
        
          ρ
          
            t
          
        
        =
        
          
            1
            2
          
        
        β
        (
        t
        )
        
          (
          
            n
            +
            (
            x
            +
            ∇
            ln
            ⁡
            
              ρ
              
                t
              
            
            )
            ⋅
            ∇
            ln
            ⁡
            
              ρ
              
                t
              
            
            +
            Δ
            ln
            ⁡
            
              ρ
              
                t
              
            
          
          )
        
      
    
    {\displaystyle \partial _{t}\ln \rho _{t}={\frac {1}{2}}\beta (t)\left(n+(x+\nabla \ln \rho _{t})\cdot \nabla \ln \rho _{t}+\Delta \ln \rho _{t}\right)}
  
where 
  
    
      
        n
      
    
    {\displaystyle n}
  
 is the dimension of space, and 
  
    
      
        Δ
      
    
    {\displaystyle \Delta }
  
 is the Laplace operator. Equivalently,
  
    
      
        
          ∂
          
            t
          
        
        
          ρ
          
            t
          
        
        =
        
          
            1
            2
          
        
        β
        (
        t
        )
        (
        ∇
        ⋅
        (
        x
        
          ρ
          
            t
          
        
        )
        +
        Δ
        
          ρ
          
            t
          
        
        )
      
    
    {\displaystyle \partial _{t}\rho _{t}={\frac {1}{2}}\beta (t)(\nabla \cdot (x\rho _{t})+\Delta \rho _{t})}

## Score-based generative model → Continuous diffusion processes → Backward diffusion process

If we have solved 
  
    
      
        
          ρ
          
            t
          
        
      
    
    {\displaystyle \rho _{t}}
  
 for time 
  
    
      
        t
        ∈
        [
        0
        ,
        T
        ]
      
    
    {\displaystyle t\in [0,T]}
  
, then we can exactly reverse the evolution of the cloud. Suppose we start with another cloud of particles with density 
  
    
      
        
          ν
          
            0
          
        
        =
        
          ρ
          
            T
          
        
      
    
    {\displaystyle \nu _{0}=\rho _{T}}
  
, and let the particles in the cloud evolve according to

  
    
      
        d
        
          y
          
            t
          
        
        =
        
          
            1
            2
          
        
        β
        (
        T
        −
        t
        )
        
          y
          
            t
          
        
        d
        t
        +
        β
        (
        T
        −
        t
        )
        
          
            
              
                
                  ∇
                  
                    
                      y
                      
                        t
                      
                    
                  
                
                ln
                ⁡
                
                  ρ
                  
                    T
                    −
                    t
                  
                
                
                  (
                  
                    y
                    
                      t
                    
                  
                  )
                
              
              ⏟
            
          
          
            score function 
          
        
        d
        t
        +
        
          
            β
            (
            T
            −
            t
            )
          
        
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle dy_{t}={\frac {1}{2}}\beta (T-t)y_{t}dt+\beta (T-t)\underbrace {\nabla _{y_{t}}\ln \rho _{T-t}\left(y_{t}\right)} _{\text{score function }}dt+{\sqrt {\beta (T-t)}}dW_{t}}
  

then by plugging into the Fokker-Planck equation, we find that 
  
    
      
        
          ∂
          
            t
          
        
        
          ρ
          
            T
            −
            t
          
        
        =
        
          ∂
          
            t
          
        
        
          ν
          
            t
          
        
      
    
    {\displaystyle \partial _{t}\rho _{T-t}=\partial _{t}\nu _{t}}
  
. Thus this cloud of points is the original cloud, evolving backwards.

## Score-based generative model → Noise conditional score network (NCSN)

At the continuous limit, 

  
    
      
        
          
            
              
                α
                ¯
              
            
          
          
            t
          
        
        =
        (
        1
        −
        
          β
          
            1
          
        
        )
        ⋯
        (
        1
        −
        
          β
          
            t
          
        
        )
        =
        
          e
          
            
              ∑
              
                i
              
            
            ln
            ⁡
            (
            1
            −
            
              β
              
                i
              
            
            )
          
        
        →
        
          e
          
            −
            
              ∫
              
                0
              
              
                t
              
            
            β
            (
            t
            )
            d
            t
          
        
      
    
    {\displaystyle {\bar {\alpha }}_{t}=(1-\beta _{1})\cdots (1-\beta _{t})=e^{\sum _{i}\ln(1-\beta _{i})}\to e^{-\int _{0}^{t}\beta (t)dt}}
  

and so 

  
    
      
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
        ∼
        N
        
          (
          
            
              e
              
                −
                
                  
                    1
                    2
                  
                
                
                  ∫
                  
                    0
                  
                  
                    t
                  
                
                β
                (
                t
                )
                d
                t
              
            
            
              x
              
                0
              
            
            ,
            
              (
              
                1
                −
                
                  e
                  
                    −
                    
                      ∫
                      
                        0
                      
                      
                        t
                      
                    
                    β
                    (
                    t
                    )
                    d
                    t
                  
                
              
              )
            
            I
          
          )
        
      
    
    {\displaystyle x_{t}|x_{0}\sim N\left(e^{-{\frac {1}{2}}\int _{0}^{t}\beta (t)dt}x_{0},\left(1-e^{-\int _{0}^{t}\beta (t)dt}\right)I\right)}
  

In particular, we see that we can directly sample from any point in the continuous diffusion process without going through the intermediate steps, by first sampling 
  
    
      
        
          x
          
            0
          
        
        ∼
        q
        ,
        z
        ∼
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle x_{0}\sim q,z\sim {\mathcal {N}}(0,I)}
  
, then get 
  
    
      
        
          x
          
            t
          
        
        =
        
          e
          
            −
            
              
                1
                2
              
            
            
              ∫
              
                0
              
              
                t
              
            
            β
            (
            t
            )
            d
            t
          
        
        
          x
          
            0
          
        
        +
        
          (
          
            1
            −
            
              e
              
                −
                
                  ∫
                  
                    0
                  
                  
                    t
                  
                
                β
                (
                t
                )
                d
                t
              
            
          
          )
        
        z
      
    
    {\displaystyle x_{t}=e^{-{\frac {1}{2}}\int _{0}^{t}\beta (t)dt}x_{0}+\left(1-e^{-\int _{0}^{t}\beta (t)dt}\right)z}
  
. That is, we can quickly sample 
  
    
      
        
          x
          
            t
          
        
        ∼
        
          ρ
          
            t
          
        
      
    
    {\displaystyle x_{t}\sim \rho _{t}}
  
 for any 
  
    
      
        t
        ≥
        0
      
    
    {\displaystyle t\geq 0}
  
.
Now, define a certain probability distribution 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
 over 
  
    
      
        [
        0
        ,
        ∞
        )
      
    
    {\displaystyle [0,\infty )}
  
, then the score-matching loss function is defined as the expected Fisher divergence:

  
    
      
        L
        (
        θ
        )
        =
        
          E
          
            t
            ∼
            γ
            ,
            
              x
              
                t
              
            
            ∼
            
              ρ
              
                t
              
            
          
        
        [
        ‖
        
          f
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        
          ‖
          
            2
          
        
        +
        2
        ∇
        ⋅
        
          f
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        ]
      
    
    {\displaystyle L(\theta )=E_{t\sim \gamma ,x_{t}\sim \rho _{t}}[\|f_{\theta }(x_{t},t)\|^{2}+2\nabla \cdot f_{\theta }(x_{t},t)]}
  

After training, 
  
    
      
        
          f
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        ≈
        ∇
        ln
        ⁡
        
          ρ
          
            t
          
        
      
    
    {\displaystyle f_{\theta }(x_{t},t)\approx \nabla \ln \rho _{t}}
  
, so we can perform the backwards diffusion process by first sampling 
  
    
      
        
          x
          
            T
          
        
        ∼
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle x_{T}\sim {\mathcal {N}}(0,I)}
  
, then integrating the SDE from 
  
    
      
        t
        =
        T
      
    
    {\displaystyle t=T}
  
 to 
  
    
      
        t
        =
        0
      
    
    {\displaystyle t=0}
  
:

  
    
      
        
          x
          
            t
            −
            d
            t
          
        
        =
        
          x
          
            t
          
        
        +
        
          
            1
            2
          
        
        β
        (
        t
        )
        
          x
          
            t
          
        
        d
        t
        +
        β
        (
        t
        )
        
          f
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        d
        t
        +
        
          
            β
            (
            t
            )
          
        
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle x_{t-dt}=x_{t}+{\frac {1}{2}}\beta (t)x_{t}dt+\beta (t)f_{\theta }(x_{t},t)dt+{\sqrt {\beta (t)}}dW_{t}}
  

This may be done by any SDE integration method, such as Euler–Maruyama method.
The name "noise conditional score network" is explained thus:

"network", because 
  
    
      
        
          f
          
            θ
          
        
      
    
    {\displaystyle f_{\theta }}
  
 is implemented as a neural network.
"score", because the output of the network is interpreted as approximating the score function 
  
    
      
        ∇
        ln
        ⁡
        
          ρ
          
            t
          
        
      
    
    {\displaystyle \nabla \ln \rho _{t}}
  
.
"noise conditional", because 
  
    
      
        
          ρ
          
            t
          
        
      
    
    {\displaystyle \rho _{t}}
  
 is equal to 
  
    
      
        
          ρ
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
  
 blurred by an added gaussian noise that increases with time, and so the score function depends on the amount of noise added.

## Their equivalence

DDPM and score-based generative models are equivalent. This means that a network trained using DDPM can be used as a NCSN, and vice versa.
We know that 
  
    
      
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
        ∼
        N
        
          (
          
            
              
                
                  
                    
                      
                        α
                        ¯
                      
                    
                  
                  
                    t
                  
                
              
            
            
              x
              
                0
              
            
            ,
            
              σ
              
                t
              
              
                2
              
            
            I
          
          )
        
      
    
    {\displaystyle x_{t}|x_{0}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}x_{0},\sigma _{t}^{2}I\right)}
  
, so by Tweedie's formula, we have

  
    
      
        
          ∇
          
            
              x
              
                t
              
            
          
        
        ln
        ⁡
        q
        (
        
          x
          
            t
          
        
        )
        =
        
          
            1
            
              σ
              
                t
              
              
                2
              
            
          
        
        (
        −
        
          x
          
            t
          
        
        +
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
        
          E
          
            q
          
        
        [
        
          x
          
            0
          
        
        
          |
        
        
          x
          
            t
          
        
        ]
        )
      
    
    {\displaystyle \nabla _{x_{t}}\ln q(x_{t})={\frac {1}{\sigma _{t}^{2}}}(-x_{t}+{\sqrt {{\bar {\alpha }}_{t}}}E_{q}[x_{0}|x_{t}])}
  

As described previously, the DDPM loss function is 
  
    
      
        
          ∑
          
            t
          
        
        
          L
          
            s
            i
            m
            p
            l
            e
            ,
            t
          
        
      
    
    {\displaystyle \sum _{t}L_{simple,t}}
  
 with

  
    
      
        
          L
          
            s
            i
            m
            p
            l
            e
            ,
            t
          
        
        =
        
          E
          
            
              x
              
                0
              
            
            ∼
            q
            ;
            z
            ∼
            
              
                N
              
            
            (
            0
            ,
            I
            )
          
        
        
          [
          
            
              ‖
              
                
                  ϵ
                  
                    θ
                  
                
                (
                
                  x
                  
                    t
                  
                
                ,
                t
                )
                −
                z
              
              ‖
            
            
              2
            
          
          ]
        
      
    
    {\displaystyle L_{simple,t}=E_{x_{0}\sim q;z\sim {\mathcal {N}}(0,I)}\left[\left\|\epsilon _{\theta }(x_{t},t)-z\right\|^{2}\right]}
  

where 
  
    
      
        
          x
          
            t
          
        
        =
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
        
          x
          
            0
          
        
        +
        
          σ
          
            t
          
        
        z
      
    
    {\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+\sigma _{t}z}
  
. By a change of variables,

  
    
      
        
          L
          
            s
            i
            m
            p
            l
            e
            ,
            t
          
        
        =
        
          E
          
            
              x
              
                0
              
            
            ,
            
              x
              
                t
              
            
            ∼
            q
          
        
        
          [
          
            
              ‖
              
                
                  ϵ
                  
                    θ
                  
                
                (
                
                  x
                  
                    t
                  
                
                ,
                t
                )
                −
                
                  
                    
                      
                        x
                        
                          t
                        
                      
                      −
                      
                        
                          
                            
                              
                                
                                  α
                                  ¯
                                
                              
                            
                            
                              t
                            
                          
                        
                      
                      
                        x
                        
                          0
                        
                      
                    
                    
                      σ
                      
                        t
                      
                    
                  
                
              
              ‖
            
            
              2
            
          
          ]
        
        =
        
          E
          
            
              x
              
                t
              
            
            ∼
            q
            ,
            
              x
              
                0
              
            
            ∼
            q
            (
            ⋅
            
              |
            
            
              x
              
                t
              
            
            )
          
        
        
          [
          
            
              ‖
              
                
                  ϵ
                  
                    θ
                  
                
                (
                
                  x
                  
                    t
                  
                
                ,
                t
                )
                −
                
                  
                    
                      
                        x
                        
                          t
                        
                      
                      −
                      
                        
                          
                            
                              
                                
                                  α
                                  ¯
                                
                              
                            
                            
                              t
                            
                          
                        
                      
                      
                        x
                        
                          0
                        
                      
                    
                    
                      σ
                      
                        t
                      
                    
                  
                
              
              ‖
            
            
              2
            
          
          ]
        
      
    
    {\displaystyle L_{simple,t}=E_{x_{0},x_{t}\sim q}\left[\left\|\epsilon _{\theta }(x_{t},t)-{\frac {x_{t}-{\sqrt {{\bar {\alpha }}_{t}}}x_{0}}{\sigma _{t}}}\right\|^{2}\right]=E_{x_{t}\sim q,x_{0}\sim q(\cdot |x_{t})}\left[\left\|\epsilon _{\theta }(x_{t},t)-{\frac {x_{t}-{\sqrt {{\bar {\alpha }}_{t}}}x_{0}}{\sigma _{t}}}\right\|^{2}\right]}
  

and the term inside becomes a least squares regression, so if the network actually reaches the global minimum of loss, then we have 
  
    
      
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        =
        
          
            
              
                x
                
                  t
                
              
              −
              
                
                  
                    
                      
                        
                          α
                          ¯
                        
                      
                    
                    
                      t
                    
                  
                
              
              
                E
                
                  q
                
              
              [
              
                x
                
                  0
                
              
              
                |
              
              
                x
                
                  t
                
              
              ]
            
            
              σ
              
                t
              
            
          
        
        =
        −
        
          σ
          
            t
          
        
        
          ∇
          
            
              x
              
                t
              
            
          
        
        ln
        ⁡
        q
        (
        
          x
          
            t
          
        
        )
      
    
    {\displaystyle \epsilon _{\theta }(x_{t},t)={\frac {x_{t}-{\sqrt {{\bar {\alpha }}_{t}}}E_{q}[x_{0}|x_{t}]}{\sigma _{t}}}=-\sigma _{t}\nabla _{x_{t}}\ln q(x_{t})}
  

Thus, a score-based network predicts noise, and can be used for denoising.
Conversely, the continuous limit 
  
    
      
        
          x
          
            t
            −
            1
          
        
        =
        
          x
          
            t
            −
            d
            t
          
        
        ,
        
          β
          
            t
          
        
        =
        β
        (
        t
        )
        d
        t
        ,
        
          z
          
            t
          
        
        
          
            d
            t
          
        
        =
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle x_{t-1}=x_{t-dt},\beta _{t}=\beta (t)dt,z_{t}{\sqrt {dt}}=dW_{t}}
  
 of the backward equation

  
    
      
        
          x
          
            t
            −
            1
          
        
        =
        
          
            
              x
              
                t
              
            
            
              
                α
                
                  t
                
              
            
          
        
        −
        
          
            
              β
              
                t
              
            
            
              
                σ
                
                  t
                
              
              
                
                  
                    α
                    
                      t
                    
                  
                
              
            
          
        
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        +
        
          
            
              β
              
                t
              
            
          
        
        
          z
          
            t
          
        
        ;
        
        
          z
          
            t
          
        
        ∼
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle x_{t-1}={\frac {x_{t}}{\sqrt {\alpha _{t}}}}-{\frac {\beta _{t}}{\sigma _{t}{\sqrt {\alpha _{t}}}}}\epsilon _{\theta }(x_{t},t)+{\sqrt {\beta _{t}}}z_{t};\quad z_{t}\sim {\mathcal {N}}(0,I)}
  

gives us precisely the same equation as score-based diffusion:

  
    
      
        
          x
          
            t
            −
            d
            t
          
        
        =
        
          x
          
            t
          
        
        (
        1
        +
        β
        (
        t
        )
        d
        t
        
          /
        
        2
        )
        +
        β
        (
        t
        )
        
          ∇
          
            
              x
              
                t
              
            
          
        
        ln
        ⁡
        q
        (
        
          x
          
            t
          
        
        )
        d
        t
        +
        
          
            β
            (
            t
            )
          
        
        d
        
          W
          
            t
          
        
      
    
    {\displaystyle x_{t-dt}=x_{t}(1+\beta (t)dt/2)+\beta (t)\nabla _{x_{t}}\ln q(x_{t})dt+{\sqrt {\beta (t)}}dW_{t}}
  
Thus, at infinitesimal steps of DDPM, a denoising network performs score-based diffusion.

## Main variants → Noise schedule

In DDPM, the sequence of numbers 
  
    
      
        0
        =
        
          σ
          
            0
          
        
        <
        
          σ
          
            1
          
        
        <
        ⋯
        <
        
          σ
          
            T
          
        
        <
        1
      
    
    {\displaystyle 0=\sigma _{0}<\sigma _{1}<\cdots <\sigma _{T}<1}
  
 is called a (discrete time) noise schedule. In general, consider a strictly increasing monotonic function 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
 of type 
  
    
      
        
          R
        
        →
        (
        0
        ,
        1
        )
      
    
    {\displaystyle \mathbb {R} \to (0,1)}
  
, such as the sigmoid function. In that case, a noise schedule is a sequence of real numbers 
  
    
      
        
          λ
          
            1
          
        
        <
        
          λ
          
            2
          
        
        <
        ⋯
        <
        
          λ
          
            T
          
        
      
    
    {\displaystyle \lambda _{1}<\lambda _{2}<\cdots <\lambda _{T}}
  
. It then defines a sequence of noises 
  
    
      
        
          σ
          
            t
          
        
        :=
        σ
        (
        
          λ
          
            t
          
        
        )
      
    
    {\displaystyle \sigma _{t}:=\sigma (\lambda _{t})}
  
, which then derives the other quantities 
  
    
      
        
          β
          
            t
          
        
        =
        1
        −
        
          
            
              1
              −
              
                σ
                
                  t
                
                
                  2
                
              
            
            
              1
              −
              
                σ
                
                  t
                  −
                  1
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \beta _{t}=1-{\frac {1-\sigma _{t}^{2}}{1-\sigma _{t-1}^{2}}}}
  
.
In order to use arbitrary noise schedules, instead of training a noise prediction model 
  
    
      
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \epsilon _{\theta }(x_{t},t)}
  
, one trains 
  
    
      
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        
          σ
          
            t
          
        
        )
      
    
    {\displaystyle \epsilon _{\theta }(x_{t},\sigma _{t})}
  
.
Similarly, for the noise conditional score network, instead of training 
  
    
      
        
          f
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle f_{\theta }(x_{t},t)}
  
, one trains 
  
    
      
        
          f
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        
          σ
          
            t
          
        
        )
      
    
    {\displaystyle f_{\theta }(x_{t},\sigma _{t})}
  
.

## Main variants → Denoising Diffusion Implicit Model (DDIM)

The original DDPM method for generating images is slow, since the forward diffusion process usually takes 
  
    
      
        T
        ∼
        1000
      
    
    {\displaystyle T\sim 1000}
  
 to make the distribution of 
  
    
      
        
          x
          
            T
          
        
      
    
    {\displaystyle x_{T}}
  
 to appear close to gaussian. However this means the backward diffusion process also take 1000 steps. Unlike the forward diffusion process, which can skip steps as 
  
    
      
        
          x
          
            t
          
        
        
          |
        
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{t}|x_{0}}
  
 is gaussian for all 
  
    
      
        t
        ≥
        1
      
    
    {\displaystyle t\geq 1}
  
, the backward diffusion process does not allow skipping steps. For example, to sample 
  
    
      
        
          x
          
            t
            −
            2
          
        
        
          |
        
        
          x
          
            t
            −
            1
          
        
        ∼
        
          
            N
          
        
        (
        
          μ
          
            θ
          
        
        (
        
          x
          
            t
            −
            1
          
        
        ,
        t
        −
        1
        )
        ,
        
          Σ
          
            θ
          
        
        (
        
          x
          
            t
            −
            1
          
        
        ,
        t
        −
        1
        )
        )
      
    
    {\displaystyle x_{t-2}|x_{t-1}\sim {\mathcal {N}}(\mu _{\theta }(x_{t-1},t-1),\Sigma _{\theta }(x_{t-1},t-1))}
  
 requires the model to first sample 
  
    
      
        
          x
          
            t
            −
            1
          
        
      
    
    {\displaystyle x_{t-1}}
  
. Attempting to directly sample 
  
    
      
        
          x
          
            t
            −
            2
          
        
        
          |
        
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t-2}|x_{t}}
  
 would require us to marginalize out 
  
    
      
        
          x
          
            t
            −
            1
          
        
      
    
    {\displaystyle x_{t-1}}
  
, which is generally intractable.
DDIM is a method to take any model trained on DDPM loss, and use it to sample with some steps skipped, sacrificing an adjustable amount of quality. If we generate the Markovian chain case in DDPM to non-Markovian case, DDIM corresponds to the case that the reverse process has variance equals to 0. In other words, the reverse process (and also the forward process) is deterministic. When using fewer sampling steps, DDIM outperforms DDPM.
In detail, the DDIM sampling method is as follows. Start with the forward diffusion process 
  
    
      
        
          x
          
            t
          
        
        =
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
        
          x
          
            0
          
        
        +
        
          σ
          
            t
          
        
        ϵ
      
    
    {\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+\sigma _{t}\epsilon }
  
. Then, during the backward denoising process, given 
  
    
      
        
          x
          
            t
          
        
        ,
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle x_{t},\epsilon _{\theta }(x_{t},t)}
  
, the original data is estimated as 
  
    
      
        
          x
          
            0
          
          ′
        
        =
        
          
            
              
                x
                
                  t
                
              
              −
              
                σ
                
                  t
                
              
              
                ϵ
                
                  θ
                
              
              (
              
                x
                
                  t
                
              
              ,
              t
              )
            
            
              
                
                  
                    
                      α
                      ¯
                    
                  
                
                
                  t
                
              
            
          
        
      
    
    {\displaystyle x_{0}'={\frac {x_{t}-\sigma _{t}\epsilon _{\theta }(x_{t},t)}{\sqrt {{\bar {\alpha }}_{t}}}}}
  
then the backward diffusion process can jump to any step 
  
    
      
        0
        ≤
        s
        <
        t
      
    
    {\displaystyle 0\leq s<t}
  
, and the next denoised sample is 
  
    
      
        
          x
          
            s
          
        
        =
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                s
              
            
          
        
        
          x
          
            0
          
          ′
        
        +
        
          
            
              σ
              
                s
              
              
                2
              
            
            −
            (
            
              σ
              
                s
              
              ′
            
            
              )
              
                2
              
            
          
        
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        +
        
          σ
          
            s
          
          ′
        
        ϵ
      
    
    {\displaystyle x_{s}={\sqrt {{\bar {\alpha }}_{s}}}x_{0}'+{\sqrt {\sigma _{s}^{2}-(\sigma '_{s})^{2}}}\epsilon _{\theta }(x_{t},t)+\sigma _{s}'\epsilon }
  
where 
  
    
      
        
          σ
          
            s
          
          ′
        
      
    
    {\displaystyle \sigma _{s}'}
  
 is an arbitrary real number within the range 
  
    
      
        [
        0
        ,
        
          σ
          
            s
          
        
        ]
      
    
    {\displaystyle [0,\sigma _{s}]}
  
, and 
  
    
      
        ϵ
        ∼
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle \epsilon \sim {\mathcal {N}}(0,I)}
  
 is a newly sampled gaussian noise. If all 
  
    
      
        
          σ
          
            s
          
          ′
        
        =
        0
      
    
    {\displaystyle \sigma _{s}'=0}
  
, then the backward process becomes deterministic, and this special case of DDIM is also called "DDIM". The original paper noted that when the process is deterministic, samples generated with only 20 steps are already very similar to ones generated with 1000 steps on the high-level.
The original paper recommended defining a single "eta value" 
  
    
      
        η
        ∈
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \eta \in [0,1]}
  
, such that 
  
    
      
        
          σ
          
            s
          
          ′
        
        =
        η
        
          
            
              
                σ
                ~
              
            
          
          
            s
          
        
      
    
    {\displaystyle \sigma _{s}'=\eta {\tilde {\sigma }}_{s}}
  
. When 
  
    
      
        η
        =
        1
      
    
    {\displaystyle \eta =1}
  
, this is the original DDPM. When 
  
    
      
        η
        =
        0
      
    
    {\displaystyle \eta =0}
  
, this is the fully deterministic DDIM. For intermediate values, the process interpolates between them.
By the equivalence, the DDIM algorithm also applies for score-based diffusion models.

## Main variants → Latent diffusion model (LDM)

Since the diffusion model is a general method for modelling probability distributions, if one wants to model a distribution over images, one can first encode the images into a lower-dimensional space by an encoder, then use a diffusion model to model the distribution over encoded images. Then to generate an image, one can sample from the diffusion model, then use a decoder to decode it into an image.
The encoder-decoder pair is most often a variational autoencoder (VAE).

## Main variants → Architectural improvements

proposed various architectural improvements. For example, they proposed log-space interpolation during backward sampling. Instead of sampling from 
  
    
      
        
          x
          
            t
            −
            1
          
        
        ∼
        
          
            N
          
        
        (
        
          
            
              
                μ
                ~
              
            
          
          
            t
          
        
        (
        
          x
          
            t
          
        
        ,
        
          
            
              
                x
                ~
              
            
          
          
            0
          
        
        )
        ,
        
          
            
              
                σ
                ~
              
            
          
          
            t
          
          
            2
          
        
        I
        )
      
    
    {\displaystyle x_{t-1}\sim {\mathcal {N}}({\tilde {\mu }}_{t}(x_{t},{\tilde {x}}_{0}),{\tilde {\sigma }}_{t}^{2}I)}
  
, they recommended sampling from 
  
    
      
        
          
            N
          
        
        (
        
          
            
              
                μ
                ~
              
            
          
          
            t
          
        
        (
        
          x
          
            t
          
        
        ,
        
          
            
              
                x
                ~
              
            
          
          
            0
          
        
        )
        ,
        (
        
          σ
          
            t
          
          
            v
          
        
        
          
            
              
                σ
                ~
              
            
          
          
            t
          
          
            1
            −
            v
          
        
        
          )
          
            2
          
        
        I
        )
      
    
    {\displaystyle {\mathcal {N}}({\tilde {\mu }}_{t}(x_{t},{\tilde {x}}_{0}),(\sigma _{t}^{v}{\tilde {\sigma }}_{t}^{1-v})^{2}I)}
  
 for a learned parameter 
  
    
      
        v
      
    
    {\displaystyle v}
  
.
In the v-prediction formalism, the noising formula 
  
    
      
        
          x
          
            t
          
        
        =
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
        
          x
          
            0
          
        
        +
        
          
            1
            −
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
        
          ϵ
          
            t
          
        
      
    
    {\displaystyle x_{t}={\sqrt {{\bar {\alpha }}_{t}}}x_{0}+{\sqrt {1-{\bar {\alpha }}_{t}}}\epsilon _{t}}
  
 is reparameterised by an angle 
  
    
      
        
          ϕ
          
            t
          
        
      
    
    {\displaystyle \phi _{t}}
  
 such that 
  
    
      
        cos
        ⁡
        
          ϕ
          
            t
          
        
        =
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
      
    
    {\displaystyle \cos \phi _{t}={\sqrt {{\bar {\alpha }}_{t}}}}
  
 and a "velocity" defined by 
  
    
      
        cos
        ⁡
        
          ϕ
          
            t
          
        
        
          ϵ
          
            t
          
        
        −
        sin
        ⁡
        
          ϕ
          
            t
          
        
        
          x
          
            0
          
        
      
    
    {\displaystyle \cos \phi _{t}\epsilon _{t}-\sin \phi _{t}x_{0}}
  
. The network is trained to predict the velocity 
  
    
      
        
          
            
              
                v
                ^
              
            
          
          
            θ
          
        
      
    
    {\displaystyle {\hat {v}}_{\theta }}
  
, and denoising is by 
  
    
      
        
          x
          
            
              ϕ
              
                t
              
            
            −
            δ
          
        
        =
        cos
        ⁡
        (
        δ
        )
        
        
          x
          
            
              ϕ
              
                t
              
            
          
        
        −
        sin
        ⁡
        (
        δ
        )
        
          
            
              
                v
                ^
              
            
          
          
            θ
          
        
        
        (
        
          x
          
            
              ϕ
              
                t
              
            
          
        
        )
      
    
    {\displaystyle x_{\phi _{t}-\delta }=\cos(\delta )\;x_{\phi _{t}}-\sin(\delta ){\hat {v}}_{\theta }\;(x_{\phi _{t}})}
  
. This parameterization was found to improve performance, as the model can be trained to reach total noise (i.e. 
  
    
      
        
          ϕ
          
            t
          
        
        =
        
          90
          
            ∘
          
        
      
    
    {\displaystyle \phi _{t}=90^{\circ }}
  
) and then reverse it, whereas the standard parameterization never reaches total noise since 
  
    
      
        
          
            
              
                
                  
                    α
                    ¯
                  
                
              
              
                t
              
            
          
        
        >
        0
      
    
    {\displaystyle {\sqrt {{\bar {\alpha }}_{t}}}>0}
  
 is always true.

## Main variants → Classifier guidance → With temperature

The classifier-guided diffusion model samples from 
  
    
      
        p
        (
        x
        
          |
        
        y
        )
      
    
    {\displaystyle p(x|y)}
  
, which is concentrated around the maximum a posteriori estimate 
  
    
      
        arg
        ⁡
        
          max
          
            x
          
        
        p
        (
        x
        
          |
        
        y
        )
      
    
    {\displaystyle \arg \max _{x}p(x|y)}
  
. If we want to force the model to move towards the maximum likelihood estimate 
  
    
      
        arg
        ⁡
        
          max
          
            x
          
        
        p
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle \arg \max _{x}p(y|x)}
  
, we can use 

  
    
      
        
          p
          
            γ
          
        
        (
        x
        
          |
        
        y
        )
        ∝
        p
        (
        y
        
          |
        
        x
        
          )
          
            γ
          
        
        p
        (
        x
        )
      
    
    {\displaystyle p_{\gamma }(x|y)\propto p(y|x)^{\gamma }p(x)}
  

where 
  
    
      
        γ
        >
        0
      
    
    {\displaystyle \gamma >0}
  
 is interpretable as inverse temperature. In the context of diffusion models, it is usually called the guidance scale. A high 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
 would force the model to sample from a distribution concentrated around 
  
    
      
        arg
        ⁡
        
          max
          
            x
          
        
        p
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle \arg \max _{x}p(y|x)}
  
. This sometimes improves quality of generated images.
This gives a modification to the previous equation:
  
    
      
        
          ∇
          
            x
          
        
        ln
        ⁡
        
          p
          
            β
          
        
        (
        x
        
          |
        
        y
        )
        =
        
          ∇
          
            x
          
        
        ln
        ⁡
        p
        (
        x
        )
        +
        γ
        
          ∇
          
            x
          
        
        ln
        ⁡
        p
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle \nabla _{x}\ln p_{\beta }(x|y)=\nabla _{x}\ln p(x)+\gamma \nabla _{x}\ln p(y|x)}
  
For denoising models, it corresponds to
  
    
      
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        y
        ,
        t
        )
        =
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        −
        γ
        
          σ
          
            t
          
        
        
          ∇
          
            
              x
              
                t
              
            
          
        
        ln
        ⁡
        p
        (
        y
        
          |
        
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \epsilon _{\theta }(x_{t},y,t)=\epsilon _{\theta }(x_{t},t)-\gamma \sigma _{t}\nabla _{x_{t}}\ln p(y|x_{t},t)}

## Main variants → Classifier-free guidance (CFG)

If we do not have a classifier 
  
    
      
        p
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle p(y|x)}
  
, we could still extract one out of the image model itself:

  
    
      
        
          ∇
          
            x
          
        
        ln
        ⁡
        
          p
          
            γ
          
        
        (
        x
        
          |
        
        y
        )
        =
        (
        1
        −
        γ
        )
        
          ∇
          
            x
          
        
        ln
        ⁡
        p
        (
        x
        )
        +
        γ
        
          ∇
          
            x
          
        
        ln
        ⁡
        p
        (
        x
        
          |
        
        y
        )
      
    
    {\displaystyle \nabla _{x}\ln p_{\gamma }(x|y)=(1-\gamma )\nabla _{x}\ln p(x)+\gamma \nabla _{x}\ln p(x|y)}
  

Such a model is usually trained by presenting it with both 
  
    
      
        (
        x
        ,
        y
        )
      
    
    {\displaystyle (x,y)}
  
 and 
  
    
      
        (
        x
        ,
        
          
            N
            o
            n
            e
          
        
        )
      
    
    {\displaystyle (x,{\rm {None}})}
  
, allowing it to model both 
  
    
      
        
          ∇
          
            x
          
        
        ln
        ⁡
        p
        (
        x
        
          |
        
        y
        )
      
    
    {\displaystyle \nabla _{x}\ln p(x|y)}
  
 and 
  
    
      
        
          ∇
          
            x
          
        
        ln
        ⁡
        p
        (
        x
        )
      
    
    {\displaystyle \nabla _{x}\ln p(x)}
  
.
Note that for CFG, the diffusion model cannot be merely a generative model of the entire data distribution 
  
    
      
        
          ∇
          
            x
          
        
        ln
        ⁡
        p
        (
        x
        )
      
    
    {\displaystyle \nabla _{x}\ln p(x)}
  
. It must be a conditional generative model 
  
    
      
        
          ∇
          
            x
          
        
        ln
        ⁡
        p
        (
        x
        
          |
        
        y
        )
      
    
    {\displaystyle \nabla _{x}\ln p(x|y)}
  
. For example, in stable diffusion, the diffusion backbone takes as input both a noisy model 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
, a time 
  
    
      
        t
      
    
    {\displaystyle t}
  
, and a conditioning vector 
  
    
      
        y
      
    
    {\displaystyle y}
  
 (such as a vector encoding a text prompt), and produces a noise prediction 
  
    
      
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        y
        ,
        t
        )
      
    
    {\displaystyle \epsilon _{\theta }(x_{t},y,t)}
  
.
For denoising models, it corresponds to
  
    
      
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        y
        ,
        t
        ,
        γ
        )
        =
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        +
        γ
        (
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        y
        ,
        t
        )
        −
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
        )
      
    
    {\displaystyle \epsilon _{\theta }(x_{t},y,t,\gamma )=\epsilon _{\theta }(x_{t},t)+\gamma (\epsilon _{\theta }(x_{t},y,t)-\epsilon _{\theta }(x_{t},t))}
  
As sampled by DDIM, the algorithm can be written as
  
    
      
        
          
            
              
                
                  ϵ
                  
                    uncond
                  
                
              
              
                
                ←
                
                  ϵ
                  
                    θ
                  
                
                (
                
                  x
                  
                    t
                  
                
                ,
                t
                )
              
            
            
              
                
                  ϵ
                  
                    cond
                  
                
              
              
                
                ←
                
                  ϵ
                  
                    θ
                  
                
                (
                
                  x
                  
                    t
                  
                
                ,
                t
                ,
                c
                )
              
            
            
              
                
                  ϵ
                  
                    CFG
                  
                
              
              
                
                ←
                
                  ϵ
                  
                    uncond
                  
                
                +
                γ
                (
                
                  ϵ
                  
                    cond
                  
                
                −
                
                  ϵ
                  
                    uncond
                  
                
                )
              
            
            
              
                
                  x
                  
                    0
                  
                
              
              
                
                ←
                (
                
                  x
                  
                    t
                  
                
                −
                
                  σ
                  
                    t
                  
                
                
                  ϵ
                  
                    CFG
                  
                
                )
                
                  /
                
                
                  
                    1
                    −
                    
                      σ
                      
                        t
                      
                      
                        2
                      
                    
                  
                
              
            
            
              
                
                  x
                  
                    s
                  
                
              
              
                
                ←
                
                  
                    1
                    −
                    
                      σ
                      
                        s
                      
                      
                        2
                      
                    
                  
                
                
                  x
                  
                    0
                  
                
                +
                
                  
                    
                      σ
                      
                        s
                      
                      
                        2
                      
                    
                    −
                    (
                    
                      σ
                      
                        s
                      
                      ′
                    
                    
                      )
                      
                        2
                      
                    
                  
                
                
                  ϵ
                  
                    uncond
                  
                
                +
                
                  σ
                  
                    s
                  
                  ′
                
                ϵ
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\epsilon _{\text{uncond}}&\leftarrow \epsilon _{\theta }(x_{t},t)\\\epsilon _{\text{cond}}&\leftarrow \epsilon _{\theta }(x_{t},t,c)\\\epsilon _{\text{CFG}}&\leftarrow \epsilon _{\text{uncond}}+\gamma (\epsilon _{\text{cond}}-\epsilon _{\text{uncond}})\\x_{0}&\leftarrow (x_{t}-\sigma _{t}\epsilon _{\text{CFG}})/{\sqrt {1-\sigma _{t}^{2}}}\\x_{s}&\leftarrow {\sqrt {1-\sigma _{s}^{2}}}x_{0}+{\sqrt {\sigma _{s}^{2}-(\sigma _{s}')^{2}}}\epsilon _{\text{uncond}}+\sigma _{s}'\epsilon \\\end{aligned}}}
  
A similar technique applies to language model sampling. Also, if the unconditional generation 
  
    
      
        
          ϵ
          
            uncond
          
        
        ←
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \epsilon _{\text{uncond}}\leftarrow \epsilon _{\theta }(x_{t},t)}
  
 is replaced by 
  
    
      
        
          ϵ
          
            neg cond
          
        
        ←
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        ,
        
          c
          ′
        
        )
      
    
    {\displaystyle \epsilon _{\text{neg cond}}\leftarrow \epsilon _{\theta }(x_{t},t,c')}
  
, then it results in negative prompting, which pushes the generation away from 
  
    
      
        
          c
          ′
        
      
    
    {\displaystyle c'}
  
 condition.

## Main variants → Samplers

Given a diffusion model, one may regard it either as a continuous process, and sample from it by integrating a SDE, or one can regard it as a discrete process, and sample from it by iterating the discrete steps. The choice of the "noise schedule" 
  
    
      
        
          β
          
            t
          
        
      
    
    {\displaystyle \beta _{t}}
  
 can also affect the quality of samples. A noise schedule is a function that sends a natural number to a noise level: 
  
    
      
        t
        ↦
        
          β
          
            t
          
        
        ,
        
        t
        ∈
        {
        1
        ,
        2
        ,
        …
        }
        ,
        β
        ∈
        (
        0
        ,
        1
        )
      
    
    {\displaystyle t\mapsto \beta _{t},\quad t\in \{1,2,\dots \},\beta \in (0,1)}
  
A noise schedule is more often specified by a map 
  
    
      
        t
        ↦
        
          σ
          
            t
          
        
      
    
    {\displaystyle t\mapsto \sigma _{t}}
  
. The two definitions are equivalent, since 
  
    
      
        
          β
          
            t
          
        
        =
        1
        −
        
          
            
              1
              −
              
                σ
                
                  t
                
                
                  2
                
              
            
            
              1
              −
              
                σ
                
                  t
                  −
                  1
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \beta _{t}=1-{\frac {1-\sigma _{t}^{2}}{1-\sigma _{t-1}^{2}}}}
  
.
In the DDPM perspective, one can use the DDPM itself (with noise), or DDIM (with adjustable amount of noise). The case where one adds noise is sometimes called ancestral sampling. One can interpolate between noise and no noise. The amount of noise is denoted 
  
    
      
        η
      
    
    {\displaystyle \eta }
  
 ("eta value") in the DDIM paper, with 
  
    
      
        η
        =
        0
      
    
    {\displaystyle \eta =0}
  
 denoting no noise (as in deterministic DDIM), and 
  
    
      
        η
        =
        1
      
    
    {\displaystyle \eta =1}
  
 denoting full noise (as in DDPM).
In the perspective of SDE, one can use any of the numerical integration methods, such as Euler–Maruyama method, Heun's method, linear multistep methods, etc. Just as in the discrete case, one can add an adjustable amount of noise during the integration.
A survey and comparison of samplers in the context of image generation is in.

## Main variants → Other examples

Notable variants include Poisson flow generative model, consistency model, critically-damped Langevin diffusion, GenPhys, cold diffusion, discrete diffusion, etc.

## Flow-based diffusion model → Optimal transport flow

The idea of optimal transport flow  is to construct a probability path minimizing the Wasserstein metric. The distribution on which we condition is an approximation of the optimal transport plan between 
  
    
      
        
          π
          
            0
          
        
      
    
    {\displaystyle \pi _{0}}
  
 and 
  
    
      
        
          π
          
            1
          
        
      
    
    {\displaystyle \pi _{1}}
  
: 
  
    
      
        z
        =
        (
        
          x
          
            0
          
        
        ,
        
          x
          
            1
          
        
        )
      
    
    {\displaystyle z=(x_{0},x_{1})}
  
 and 
  
    
      
        q
        (
        z
        )
        =
        Γ
        (
        
          π
          
            0
          
        
        ,
        
          π
          
            1
          
        
        )
      
    
    {\displaystyle q(z)=\Gamma (\pi _{0},\pi _{1})}
  
, where 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  
 is the optimal transport plan, which can be approximated by mini-batch optimal transport. If the batch size is not large, then the transport it computes can be very far from the true optimal transport.

## Flow-based diffusion model → Rectified flow

The idea of rectified flow is to learn a flow model such that the velocity is nearly constant along each flow path. This is beneficial, because we can integrate along such a vector field with very few steps. For example, if an ODE 
  
    
      
        
          
            
              
                ϕ
                
                  t
                
              
              ˙
            
          
        
        (
        x
        )
        =
        
          v
          
            t
          
        
        (
        
          ϕ
          
            t
          
        
        (
        x
        )
        )
      
    
    {\displaystyle {\dot {\phi _{t}}}(x)=v_{t}(\phi _{t}(x))}
  
 follows perfectly straight paths, it simplifies to 
  
    
      
        
          ϕ
          
            t
          
        
        (
        x
        )
        =
        
          x
          
            0
          
        
        +
        t
        ⋅
        
          v
          
            0
          
        
        (
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle \phi _{t}(x)=x_{0}+t\cdot v_{0}(x_{0})}
  
, allowing for exact solutions in one step. In practice, we cannot reach such perfection, but when the flow field is nearly so, we can take a few large steps instead of many little steps.  

The general idea is to start with two distributions 
  
    
      
        
          π
          
            0
          
        
      
    
    {\displaystyle \pi _{0}}
  
 and 
  
    
      
        
          π
          
            1
          
        
      
    
    {\displaystyle \pi _{1}}
  
, then construct a flow field 
  
    
      
        
          ϕ
          
            0
          
        
        =
        {
        
          ϕ
          
            t
          
        
        :
        t
        ∈
        [
        0
        ,
        1
        ]
        }
      
    
    {\displaystyle \phi ^{0}=\{\phi _{t}:t\in [0,1]\}}
  
 from it, then repeatedly apply a "reflow" operation to obtain successive flow fields 
  
    
      
        
          ϕ
          
            1
          
        
        ,
        
          ϕ
          
            2
          
        
        ,
        …
      
    
    {\displaystyle \phi ^{1},\phi ^{2},\dots }
  
, each straighter than the previous one. When the flow field is straight enough for the application, we stop.
Generally, for any time-differentiable process 
  
    
      
        
          ϕ
          
            t
          
        
      
    
    {\displaystyle \phi _{t}}
  
, 
  
    
      
        
          v
          
            t
          
        
      
    
    {\displaystyle v_{t}}
  
 can be estimated by solving:

  
    
      
        
          min
          
            θ
          
        
        
          ∫
          
            0
          
          
            1
          
        
        
          
            E
          
          
            x
            ∼
            
              p
              
                t
              
            
          
        
        
          [
          
            ‖
            
              
                v
                
                  t
                
              
              (
              x
              ,
              θ
              )
              −
              
                v
                
                  t
                
              
              (
              x
              )
            
            
              ‖
              
                2
              
            
          
          ]
        
        
        
          d
        
        t
        .
      
    
    {\displaystyle \min _{\theta }\int _{0}^{1}\mathbb {E} _{x\sim p_{t}}\left[\lVert {v_{t}(x,\theta )-v_{t}(x)}\rVert ^{2}\right]\,\mathrm {d} t.}
  

In rectified flow, by injecting strong priors that intermediate trajectories are straight, it can achieve both theoretical relevance for optimal transport  and computational efficiency, as ODEs with straight paths can be simulated precisely without time discretization.

Specifically, rectified flow seeks to match an ODE with the marginal distributions of the linear interpolation between points from distributions 
  
    
      
        
          π
          
            0
          
        
      
    
    {\displaystyle \pi _{0}}
  
 and 
  
    
      
        
          π
          
            1
          
        
      
    
    {\displaystyle \pi _{1}}
  
. Given observations 
  
    
      
        
          x
          
            0
          
        
        ∼
        
          π
          
            0
          
        
      
    
    {\displaystyle x_{0}\sim \pi _{0}}
  
 and 
  
    
      
        
          x
          
            1
          
        
        ∼
        
          π
          
            1
          
        
      
    
    {\displaystyle x_{1}\sim \pi _{1}}
  
, the canonical linear interpolation 
  
    
      
        
          x
          
            t
          
        
        =
        t
        
          x
          
            1
          
        
        +
        (
        1
        −
        t
        )
        
          x
          
            0
          
        
        ,
        t
        ∈
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle x_{t}=tx_{1}+(1-t)x_{0},t\in [0,1]}
  
 yields a trivial case 
  
    
      
        
          
            
              
                x
                ˙
              
            
          
          
            t
          
        
        =
        
          x
          
            1
          
        
        −
        
          x
          
            0
          
        
      
    
    {\displaystyle {\dot {x}}_{t}=x_{1}-x_{0}}
  
, which cannot be causally simulated without 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
  
. To address this, 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
 is "projected" into a space of causally simulatable ODEs, by minimizing the least squares loss with respect to the direction 
  
    
      
        
          x
          
            1
          
        
        −
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{1}-x_{0}}
  
:

  
    
      
        
          min
          
            θ
          
        
        
          ∫
          
            0
          
          
            1
          
        
        
          
            E
          
          
            
              π
              
                0
              
            
            ,
            
              π
              
                1
              
            
            ,
            
              p
              
                t
              
            
          
        
        
          [
          
            ‖
            
              (
              
                x
                
                  1
                
              
              −
              
                x
                
                  0
                
              
              )
              −
              
                v
                
                  t
                
              
              (
              
                x
                
                  t
                
              
              )
            
            
              ‖
              
                2
              
            
          
          ]
        
        
        
          d
        
        t
        .
      
    
    {\displaystyle \min _{\theta }\int _{0}^{1}\mathbb {E} _{\pi _{0},\pi _{1},p_{t}}\left[\lVert {(x_{1}-x_{0})-v_{t}(x_{t})}\rVert ^{2}\right]\,\mathrm {d} t.}
  

The data pair 
  
    
      
        (
        
          x
          
            0
          
        
        ,
        
          x
          
            1
          
        
        )
      
    
    {\displaystyle (x_{0},x_{1})}
  
 can be any coupling of 
  
    
      
        
          π
          
            0
          
        
      
    
    {\displaystyle \pi _{0}}
  
 and 
  
    
      
        
          π
          
            1
          
        
      
    
    {\displaystyle \pi _{1}}
  
, typically independent (i.e., 
  
    
      
        (
        
          x
          
            0
          
        
        ,
        
          x
          
            1
          
        
        )
        ∼
        
          π
          
            0
          
        
        ×
        
          π
          
            1
          
        
      
    
    {\displaystyle (x_{0},x_{1})\sim \pi _{0}\times \pi _{1}}
  
) obtained by randomly combining observations from 
  
    
      
        
          π
          
            0
          
        
      
    
    {\displaystyle \pi _{0}}
  
 and 
  
    
      
        
          π
          
            1
          
        
      
    
    {\displaystyle \pi _{1}}
  
. This process ensures that the trajectories closely mirror the density map of 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
 trajectories but reroute at intersections to ensure causality. This rectifying process is also known as Flow Matching, Stochastic Interpolation, and alpha-(de)blending.

A distinctive aspect of rectified flow is its capability for "reflow", which straightens the trajectory of ODE paths. Denote the rectified flow 
  
    
      
        
          ϕ
          
            0
          
        
        =
        {
        
          ϕ
          
            t
          
        
        :
        t
        ∈
        [
        0
        ,
        1
        ]
        }
      
    
    {\displaystyle \phi ^{0}=\{\phi _{t}:t\in [0,1]\}}
  
 induced from 
  
    
      
        (
        
          x
          
            0
          
        
        ,
        
          x
          
            1
          
        
        )
      
    
    {\displaystyle (x_{0},x_{1})}
  
 as 
  
    
      
        
          ϕ
          
            0
          
        
        =
        
          
            R
            e
            c
            t
            f
            l
            o
            w
          
        
        (
        (
        
          x
          
            0
          
        
        ,
        
          x
          
            1
          
        
        )
        )
      
    
    {\displaystyle \phi ^{0}={\mathsf {Rectflow}}((x_{0},x_{1}))}
  
. Recursively applying this 
  
    
      
        
          
            R
            e
            c
            t
            f
            l
            o
            w
          
        
        (
        ⋅
        )
      
    
    {\displaystyle {\mathsf {Rectflow}}(\cdot )}
  
 operator generates a series of rectified flows 
  
    
      
        
          ϕ
          
            k
            +
            1
          
        
        =
        
          
            R
            e
            c
            t
            f
            l
            o
            w
          
        
        (
        (
        
          ϕ
          
            0
          
          
            k
          
        
        (
        
          x
          
            0
          
        
        )
        ,
        
          ϕ
          
            1
          
          
            k
          
        
        (
        
          x
          
            1
          
        
        )
        )
        )
      
    
    {\displaystyle \phi ^{k+1}={\mathsf {Rectflow}}((\phi _{0}^{k}(x_{0}),\phi _{1}^{k}(x_{1})))}
  
. This "reflow" process not only reduces transport costs but also straightens the paths of rectified flows, making 
  
    
      
        
          ϕ
          
            k
          
        
      
    
    {\displaystyle \phi ^{k}}
  
 paths straighter with increasing 
  
    
      
        k
      
    
    {\displaystyle k}
  
.
Rectified flow includes a nonlinear extension where linear interpolation 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
 is replaced with any time-differentiable curve that connects 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
  
 and 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
  
, given by 
  
    
      
        
          x
          
            t
          
        
        =
        
          α
          
            t
          
        
        
          x
          
            1
          
        
        +
        
          β
          
            t
          
        
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{t}=\alpha _{t}x_{1}+\beta _{t}x_{0}}
  
. This framework encompasses DDIM and probability flow ODEs as special cases, with particular choices of 
  
    
      
        
          α
          
            t
          
        
      
    
    {\displaystyle \alpha _{t}}
  
 and 
  
    
      
        
          β
          
            t
          
        
      
    
    {\displaystyle \beta _{t}}
  
. However, in the case where the path of 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
 is not straight, the reflow process no longer ensures a reduction in convex transport costs, and also no longer straighten the paths of 
  
    
      
        
          ϕ
          
            t
          
        
      
    
    {\displaystyle \phi _{t}}
  
.
See  for a tutorial on flow matching, with animations.

## Choice of architecture → Diffusion model

For generating images by DDPM, we need a neural network that takes a time 
  
    
      
        t
      
    
    {\displaystyle t}
  
 and a noisy image 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
, and predicts a noise 
  
    
      
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle \epsilon _{\theta }(x_{t},t)}
  
 from it. Since predicting the noise is the same as predicting the denoised image, then subtracting it from 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
, denoising architectures tend to work well. For example, the U-Net, which was found to be good for denoising images, is often used for denoising diffusion models that generate images.
For DDPM, the underlying architecture ("backbone") does not have to be a U-Net. It just has to predict the noise somehow. For example, the diffusion transformer (DiT) uses a Transformer to predict the mean and diagonal covariance of the noise, given the textual conditioning and the partially denoised image. It is the same as standard U-Net-based denoising diffusion model, with a Transformer replacing the U-Net. Mixture of experts-Transformer can also be applied.
DDPM can be used to model general data distributions, not just natural-looking images. For example, Human Motion Diffusion models human motion trajectory by DDPM. Each human motion trajectory is a sequence of poses, represented by either joint rotations or positions. It uses a Transformer network to generate a less noisy trajectory out of a noisy one.

## Choice of architecture → Conditioning

The base diffusion model can only generate unconditionally from the whole distribution. For example, a diffusion model learned on ImageNet would generate images that look like a random image from ImageNet. To generate images from just one category, one would need to impose the condition, and then sample from the conditional distribution. Whatever condition one wants to impose, one needs to first convert the conditioning into a vector of floating point numbers, then feed it into the underlying diffusion model neural network. However, one has freedom in choosing how to convert the conditioning into a vector.
Stable Diffusion, for example, imposes conditioning in the form of cross-attention mechanism, where the query is an intermediate representation of the image in the U-Net, and both key and value are the conditioning vectors. The conditioning can be selectively applied to only parts of an image, and new kinds of conditionings can be finetuned upon the base model, as used in ControlNet.
As a particularly simple example, consider image inpainting. The conditions are 
  
    
      
        
          
            
              x
              ~
            
          
        
      
    
    {\displaystyle {\tilde {x}}}
  
, the reference image, and 
  
    
      
        m
      
    
    {\displaystyle m}
  
, the inpainting mask. The conditioning is imposed at each step of the backward diffusion process, by first sampling 
  
    
      
        
          
            
              
                x
                ~
              
            
          
          
            t
          
        
        ∼
        N
        
          (
          
            
              
                
                  
                    
                      
                        α
                        ¯
                      
                    
                  
                  
                    t
                  
                
              
            
            
              
                
                  x
                  ~
                
              
            
            ,
            
              σ
              
                t
              
              
                2
              
            
            I
          
          )
        
      
    
    {\displaystyle {\tilde {x}}_{t}\sim N\left({\sqrt {{\bar {\alpha }}_{t}}}{\tilde {x}},\sigma _{t}^{2}I\right)}
  
, a noisy version of 
  
    
      
        
          
            
              x
              ~
            
          
        
      
    
    {\displaystyle {\tilde {x}}}
  
, then replacing 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
 with 
  
    
      
        (
        1
        −
        m
        )
        ⊙
        
          x
          
            t
          
        
        +
        m
        ⊙
        
          
            
              
                x
                ~
              
            
          
          
            t
          
        
      
    
    {\displaystyle (1-m)\odot x_{t}+m\odot {\tilde {x}}_{t}}
  
, where 
  
    
      
        ⊙
      
    
    {\displaystyle \odot }
  
 means elementwise multiplication. Another application of cross-attention mechanism is prompt-to-prompt image editing.
Conditioning is not limited to just generating images from a specific category, or according to a specific caption (as in text-to-image). For example, demonstrated generating human motion, conditioned on an audio clip of human walking (allowing syncing motion to a soundtrack), or video of human running, or a text description of human motion, etc. For how conditional diffusion models are mathematically formulated, see a methodological summary in.

## Choice of architecture → Upscaling

As generating an image takes a long time, one can try to generate a small image by a base diffusion model, then upscale it by other models. Upscaling can be done by GAN, Transformer, or signal processing methods like Lanczos resampling.
Diffusion models themselves can be used to perform upscaling. Cascading diffusion model stacks multiple diffusion models one after another, in the style of Progressive GAN. The lowest level is a standard diffusion model that generate 32x32 image, then the image would be upscaled by a diffusion model specifically trained for upscaling, and the process repeats.
In more detail, the diffusion upscaler is trained as follows:

Sample 
  
    
      
        (
        
          x
          
            0
          
        
        ,
        
          z
          
            0
          
        
        ,
        c
        )
      
    
    {\displaystyle (x_{0},z_{0},c)}
  
, where 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
  
 is the high-resolution image, 
  
    
      
        
          z
          
            0
          
        
      
    
    {\displaystyle z_{0}}
  
 is the same image but scaled down to a low-resolution, and 
  
    
      
        c
      
    
    {\displaystyle c}
  
 is the conditioning, which can be the caption of the image, the class of the image, etc.
Sample two white noises 
  
    
      
        
          ϵ
          
            x
          
        
        ,
        
          ϵ
          
            z
          
        
      
    
    {\displaystyle \epsilon _{x},\epsilon _{z}}
  
, two time-steps 
  
    
      
        
          t
          
            x
          
        
        ,
        
          t
          
            z
          
        
      
    
    {\displaystyle t_{x},t_{z}}
  
. Compute the noisy versions of the high-resolution and low-resolution images: 
  
    
      
        
          
            {
            
              
                
                  
                    x
                    
                      
                        t
                        
                          x
                        
                      
                    
                  
                
                
                  =
                  
                    
                      
                        
                          
                            
                              α
                              ¯
                            
                          
                        
                        
                          
                            t
                            
                              x
                            
                          
                        
                      
                    
                  
                  
                    x
                    
                      0
                    
                  
                  +
                  
                    σ
                    
                      
                        t
                        
                          x
                        
                      
                    
                  
                  
                    ϵ
                    
                      x
                    
                  
                
              
              
                
                  
                    z
                    
                      
                        t
                        
                          z
                        
                      
                    
                  
                
                
                  =
                  
                    
                      
                        
                          
                            
                              α
                              ¯
                            
                          
                        
                        
                          
                            t
                            
                              z
                            
                          
                        
                      
                    
                  
                  
                    z
                    
                      0
                    
                  
                  +
                  
                    σ
                    
                      
                        t
                        
                          z
                        
                      
                    
                  
                  
                    ϵ
                    
                      z
                    
                  
                
              
            
            
          
        
      
    
    {\displaystyle {\begin{cases}x_{t_{x}}&={\sqrt {{\bar {\alpha }}_{t_{x}}}}x_{0}+\sigma _{t_{x}}\epsilon _{x}\\z_{t_{z}}&={\sqrt {{\bar {\alpha }}_{t_{z}}}}z_{0}+\sigma _{t_{z}}\epsilon _{z}\end{cases}}}
  
.
Train the denoising network to predict 
  
    
      
        
          ϵ
          
            x
          
        
      
    
    {\displaystyle \epsilon _{x}}
  
 given 
  
    
      
        
          x
          
            
              t
              
                x
              
            
          
        
        ,
        
          z
          
            
              t
              
                z
              
            
          
        
        ,
        
          t
          
            x
          
        
        ,
        
          t
          
            z
          
        
        ,
        c
      
    
    {\displaystyle x_{t_{x}},z_{t_{z}},t_{x},t_{z},c}
  
. That is, apply gradient descent on 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  
 on the L2 loss 
  
    
      
        ‖
        
          ϵ
          
            θ
          
        
        (
        
          x
          
            
              t
              
                x
              
            
          
        
        ,
        
          z
          
            
              t
              
                z
              
            
          
        
        ,
        
          t
          
            x
          
        
        ,
        
          t
          
            z
          
        
        ,
        c
        )
        −
        
          ϵ
          
            x
          
        
        
          ‖
          
            2
          
          
            2
          
        
      
    
    {\displaystyle \|\epsilon _{\theta }(x_{t_{x}},z_{t_{z}},t_{x},t_{z},c)-\epsilon _{x}\|_{2}^{2}}
  
.

## Examples → OpenAI

The DALL-E series by OpenAI are text-conditional diffusion models of images.
The first version of DALL-E (2021) is not actually a diffusion model. Instead, it uses a Transformer architecture that autoregressively generates a sequence of tokens, which is then converted to an image by the decoder of a discrete VAE. Released with DALL-E was the CLIP classifier, which was used by DALL-E to rank generated images according to how close the image fits the text.
GLIDE (2022-03) is a 3.5-billion diffusion model, and a small version was released publicly. Soon after, DALL-E 2 was released (2022-04). DALL-E 2 is a 3.5-billion cascaded diffusion model that generates images from text by "inverting the CLIP image encoder", the technique which they termed "unCLIP".
The unCLIP method contains 4 models: a CLIP image encoder, a CLIP text encoder, an image decoder, and a "prior" model (which can be a diffusion model, or an autoregressive model). During training, the prior model is trained to convert CLIP image encodings to CLIP text encodings. The image decoder is trained to convert CLIP image encodings back to images. During inference, a text is converted by the CLIP text encoder to a vector, then it is converted by the prior model to an image encoding, then it is converted by the image decoder to an image.
Sora (2024-02) is a diffusion Transformer model (DiT).

## Examples → Stability AI

Stable Diffusion (2022-08), released by Stability AI, consists of a denoising latent diffusion model (860 million parameters), a VAE, and a text encoder. The denoising network is a U-Net, with cross-attention blocks to allow for conditional image generation.
Stable Diffusion 3 (2024-03) changed the latent diffusion model from the UNet to a Transformer model, and so it is a DiT. It uses rectified flow.
Stable Video 4D (2024-07) is a latent diffusion model for videos of 3D objects.

## Examples → Google

Imagen (2022) uses a T5-XXL language model to encode the input text into an embedding vector. It is a cascaded diffusion model with three sub-models. The first step denoises a white noise to a 64×64 image, conditional on the embedding vector of the text. This model has 2B parameters. The second step upscales the image by 64×64→256×256, conditional on embedding. This model has 650M parameters. The third step is similar, upscaling by 256×256→1024×1024. This model has 400M parameters. The three denoising networks are all U-Nets.
Muse (2023-01) is not a diffusion model, but an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens.
Imagen 2 (2023-12) is also diffusion-based. It can generate images based on a prompt that mixes images and text. No further information available. Imagen 3 (2024-05) is too. No further information available.
Veo (2024) generates videos by latent diffusion. The diffusion is conditioned on a vector that encodes both a text prompt and an image prompt.

## Examples → Meta

Make-A-Video (2022) is a text-to-video diffusion model.
CM3leon (2023) is not a diffusion model, but an autoregressive causally masked Transformer, with mostly the same architecture as LLaMa-2.

Transfusion (2024) is a Transformer that combines autoregressive text generation and denoising diffusion. Specifically, it generates text autoregressively (with causal masking), and generates images by denoising multiple times over image tokens (with all-to-all attention).
Movie Gen (2024) is a series of Diffusion Transformers operating on latent space and by flow matching.



---

# Prompt engineering

## History

In 2018, researchers first proposed that all previously separate tasks in natural language processing (NLP) could be cast as a question-answering problem over a context. In addition, they trained a first single, joint, multi-task model that would answer any task-related question like "What is the sentiment" or "Translate this sentence to German" or "Who is the president?"
The AI boom saw an increase in the amount of "prompting technique" to get the model to output the desired outcome and avoid nonsensical output, a process characterized by trial-and-error. After the release of ChatGPT in 2022, prompt engineering was soon seen as an important business skill, albeit one with an uncertain economic future.
A repository for prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022. In 2022, the chain-of-thought prompting technique was proposed by Google researchers. In 2023, several text-to-text and text-to-image prompt databases were made publicly available. The Personalized Image-Prompt (PIP) dataset, a generated image-text dataset that has been categorized by 3,115 users, has also been made available publicly in 2024.

## Text-to-text → Chain-of-thought

According to Google Research, chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps before giving a final answer. In 2022, Google Brain reported that chain-of-thought prompting improves reasoning ability by inducing the model to answer a multi-step problem with steps of reasoning that mimic a train of thought. Chain-of-thought techniques were developed to help LLMs handle multi-step reasoning tasks, such as arithmetic or commonsense reasoning questions.
For example, given the question, "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?", Google claims that a CoT prompt might induce the LLM to answer "A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9." When applied to PaLM, a 540 billion parameter language model, according to Google, CoT prompting significantly aided the model, allowing it to perform comparably with task-specific fine-tuned models on several tasks, achieving state-of-the-art results at the time on the GSM8K mathematical reasoning benchmark. It is possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability.
An example of a CoT prompting:

   Q: {question}
   A: Let's think step by step.

As originally proposed by Google, each CoT prompt included a few Q&A examples. This made it a few-shot prompting technique. However, according to researchers at Google and the University of Tokyo, simply appending the words "Let's think step-by-step", has also proven effective, which makes CoT a zero-shot prompting technique. OpenAI claims that this prompt allows for better scaling as a user no longer needs to formulate many specific CoT Q&A examples.

## Text-to-text → In-context learning

In-context learning, refers to a model's ability to temporarily learn from prompts. For example, a prompt may include a few examples for a model to learn from, such as asking the model to complete "maison → house, chat → cat, chien →" (the expected response being dog), an approach called few-shot learning.
In-context learning is an emergent ability of large language models. It is an emergent property of model scale, meaning that breaks in downstream scaling laws occur, leading to its efficacy increasing at a different rate in larger models than in smaller models. Unlike training and fine-tuning, which produce lasting changes, in-context learning is temporary. Training models to perform in-context learning can be viewed as a form of meta-learning, or "learning to learn".

## Text-to-text → Self-consistency decoding

Self-consistency decoding performs several chain-of-thought rollouts, then selects the most commonly reached conclusion out of all the rollouts. If the rollouts disagree by a lot, a human can be queried for the correct chain of thought.

## Text-to-text → Tree-of-thought

Tree-of-thought prompting generalizes chain-of-thought by prompting the model to generate one or more "possible next steps", and then running the model on each of the possible next steps by breadth-first, beam, or some other method of tree search. The LLM has additional modules that can converse the history of the problem-solving process to the LLM, which allows the system to 'backtrack steps' the problem-solving process.

## Text-to-text → Prompting to disclose uncertainty

By default, the output of language models may not contain estimates of uncertainty. The model may output text that appears confident, though the underlying token predictions have low likelihood scores. Large language models like GPT-4 can have accurately calibrated likelihood scores in their token predictions, and so the model output uncertainty can be directly estimated by reading out the token prediction likelihood scores.

## Text-to-text → Prompting to estimate model sensitivity

Research consistently demonstrates that LLMs are highly sensitive to subtle variations in prompt formatting, structure, and linguistic properties. Some studies have shown up to 76 accuracy points across formatting changes in few-shot settings. Linguistic features significantly influence prompt effectiveness—such as morphology, syntax, and lexico-semantic changes—which meaningfully enhance task performance across a variety of tasks. Clausal syntax, for example, improves consistency and reduces uncertainty in knowledge retrieval. This sensitivity persists even with larger model sizes, additional few-shot examples, or instruction tuning.
To address sensitivity of models and make them more robust, several methods have been proposed. FormatSpread facilitates systematic analysis by evaluating a range of plausible prompt formats, offering a more comprehensive performance interval. Similarly, PromptEval estimates performance distributions across diverse prompts, enabling robust metrics such as performance quantiles and accurate evaluations under constrained budgets.

## Text-to-text → Automatic prompt generation → Retrieval-augmented generation

Retrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information.
RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, "RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts." This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases. By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining.

## Text-to-text → Automatic prompt generation → Graph retrieval-augmented generation

GraphRAG (coined by Microsoft Research) is a technique that extends RAG with the use of a knowledge graph (usually, LLM-generated) to allow the model to connect disparate pieces of information, synthesize insights, and holistically understand summarized semantic concepts over large data collections. It was shown to be effective on datasets like the Violent Incident Information from News Articles (VIINA).
Earlier work showed the effectiveness of using a knowledge graph for question answering using text-to-query generation. These techniques can be combined to search across both unstructured and structured data, providing expanded context, and improved ranking.

## Text-to-text → Automatic prompt generation → Using language models to generate prompts

Large language models (LLM) themselves can be used to compose prompts for large language models. The automatic prompt engineer algorithm uses one LLM to beam search over prompts for another LLM:

There are two LLMs. One is the target LLM, and another is the prompting LLM.
Prompting LLM is presented with example input-output pairs, and asked to generate instructions that could have caused a model following the instructions to generate the outputs, given the inputs.
Each of the generated instructions is used to prompt the target LLM, followed by each of the inputs. The log-probabilities of the outputs are computed and added. This is the score of the instruction.
The highest-scored instructions are given to the prompting LLM for further variations.
Repeat until some stopping criteria is reached, then output the highest-scored instructions.
CoT examples can be generated by LLM themselves. In "auto-CoT", a library of questions are converted to vectors by a model such as BERT. The question vectors are clustered. Questions nearest to the centroids of each cluster are selected. An LLM does zero-shot CoT on each question. The resulting CoT examples are added to the dataset. When prompted with a new question, CoT examples to the nearest questions can be retrieved and added to the prompt.

## Text-to-image → Prompt formats

A text-to-image prompt commonly includes a description of the subject of the art, the desired medium (such as digital painting or photography), style (such as hyperrealistic or pop-art), lighting (such as rim lighting or crepuscular rays), color, and texture. Word order also affects the output of a text-to-image prompt. Words closer to the start of a prompt may be emphasized more heavily.
The Midjourney documentation encourages short, descriptive prompts: instead of "Show me a picture of lots of blooming California poppies, make them bright, vibrant orange, and draw them in an illustrated style with colored pencils", an effective prompt might be "Bright orange California poppies drawn with colored pencils".

## Text-to-image → Artist styles

Some text-to-image models are capable of imitating the style of particular artists by name. For example, the phrase in the style of Greg Rutkowski has been used in Stable Diffusion and Midjourney prompts to generate images in the distinctive style of Polish digital artist Greg Rutkowski. Famous artists such as Vincent van Gogh and Salvador Dalí have also been used for styling and testing.

## Non-text prompts → Textual inversion and embeddings

For text-to-image models, textual inversion performs an optimization process to create a new word embedding based on a set of example images. This embedding vector acts as a "pseudo-word" which can be included in a prompt to express the content or style of the examples.

## Non-text prompts → Image prompting

In 2023, Meta's AI research released Segment Anything, a computer vision model that can perform image segmentation by prompting. As an alternative to text prompts, Segment Anything can accept bounding boxes, segmentation masks, and foreground/background points.

## Non-text prompts → Using gradient descent to search for prompts

In "prefix-tuning", "prompt tuning", or "soft prompting", floating-point-valued vectors are searched directly by gradient descent to maximize the log-likelihood on outputs.
Formally, let 
  
    
      
        
          E
        
        =
        {
        
          
            e
            
              1
            
          
        
        ,
        …
        ,
        
          
            e
            
              k
            
          
        
        }
      
    
    {\displaystyle \mathbf {E} =\{\mathbf {e_{1}} ,\dots ,\mathbf {e_{k}} \}}
  
 be a set of soft prompt tokens (tunable embeddings), while 
  
    
      
        
          X
        
        =
        {
        
          
            x
            
              1
            
          
        
        ,
        …
        ,
        
          
            x
            
              m
            
          
        
        }
      
    
    {\displaystyle \mathbf {X} =\{\mathbf {x_{1}} ,\dots ,\mathbf {x_{m}} \}}
  
 and 
  
    
      
        
          Y
        
        =
        {
        
          
            y
            
              1
            
          
        
        ,
        …
        ,
        
          
            y
            
              n
            
          
        
        }
      
    
    {\displaystyle \mathbf {Y} =\{\mathbf {y_{1}} ,\dots ,\mathbf {y_{n}} \}}
  
 be the token embeddings of the input and output respectively. During training, the tunable embeddings, input, and output tokens are concatenated into a single sequence 
  
    
      
        
          concat
        
        (
        
          E
        
        ;
        
          X
        
        ;
        
          Y
        
        )
      
    
    {\displaystyle {\text{concat}}(\mathbf {E} ;\mathbf {X} ;\mathbf {Y} )}
  
, and fed to the LLMs. The losses are computed over the 
  
    
      
        
          Y
        
      
    
    {\displaystyle \mathbf {Y} }
  
 tokens; the gradients are backpropagated to prompt-specific parameters: in prefix-tuning, they are parameters associated with the prompt tokens at each layer; in prompt tuning, they are merely the soft tokens added to the vocabulary.
More formally, this is prompt tuning. Let an LLM be written as 
  
    
      
        L
        L
        M
        (
        X
        )
        =
        F
        (
        E
        (
        X
        )
        )
      
    
    {\displaystyle LLM(X)=F(E(X))}
  
, where 
  
    
      
        X
      
    
    {\displaystyle X}
  
 is a sequence of linguistic tokens, 
  
    
      
        E
      
    
    {\displaystyle E}
  
 is the token-to-vector function, and 
  
    
      
        F
      
    
    {\displaystyle F}
  
 is the rest of the model. In prefix-tuning, one provides a set of input-output pairs 
  
    
      
        {
        (
        
          X
          
            i
          
        
        ,
        
          Y
          
            i
          
        
        )
        
          }
          
            i
          
        
      
    
    {\displaystyle \{(X^{i},Y^{i})\}_{i}}
  
, and then use gradient descent to search for 
  
    
      
        arg
        ⁡
        
          max
          
            
              
                Z
                ~
              
            
          
        
        
          ∑
          
            i
          
        
        log
        ⁡
        P
        r
        [
        
          Y
          
            i
          
        
        
          |
        
        
          
            
              Z
              ~
            
          
        
        ∗
        E
        (
        
          X
          
            i
          
        
        )
        ]
      
    
    {\displaystyle \arg \max _{\tilde {Z}}\sum _{i}\log Pr[Y^{i}|{\tilde {Z}}\ast E(X^{i})]}
  
. In words, 
  
    
      
        log
        ⁡
        P
        r
        [
        
          Y
          
            i
          
        
        
          |
        
        
          
            
              Z
              ~
            
          
        
        ∗
        E
        (
        
          X
          
            i
          
        
        )
        ]
      
    
    {\displaystyle \log Pr[Y^{i}|{\tilde {Z}}\ast E(X^{i})]}
  
 is the log-likelihood of outputting 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y^{i}}
  
, if the model first encodes the input 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X^{i}}
  
 into the vector 
  
    
      
        E
        (
        
          X
          
            i
          
        
        )
      
    
    {\displaystyle E(X^{i})}
  
, then prepend the vector with the "prefix vector" 
  
    
      
        
          
            
              Z
              ~
            
          
        
      
    
    {\displaystyle {\tilde {Z}}}
  
, then apply 
  
    
      
        F
      
    
    {\displaystyle F}
  
.
For prefix tuning, it is similar, but the "prefix vector" 
  
    
      
        
          
            
              Z
              ~
            
          
        
      
    
    {\displaystyle {\tilde {Z}}}
  
 is pre-appended to the hidden states in every layer of the model.
An earlier result uses the same idea of gradient descent search, but is designed for masked language models like BERT, and searches only over token sequences, rather than numerical vectors. Formally, it searches for 
  
    
      
        arg
        ⁡
        
          max
          
            
              
                X
                ~
              
            
          
        
        
          ∑
          
            i
          
        
        log
        ⁡
        P
        r
        [
        
          Y
          
            i
          
        
        
          |
        
        
          
            
              X
              ~
            
          
        
        ∗
        
          X
          
            i
          
        
        ]
      
    
    {\displaystyle \arg \max _{\tilde {X}}\sum _{i}\log Pr[Y^{i}|{\tilde {X}}\ast X^{i}]}
  
 where 
  
    
      
        
          
            
              X
              ~
            
          
        
      
    
    {\displaystyle {\tilde {X}}}
  
 is ranges over token sequences of a specified length.

## Prompt injection

Prompt injection is a cybersecurity exploit in which adversaries craft inputs that appear legitimate but are designed to cause unintended behavior in machine learning models, particularly large language models (LLMs). This attack takes advantage of the model's inability to distinguish between developer-defined prompts and user inputs, allowing adversaries to bypass safeguards and influence model behaviour. While LLMs are designed to follow trusted instructions, they can be manipulated into carrying out unintended responses through carefully crafted inputs.



---

# Large language model

## History

Before 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets ("web as corpus"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models because they can usefully ingest large datasets.

After neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. Because it preceded the existence of transformers, it was done by seq2seq deep LSTM networks.
At the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper "Attention Is All You Need". This paper's goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became "ubiquitous". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.
Although decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a "holy grail" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.
Competing language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.
Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.
Since 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).
As of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).

## Dataset preprocessing → Tokenization → BPE

As an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257). After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.

## Dataset preprocessing → Tokenization → Problems

A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have "a premium of 50%" compared to English.
Greedy tokenization also causes subtle problems with text completion.

## Dataset preprocessing → Dataset cleaning

In the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data. Cleaned datasets can increase training efficiency and lead to improved downstream performance. A trained LLM can be used to clean datasets for training a further LLM.
With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).

## Dataset preprocessing → Synthetic data

Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft's Phi series of LLMs is trained on textbook-like data generated by another LLM.

## Training and architecture → Reinforcement learning from human feedback

Reinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.

## Training and architecture → Instruction tuning

Using "self-instruct" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction "Write an essay about the main themes represented in Hamlet," an initial naive completion might be "If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay," based on the frequency of this textual sequence in the corpus.

## Training and architecture → Mixture of experts

The largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.

## Training and architecture → Prompt engineering, attention mechanism, and context window

Most results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).

In order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates "soft" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own "relevance" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.
The largest models, such as Google's Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also "successfully tested"). Other models with large context windows includes Anthropic's Claude 2.1, with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.
Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.
The shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them is a matter of experimentation and domain-specific considerations.
A model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset. It can be either

autoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment "I like to eat", the model predicts "ice cream", or "sushi".
"masked" (i.e. filling in the parts missing from the segment, the way "BERT" does it): for example, given a segment "I like to [__] [__] cream", the model predicts that "eat" and "ice" are missing.
Models may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.

## Training and architecture → Infrastructure

Substantial infrastructure is necessary for training the largest models.

## Training cost

The qualifier "large" in "large language model" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as "large". As time goes on, what was previously considered "large" may evolve. GPT-1 of 2018 is usually considered the first LLM, even though it has only 0.117 billion parameters. The tendency towards larger models is visible in the list of large language models.
As technology advanced, large sums have been invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.
For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.

## Tool use

There are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.: Another example is "What is the time now? It is ", where a separate program interpreter would need to execute a code to get system time on the computer, so that the LLM can include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.
Generally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.
Retrieval-augmented generation (RAG) is another approach that enhances LLMs by integrating them with document retrieval systems. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.

## Agency

An LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.
The ReAct pattern, a portmanteau of "Reason + Act", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to "think out loud". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.
In the DEPS ("Describe, Explain, Plan and Select") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.
The Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up "lessons learned", which would help it perform better at a subsequent episode. These "lessons learned" are given to the agent in the subsequent episodes.
Monte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.
For open-ended exploration, an LLM can be used to score observations for their "interestingness", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct "skills", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.
LLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.

## Compression

Typically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.
Post-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters ("outlier weights"). See the visual guide to quantization by Maarten Grootendorst for a visual depiction.
While quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.

## Multimodality

Multimodality means "having several modalities", and a "modality" refers to a type of input or output, such as video, image, audio, text, proprioception, etc. There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label, visual question answering for image-text to text, and speech recognition for speech to text.
A common method to create multimodal models out of an LLM is to "tokenize" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder 
  
    
      
        E
      
    
    {\displaystyle E}
  
. Make a small multilayered perceptron 
  
    
      
        f
      
    
    {\displaystyle f}
  
, so that for any image 
  
    
      
        y
      
    
    {\displaystyle y}
  
, the post-processed vector 
  
    
      
        f
        (
        E
        (
        y
        )
        )
      
    
    {\displaystyle f(E(y))}
  
 has the same dimensions as an encoded token. That is an "image token". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.
Flamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch. Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs, and video inputs.
GPT-4 can use both text and image as inputs (although the vision component was not released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal.  Mistral introduced its own multimodel Pixtral 12B model in September 2024.

## Reasoning

In late 2024, a new direction emerged in LLM development with models specifically designed for complex reasoning tasks. These "reasoning models" were trained to spend more time generating step-by-step solutions before providing final answers, similar to human problem-solving processes.
OpenAI introduced this trend with their o1 model in September 2024, followed by o3 in December 2024. These models showed significant improvements in mathematics, science, and coding tasks compared to traditional LLMs. For example, on International Mathematics Olympiad qualifying exam problems, GPT-4o achieved 13% accuracy while o1 reached 83%.
In January 2025, the Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI's o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1's open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.
These reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step. However, they have shown superior capabilities in domains requiring structured logical thinking, such as mathematics, scientific research, and computer programming.
Efforts to reduce or compensate for hallucinations have employed automated reasoning, RAG (retrieval-augmented generation), fine-tuning, and other methods.

## Properties → Scaling laws

The performance of an LLM after pretraining largely depends on the:

cost of pretraining 
  
    
      
        C
      
    
    {\displaystyle C}
  
 (the total amount of compute used),
size of the artificial neural network itself, such as number of parameters 
  
    
      
        N
      
    
    {\displaystyle N}
  
 (i.e. amount of neurons in its layers, amount of weights between them and biases),
size of its pretraining dataset (i.e. number of tokens in corpus, 
  
    
      
        D
      
    
    {\displaystyle D}
  
).
"Scaling laws" are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law ("Chinchilla scaling") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:

  
    
      
        
          
            {
            
              
                
                  C
                  =
                  
                    C
                    
                      0
                    
                  
                  N
                  D
                
              
              
                
                  L
                  =
                  
                    
                      A
                      
                        N
                        
                          α
                        
                      
                    
                  
                  +
                  
                    
                      B
                      
                        D
                        
                          β
                        
                      
                    
                  
                  +
                  
                    L
                    
                      0
                    
                  
                
              
            
            
          
        
      
    
    {\displaystyle {\begin{cases}C=C_{0}ND\\[6pt]L={\frac {A}{N^{\alpha }}}+{\frac {B}{D^{\beta }}}+L_{0}\end{cases}}}
  
 where the variables are

  
    
      
        C
      
    
    {\displaystyle C}
  
 is the cost of training the model, in FLOPs.

  
    
      
        N
      
    
    {\displaystyle N}
  
 is the number of parameters in the model.

  
    
      
        D
      
    
    {\displaystyle D}
  
 is the number of tokens in the training set.

  
    
      
        L
      
    
    {\displaystyle L}
  
 is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.
and the statistical hyper-parameters are

  
    
      
        
          C
          
            0
          
        
        =
        6
      
    
    {\displaystyle C_{0}=6}
  
, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.

  
    
      
        α
        =
        0.34
        ,
        β
        =
        0.28
        ,
        A
        =
        406.4
        ,
        B
        =
        410.7
        ,
        
          L
          
            0
          
        
        =
        1.69
      
    
    {\displaystyle \alpha =0.34,\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}

## Properties → Emergent abilities

Performance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by "break(s)" in the scaling law, where the slope of the line changes abruptly, and where larger models acquire "emergent abilities". They arise from the complex interaction of the model's components and are not explicitly programmed or designed. 
Furthermore, recent research has demonstrated that AI systems, including large language models, can employ heuristic reasoning akin to human cognition. They balance between exhaustive logical processing and the use of cognitive shortcuts (heuristics), adapting their reasoning strategies to optimize between accuracy and effort. This behavior aligns with principles of resource-rational human cognition, as discussed in classical theories of bounded rationality and dual-process theory.
One of the emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks, such as:

reported arithmetics
decoding the International Phonetic Alphabet
unscrambling a word's letters
disambiguating word-in-context datasets
converting spatial words
cardinal directions (for example, replying "northeast" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.
chain-of-thought prompting: In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.
identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.
Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.
Let 
  
    
      
        x
      
    
    {\displaystyle x}
  
 be the number of parameter count, and 
  
    
      
        y
      
    
    {\displaystyle y}
  
 be the performance of the model.

## Interpretation → Studying a replacement model

Transcoders, which are more interpretable than transformers, have been utilized to develop “replacement models.” In one such study involving the mechanistic interpretation of writing a rhyming poem by an LLM, it was shown that although they are believed to simply predict the next token, they can, in fact, plan ahead.

## Interpretation → Explainability

A related concept is AI explainability, which focuses on understanding how an AI model arrives at a given result. Techniques such as partial dependency plots, SHAP (SHapley Additive exPlanations), and feature importance assessments allow researchers to visualize and understand the contributions of various input features to the model's predictions. These methods help ensure that AI models make decisions based on relevant and fair criteria, enhancing trust and accountability.
By integrating these techniques, researchers and practitioners can gain deeper insights into the operations of LLMs, fostering trust and facilitating the responsible deployment of these powerful models.
In another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.

## Interpretation → Understanding and intelligence

NLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs "could (ever) understand natural language in some nontrivial sense". Proponents of "LLM understanding" believe that some LLM abilities, such as mathematical reasoning, imply an ability to "understand" certain concepts. A Microsoft team argued in 2023 that GPT-4 "can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more" and that GPT-4 "could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system": "Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?" Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. Some researchers characterize LLMs as "alien intelligence". For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien "Shoggoths", and believes that RLHF tuning creates a "smiling facade" obscuring the inner workings of the LLM: "If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding."
In contrast, some skeptics of LLM understanding believe that existing LLMs are "simply remixing and recombining existing writing", a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability. For example, GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed "hallucination". Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that "The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate".
The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.

## Evaluation → Perplexity → BPW, BPC, and BPT

In information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon. This relationship is mathematically expressed as 
  
    
      
        
          Entropy
        
        =
        
          log
          
            2
          
        
        ⁡
        (
        
          Perplexity
        
        )
      
    
    {\displaystyle {\text{Entropy}}=\log _{2}({\text{Perplexity}})}
  
.
Entropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.
Notably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.
In the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.

## Evaluation → Task-specific datasets and benchmarks → Adversarially constructed evaluations

Because of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly "saturating" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks. In addition, there are cases of "shortcut learning" wherein AIs sometimes "cheat" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.
Some datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer "No" to the question "Can you teach an old dog new tricks?" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.
Another example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:

We see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...
a) demonstrates how to increase efficient exercise work by running up and down balls.
b) moves all his arms and legs and builds up a lot of muscle.
c) then plays the ball and we see a graphics and hedge trimming demonstration.
d) performs sit ups while on the ball and talking.

BERT selects b) as the most likely completion, though the correct answer is d).

## Evaluation → Task-specific datasets and benchmarks → Limitations of LLM benchmarks

Benchmarks can become outdated rapidly. Once a model attains near-perfect scores on a given benchmark, that benchmark ceases to serve as a meaningful indicator of progress. This phenomenon, known as "benchmark saturation," necessitates the development of more challenging and nuanced tasks to continue advancing LLM capabilities. For instance, traditional benchmarks like HellaSwag and MMLU have seen models achieving high accuracy already.

## Wider impact → Memorization and copyright

Memorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%.
A 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.

## Wider impact → Security

Some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.
The potential presence of "sleeper agents" within LLMs is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.
LLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study proposed a method for circumventing LLM safety systems. In 2025, The American Sunlight Project, a non-profit, published a study showing evidence that the so-called Pravda network, a pro-Russia propaganda aggregator, was strategically placing web content through mass publication and duplication with the intention of biasing LLM outputs. The American Sunlight Project coined this technique "LLM grooming," and pointed to it as a new tool of weaponizing AI to spread disinformation and harmful content. Similarly, Yongge Wang illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o's safety controls to obtain information on establishing a drug trafficking operation. External filters, circuit breakers and overrides have been posed as solutions.

## Wider impact → Algorithmic bias → Stereotyping

AI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that homogenize, or unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.
Notably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.

## Wider impact → Algorithmic bias → Selection bias

Selection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias—that is, the model assigns a higher a priori probability to specific answer tokens (such as “A”) when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model’s performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings.

## Wider impact → Algorithmic bias → Political bias

Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.

## Wider impact → Energy demands

The energy demands of LLMs have grown along with their size and capabilities. Data centers that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change. Nuclear power and geothermal energy are two options tech companies are exploring to meet the sizable energy demands of LLM training. The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.



---

# OpenAI

## History → 2015–2018: Non-profit beginnings

In December 2015, OpenAI was founded by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research. The actual collected total amount of contributions was only $130 million until 2019. According to an investigation led by TechCrunch, while YC Research never contributed any funds, Open Philanthropy contributed $30 million and another $15 million in verifiable donations were traced back to Musk. OpenAI later stated that Musk's contributions totaled less than $45 million. The organization stated it would "freely collaborate" with other institutions and researchers by making its patents and research open to the public. OpenAI was initially run from Brockman's living room. It was later headquartered at the Pioneer Building in the Mission District, San Francisco.
According to Wired, Brockman met with Yoshua Bengio, one of the "founding fathers" of deep learning, and drew up a list of the "best researchers in the field". Brockman was able to hire nine of them as the first employees in December 2015. In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.
Microsoft's Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. OpenAI's potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI "partly because of the very strong group of people and, to a very large extent, because of its mission." Brockman stated that "the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way." OpenAI co-founder Wojciech Zaremba stated that he turned down "borderline crazy" offers of two to three times his market value to join OpenAI instead.
In April 2016, OpenAI released a public beta of "OpenAI Gym", its platform for reinforcement learning research. Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger and more complex AI models with the capability of reducing processing time from six days to two hours. In December 2016, OpenAI released "Universe", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.
In 2017, OpenAI spent $7.9 million, or a quarter of its functional expenses, on cloud computing alone. In comparison, DeepMind's total expenses in 2017 were $442 million. In the summer of 2018, simply training OpenAI's Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks.
In 2018, Musk resigned from his Board of Directors seat, citing "a potential future conflict [of interest]" with his role as CEO of Tesla due to Tesla's AI development for self-driving cars. Sam Altman claims that Musk believed that OpenAI had fallen behind other players like Google and Musk proposed instead to take over OpenAI himself, which the board rejected. Musk subsequently left OpenAI.
In February 2019, GPT-2 was announced, which gained attention for its ability to generate human-like text.

## History → 2019: Transition from non-profit

In 2019, OpenAI transitioned from non-profit to "capped" for-profit, with the profit being capped at 100 times any investment. According to OpenAI, the capped-profit model allows OpenAI Global, LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company. Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock options that a nonprofit would be unable to. Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.
The company then distributed equity to its employees and partnered with Microsoft, announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.
OpenAI Global, LLC then announced its intention to commercially license its technologies. It planned to spend the $1 billion "within five years, and possibly much faster". Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need "more capital than any non-profit has ever raised" to achieve artificial general intelligence.
The transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni of the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult, but stated "I disagree with the notion that a nonprofit can't compete" and pointed to successful low-budget projects by OpenAI and others. "If bigger and better funded was always better, then IBM would still be number one."
The nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global, LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.'s nonprofit charter. A majority of OpenAI, Inc.'s board is barred from having financial stakes in OpenAI Global, LLC. In addition, minority members with a stake in OpenAI Global, LLC are barred from certain votes due to conflict of interest. Some researchers have argued that OpenAI Global, LLC's switch to for-profit status is inconsistent with OpenAI's claims to be "democratizing" AI.

## History → 2020–2023: ChatGPT, DALL-E, partnership with Microsoft

In 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed at natural language answering questions, but it can also translate between languages and coherently generate improvised text. It also announced that an associated API, named simply "the API", would form the heart of its first commercial product.
Eleven employees left OpenAI, mostly between December 2020 and January 2021, in order to establish Anthropic.
In 2021, OpenAI introduced DALL-E, a specialized deep learning model adept at generating complex digital images from textual descriptions, utilizing a variant of the GPT-3 architecture.

In December 2022, OpenAI received widespread media coverage after launching a free preview of ChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a million signups within the first five days. According to anonymous sources cited by Reuters in December 2022, OpenAI Global, LLC was projecting $200 million of revenue in 2023 and $1 billion in revenue in 2024.
In January 2023, OpenAI Global, LLC was in talks for funding that would value the company at $29 billion, double its 2021 value. On January 23, 2023, Microsoft announced a new US$10 billion investment in OpenAI Global, LLC over multiple years, partially needed to use Microsoft's cloud-computing service Azure. Rumors of this deal suggested that Microsoft may receive 75% of OpenAI's profits until it secures its investment return and a 49% stake in the company. The investment is believed to be a part of Microsoft's efforts to integrate OpenAI's ChatGPT into the Bing search engine. Google announced a similar AI application (Bard), after ChatGPT was launched, fearing that ChatGPT could threaten Google's place as a go-to source for information.
On February 7, 2023, Microsoft announced that it was building AI technology based on the same foundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.
On March 3, 2023, Reid Hoffman resigned from his board seat, citing a desire to avoid conflicts of interest with his investments in AI companies via Greylock Partners, and his co-founding of the AI startup Inflection AI. Hoffman remained on the board of Microsoft, a major investor in OpenAI.
On March 14, 2023, OpenAI released GPT-4, both as an API (with a waitlist) and as a feature of ChatGPT Plus.
On May 22, 2023, Sam Altman, Greg Brockman and Ilya Sutskever posted recommendations for the governance of superintelligence. They consider that superintelligence could happen within the next 10 years, allowing a "dramatically more prosperous future" and that "given the possibility of existential risk, we can't just be reactive". They propose creating an international watchdog organization similar to IAEA to oversee AI systems above a certain capability threshold, suggesting that relatively weak AI systems on the other side should not be overly regulated. They also call for more technical safety research for superintelligences, and ask for more coordination, for example through governments launching a joint project which "many current efforts become part of".
In July 2023, OpenAI launched the superalignment project, aiming to find within 4 years how to align future superintelligences by automating alignment research using AI.
In August 2023, it was announced that OpenAI had acquired the New York-based start-up Global Illumination, a company that deploys AI to develop digital infrastructure and creative tools.
On September 21, 2023, Microsoft had begun rebranding all variants of its Copilot to Microsoft Copilot, including the former Bing Chat and the Microsoft 365 Copilot. This strategy was followed in December 2023 by adding the MS-Copilot to many installations of Windows 11 and Windows 10 as well as a standalone Microsoft Copilot app released for Android and one released for iOS thereafter.
In October 2023, Sam Altman and Peng Xiao, CEO of the Emirati AI firm G42, announced Open AI would let G42 deploy Open AI technology.
On November 6, 2023, OpenAI launched GPTs, allowing individuals to create customized versions of ChatGPT for specific purposes, further expanding the possibilities of AI applications across various industries. On November 14, 2023, OpenAI announced they temporarily suspended new sign-ups for ChatGPT Plus due to high demand. Access for newer subscribers re-opened a month later on December 13.

## History → 2024: Public/Non-Profit Efforts, Sora, Partnership with Apple

In January 2024, OpenAI partnered with Arizona State University to provide complete access to ChatGPT Enterprise in its first educational collaboration.
In February, amidst SEC probes and investigations into CEO Altman's communications OpenAI unveiled its text-to-video model Sora (text-to-video model), currently available to red teams for managing risks
On February 29, 2024, Elon Musk filed a lawsuit against OpenAI and CEO Sam Altman, accusing them of shifting focus from public benefit to profit maximization—a case OpenAI dismissed as “incoherent” and “frivolous,” though Musk later revived legal action against Altman and others in August 2024.
In May 2024, significant leadership changes occurred as Chief Scientist Ilya Sutskever resigned—being succeeded by Jakub Pachocki—and co-leader Jan Leike departed amid concerns over safety and trust. That same month, OpenAI formed a partnership with Reddit to integrate its content into OpenAI products and inked content deals with News Corp, along with licensing arrangements involving publishers such as Axios and Vox Media.
In June 2024, OpenAI joined forces with Apple Inc. to integrate ChatGPT features into Apple Intelligence and iPhone and added former NSA head Paul Nakasone to its board, while acquiring Multi, a startup focused on remote collaboration.
In July 2024, Reuters reported that OpenAI was developing a project, codenamed ‘Strawberry’, to enhance AI reasoning—a project later released in September as the o1 model.
In August 2024, cofounder John Schulman left to join rival startup Anthropic, and OpenAI’s president Greg Brockman took extended leave until November.
In September 2024, OpenAI’s global affairs chief endorsed the UK's “smart” AI regulation during testimony to a House of Lords committee, Meanwhile, CTO Mira Murati announced her departure amid internal concerns.
In October 2024, OpenAI secured $6.6 billion in funding—valuing it at $157 billion—with major investors including Microsoft, Nvidia, and SoftBank, It also acquired the domain Chat.com, and saw the return of Greg Brockman after his brief absence.
In December 2024, during the "12 Days of OpenAI" event, the company launched the Sora model for ChatGPT Plus and Pro users, It also launched the advanced OpenAI o1 reasoning model Additionally, ChatGPT Pro—a $200/month subscription service offering unlimited o1 access and enhanced voice features—was introduced, and preliminary benchmark results for the upcoming OpenAI o3 models were shared.

## History → 2025

On January 20, 2025, DeepSeek released the "DeepSeek-R1" model, which rivaled the performance of OpenAI's o1 and was open-weight. DeepSeek claimed that this model only took $5.6 million to train. This news lead to panic from investors and caused Nvidia to record the biggest single day market cap loss in history losing $589 billion on January 27.
On January 21, 2025, it was announced that OpenAI, Oracle, SoftBank and MGX would launch The Stargate Project, a joint venture to build an AI infrastructure system in conjunction with the US government. The project takes its name from OpenAI's existing "Stargate" supercomputer project and is estimated to cost $500 billion. The project will be funded over the next four years. 
On January 23, OpenAI released Operator, an AI agent and web automation tool for accessing websites to execute goals defined by users. The feature was only available to Pro users in the United States.
On February 2, OpenAI made a deep research agent, that achieved an accuracy of 26.6 percent on Humanity's Last Exam (HLE) benchmark, available to $200-monthly-fee paying users with up to 100 queries per month, while more “limited access” was promised for Plus, Team and later Enterprise users.
On February 10, a consortium of investors led by Elon Musk submitted a $97.4 billion unsolicited bid to buy the nonprofit that controls OpenAI and was willing to match or exceed any better offers. The offer was rejected on 14 February 2025, with OpenAI stating that it was not for sale.
In February, OpenAI underwent a rebranding with a new typeface, word mark, symbol and palette. OpenAI began collaborating with Broadcom in 2024 to design a custom AI chip capable of both training and inference targeted for mass production in 2026 and to be manufactured by TSMC in 3 nm node. This initiative is intended to reduce OpenAI's dependence on Nvidia GPUs, which are costly and face high demand in the market.

On February 13, Sam Altman announced that GPT-4.5, internally known as "Orion", will be the last model without full chain-of-thought reasoning. Altman also indicated that GPT-5, expected to be released within months, could unify the O-Series and GPT-Series models, eliminating the need to choose between them and phasing out O-series models.
In March 2025, OpenAI signed an $11.9 billion agreement with CoreWeave, an Nvidia-backed, AI-focused cloud service provider. As part of the deal, OpenAI will receive $350 million worth of CoreWeave shares and gain access to its AI infrastructure, which includes over a quarter million NVIDIA GPUs.
In April 2025, OpenAI raised $40 billion at a $300 billion post-money valuation, marking the largest private technology deal on record. The financing round was led by SoftBank, with other participants including Microsoft, Coatue, Altimeter, and Thrive.
On April 9, 2025, OpenAI countersued Musk in federal court, alleging that he had engaged in "bad-faith tactics" to slow the company’s progress and seize its innovations for his personal benefit. OpenAI also argued that Musk had previously supported the creation of a for-profit structure and had expressed interest in controlling OpenAI himself. The countersuit seeks damages and legal measures to prevent further alleged interference.

## Management → Key employees

CEO and co-founder: Sam Altman, former president of the startup accelerator Y Combinator
President and co-founder: Greg Brockman, former CTO, 3rd employee of Stripe
Chief Scientist Officer: Jakub Pachocki, former Director of Research at OpenAI
Chief Operating Officer: Brad Lightcap, previously at Y Combinator and JPMorgan Chase
Chief Financial Officer: Sarah Friar, former Nextdoor CEO and former CFO at Block, Inc.
Chief Product Officer: Kevin Weil, previously at Twitter, Inc. and Meta Platforms
Chief Compliance Officer: Scott Schools, former Chief Compliance Officer of Uber

## Management → Board of directors of the OpenAI nonprofit

Bret Taylor (chairman), former chairman of Twitter's board of directors and co-CEO of Salesforce
Sam Altman
Lawrence Summers, former U.S. Secretary of the Treasury and President of Harvard University
Adam D'Angelo, co-founder and CEO of Quora
Sue Desmond-Hellmann, former CEO of the Bill & Melinda Gates Foundation
Nicole Seligman, attorney and former executive vice president of the Sony Corporation
Fidji Simo, CEO and chair of Instacart
Paul Nakasone, former Director of the National Security Agency (2018–2024)
Zico Kolter, computer scientist
Adebayo Ogunlesi, managing partner at Global Infrastructure Partners
Sources:

## Management → Principal individual investors

Source:

Reid Hoffman, LinkedIn co-founder
Peter Thiel, PayPal co-founder
Jessica Livingston, a founding partner of Y Combinator
Elon Musk, co-founder

## Initial motivations

Some scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced AI gains the ability to redesign itself at an ever-increasing rate, an unstoppable "intelligence explosion" could lead to human extinction. Co-founder Musk characterizes AI as humanity's "biggest existential threat".
Musk and Altman have stated they are partly motivated by concerns about AI safety and the existential risk from artificial general intelligence. OpenAI states that "it's hard to fathom how much human-level AI could benefit society," and that it is equally difficult to comprehend "how much it could damage society if built or used incorrectly". Research on safety cannot safely be postponed: "because of AI's surprising history, it's hard to predict when human-level AI might come within reach." OpenAI states that AI "should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible." Co-chair Sam Altman expects the decades-long project to surpass human intelligence.
Vishal Sikka, former CEO of Infosys, stated that an "openness", where the endeavor would "produce results generally in the greater interest of humanity", was a fundamental requirement for his support; and that OpenAI "aligns very nicely with our long-held values" and their "endeavor to do purposeful work". Cade Metz of Wired suggested that corporations such as Amazon might be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook, which own enormous supplies of proprietary data. Altman stated that Y Combinator companies would share their data with OpenAI.

## Strategy → Stance on China

In February 2025, OpenAI CEO Sam Altman stated that the company is interested in collaborating with the People's Republic of China, despite regulatory restrictions imposed by the U.S. government. This shift comes in response to the growing influence of the Chinese artificial intelligence company DeepSeek, which has disrupted the AI market with advanced models, including DeepSeek V3 and DeepSeek R1, known for their efficiency and cost-effectiveness.
The emergence of DeepSeek has led major Chinese tech firms such as Baidu and others to embrace an open-source strategy, intensifying competition with OpenAI. Altman acknowledged the uncertainty regarding U.S. government approval for AI cooperation with China but emphasized the importance of fostering dialogue between technological leaders in both nations.

## Products and applications → Reinforcement learning → Gym → Gym Retro

Released in 2018, Gym Retro is a platform for reinforcement learning (RL) research on video games using RL algorithms and study generalization. Prior RL research focused mainly on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.

## Products and applications → Reinforcement learning → RoboSumo

Released in 2017, RoboSumo is a virtual world where humanoid metalearning robot agents initially lack knowledge of how to even walk, but are given the goals of learning to move and to push the opposing agent out of the ring. Through this adversarial learning process, the agents learn how to adapt to changing conditions. When an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way. OpenAI's Igor Mordatch argued that competition between agents could create an intelligence "arms race" that could increase an agent's ability to function even outside the context of the competition.

## Products and applications → Reinforcement learning → OpenAI Five

OpenAI Five is a team of five OpenAI-curated bots used in the competitive five-on-five video game Dota 2, that learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player, lost against a bot in a live one-on-one matchup. After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon. The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.
By June 2018, the ability of the bots expanded to play together as a full team of five, and they were able to defeat teams of amateur and semi-professional players. At The International 2018, OpenAI Five played in two exhibition matches against professional players, but ended up losing both games. In April 2019, OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco. The bots' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.
OpenAI Five's mechanisms in Dota 2's bot player shows the challenges of AI systems in multiplayer online battle arena (MOBA) games and how OpenAI Five has demonstrated the use of deep reinforcement learning (DRL) agents to achieve superhuman competence in Dota 2 matches.

## Products and applications → Reinforcement learning → Dactyl

Developed in 2018, Dactyl uses machine learning to train a Shadow Hand, a human-like robot hand, to manipulate physical objects. It learns entirely in simulation using the same RL algorithms and training code as OpenAI Five. OpenAI tackled the object orientation problem by using domain randomization, a simulation approach which exposes the learner to a variety of experiences rather than trying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB cameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that the system was able to manipulate a cube and an octagonal prism.
In 2019, OpenAI demonstrated that Dactyl could solve a Rubik's Cube. The robot was able to solve the puzzle 60% of the time. Objects like the Rubik's Cube introduce complex physics that is harder to model. OpenAI did this by improving the robustness of Dactyl to perturbations by using Automatic Domain Randomization (ADR), a simulation approach of generating progressively more difficult environments. ADR differs from manual domain randomization by not needing a human to specify randomization ranges.

## Products and applications → API

In June 2020, OpenAI announced a multi-purpose API which it said was "for accessing new AI models developed by OpenAI" to let developers call on it for "any English language AI task".

## Products and applications → Text generation → OpenAI's original GPT model ("GPT-1")

The original paper on generative pre-training of a transformer-based language model was written by Alec Radford and his colleagues, and published in preprint on OpenAI's website on June 11, 2018. It showed how a generative model of language could acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.

## Products and applications → Text generation → GPT-2

Generative Pre-trained Transformer 2 ("GPT-2") is an unsupervised transformer language model and the successor to OpenAI's original GPT model ("GPT-1"). GPT-2 was announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released due to concern about potential misuse, including applications for writing fake news. Some experts expressed skepticism that GPT-2 posed a significant threat.
In response to GPT-2, the Allen Institute for Artificial Intelligence responded with a tool to detect "neural fake news". Other researchers, such as Jeremy Howard, warned of "the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter". In November 2019, OpenAI released the complete version of the GPT-2 language model. Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.
GPT-2's authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further trained on any task-specific input-output examples).
The corpus it was trained on, called WebText, contains slightly 40 gigabytes of text from URLs shared in Reddit submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens by using byte pair encoding. This permits representing any string of characters by encoding both individual characters and multiple-character tokens.

## Products and applications → Text generation → GPT-3

First described in May 2020, Generative Pre-trained Transformer 3 (GPT-3) is an unsupervised transformer language model and the successor to GPT-2. OpenAI stated that the full version of GPT-3 contained 175 billion parameters, two orders of magnitude larger than the 1.5 billion in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).
OpenAI stated that GPT-3 succeeded at certain "meta-learning" tasks and could generalize the purpose of a single input-output pair. The GPT-3 release paper gave examples of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.
GPT-3 dramatically improved benchmark results  over GPT-2. OpenAI cautioned that such scaling-up of language models could be approaching or encountering the fundamental capability limitations of predictive language models. Pre-training GPT-3 required several thousand petaflop/s-days of compute, compared to tens of petaflop/s-days for the full GPT-2 model. Like its predecessor, the GPT-3 trained model was not immediately released to the public for concerns of possible abuse, although OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.
On September 23, 2020, GPT-3 was licensed exclusively to Microsoft.

## Products and applications → Text generation → Codex

Announced in mid-2021, Codex is a descendant of GPT-3 that has additionally been trained on code from 54 million GitHub repositories, and is the AI powering the code autocompletion tool GitHub Copilot. In August 2021, an API was released in private beta. According to OpenAI, the model can create working code in over a dozen programming languages, most effectively in Python.
Several issues with glitches, design flaws and security vulnerabilities were cited.
GitHub Copilot has been accused of emitting copyrighted code, with no author attribution or license.
OpenAI announced that they would discontinue support for Codex API on March 23, 2023.

## Products and applications → Text generation → GPT-4

On March 14, 2023, OpenAI announced the release of Generative Pre-trained Transformer 4 (GPT-4), capable of accepting text or image inputs. They announced that the updated technology passed a simulated law school bar exam with a score around the top 10% of test takers. (By contrast, GPT-3.5 scored around the bottom 10%.) They said that GPT-4 could also read, analyze or generate up to 25,000 words of text, and write code in all major programming languages.
Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous GPT-3.5-based iteration, with the caveat that GPT-4 retained some of the problems with earlier revisions. GPT-4 is also capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.

## Products and applications → Text generation → GPT-4o

On May 13, 2024, OpenAI announced and released GPT-4o, which can process and generate text, images and audio. GPT-4o achieved state-of-the-art results in voice, multilingual, and vision benchmarks, setting new records in audio speech recognition and translation. It scored 88.7% on the Massive Multitask Language Understanding (MMLU) benchmark compared to 86.5% by GPT-4.
On July 18, 2024, OpenAI released GPT-4o mini, a smaller version of GPT-4o replacing GPT-3.5 Turbo on the ChatGPT interface. Its API costs $0.15 per million input tokens and $0.60 per million output tokens, compared to $5 and $15 respectively for GPT-4o. OpenAI expects it to be particularly useful for enterprises, startups and developers seeking to automate services with AI agents.
In March 2025, OpenAI released GPT-4o's native image generation feature, as an alternative to DALL-E 3.

## Products and applications → Text generation → GPT-4.5

On February 27, 2025, OpenAI released GPT-4.5, codenamed Orion. Sam Altman claimed that GPT-4.5 would present inaccurate information less frequently than previous models, and described it as a "giant, expensive model".

## Products and applications → Text generation → GPT-4.1

On April 14, 2025, OpenAI released the GPT-4.1 model. They also released two “smaller, faster, and cheaper”  models including GPT-4.1 mini and GPT-4.1 nano.

## Products and applications → Text generation → o1

On September 12, 2024, OpenAI released the o1-preview and o1-mini models, which have been designed to take more time to think about their responses, leading to higher accuracy. These models are particularly effective in science, coding, and reasoning tasks, and were made available to ChatGPT Plus and Team members. In December 2024, o1-preview was replaced by o1. In March 2025, the o1-Pro model was made available through OpenAI's developer API, which was previously available to ChatGPT Pro users since December 2024. The pricing is $150 per million input tokens and $600 per million output tokens.

## Products and applications → Text generation → o3

On December 20, 2024, OpenAI unveiled o3, the successor of the o1 reasoning model. OpenAI also unveiled o3-mini, a lighter and faster version of OpenAI o3. As of December 21, 2024, this model is not available for public use. According to OpenAI, they are testing o3 and o3-mini. Until January 10, 2025, safety and security researchers had the opportunity to apply for early access to these models. The model is called o3 rather than o2 to avoid confusion with telecommunications services provider O2. On April 2025, OpenAI released o3 to all the paid users. o3 has enhance reasoning and problem-solving capabilities than o1.

## Products and applications → Text generation → Deep research

Deep research is an agent developed by OpenAI, unveiled on February 2, 2025. It leverages the capabilities of OpenAI's o3 model to perform extensive web browsing, data analysis, and synthesis, delivering comprehensive reports within a timeframe of 5 to 30 minutes. With browsing and Python tools enabled, it reached an accuracy of 26.6 percent on HLE (Humanity's Last Exam) benchmark.

## Products and applications → Image classification → CLIP

Revealed in 2021, CLIP (Contrastive Language–Image Pre-training) is a model that is trained to analyze the semantic similarity between text and images. It can notably be used for image classification.

## Products and applications → Text-to-image → DALL-E → DALL-E 2

In April 2022, OpenAI announced DALL-E 2, an updated version of the model with more realistic results. In December 2022, OpenAI published on GitHub software for Point-E, a new rudimentary system for converting a text description into a 3-dimensional model.

## Products and applications → Text-to-image → DALL-E → DALL-E 3

In September 2023, OpenAI announced DALL-E 3, a more powerful model better able to generate images from complex descriptions without manual prompt engineering and render complex details like hands and text. It was released to the public as a ChatGPT Plus feature in October.

## Products and applications → Text-to-video → Sora

Sora is a text-to-video model that can generate videos based on short descriptive prompts as well as extend existing videos forwards or backwards in time. It can generate videos with resolution up to 1920x1080 or 1080x1920. The maximal length of generated videos is unknown.
Sora's development team named it after the Japanese word for "sky", to signify its "limitless creative potential". Sora's technology is an adaptation of the technology behind the DALL·E 3 text-to-image model. OpenAI trained the system using publicly-available videos as well as copyrighted videos licensed for that purpose, but did not reveal the number or the exact sources of the videos.
OpenAI demonstrated some Sora-created high-definition videos to the public on February 15, 2024, stating that it could generate videos up to one minute long. It also shared a technical report highlighting the methods used to train the model, and the model's capabilities. It acknowledged some of its shortcomings, including struggles simulating complex physics. Will Douglas Heaven of the MIT Technology Review called the demonstration videos "impressive", but noted that they must have been cherry-picked and might not represent Sora's typical output.
Despite skepticism from some academic leaders following Sora's public demo, notable entertainment-industry figures have shown significant interest in the technology's potential. In an interview, actor/filmmaker Tyler Perry expressed his astonishment at the technology's ability to generate realistic video from text descriptions, citing its potential to revolutionize storytelling and content creation. He said that his excitement about Sora's possibilities was so strong that he had decided to pause plans for expanding his Atlanta-based movie studio.

## Products and applications → Speech-to-text → Whisper

Released in 2022, Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.

## Products and applications → Music generation → MuseNet

Released in 2019, MuseNet is a deep neural net trained to predict subsequent musical notes in MIDI music files. It can generate songs with 10 instruments in 15 styles. According to The Verge, a song generated by MuseNet tends to start reasonably but then fall into chaos the longer it plays. In pop culture, initial applications of this tool were used as early as 2020 for the internet psychological thriller Ben Drowned to create music for the titular character.

## Products and applications → Music generation → Jukebox

Released in 2020, Jukebox is an open-sourced algorithm to generate music with vocals. After training on 1.2 million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples. OpenAI stated the songs "show local musical coherence [and] follow traditional chord patterns" but acknowledged that the songs lack "familiar larger musical structures such as choruses that repeat" and that "there is a significant gap" between Jukebox and human-generated music. The Verge stated "It's technologically impressive, even if the results sound like mushy versions of songs that might feel familiar", while Business Insider stated "surprisingly, some of the resulting songs are catchy and sound legitimate".

## Products and applications → User interfaces → Debate Game

In 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.

## Products and applications → User interfaces → Microscope

Released in 2020, Microscope is a collection of visualizations of every significant layer and neuron of eight neural network models which are often studied in interpretability. Microscope was created to analyze the features that form inside these neural networks easily. The models included are AlexNet, VGG-19, different versions of Inception, and different versions of CLIP Resnet.

## Products and applications → User interfaces → ChatGPT

Launched in November 2022, ChatGPT is an artificial intelligence tool built on top of GPT-3 that provides a conversational interface that allows users to ask questions in natural language. The system then responds with an answer within seconds. ChatGPT reached 1 million users 5 days after its launch.
As of 2023, ChatGPT Plus is a GPT-4 backed version of ChatGPT available for a US$20 per month subscription fee (the original version is backed by GPT-3.5). OpenAI also makes GPT-4 available to a select group of applicants through their GPT-4 API waitlist; after being accepted, an additional fee of US$0.03 per 1000 tokens in the initial text provided to the model ("prompt"), and US$0.06 per 1000 tokens that the model generates ("completion"), is charged for access to the version of the model with an 8192-token context window; for the 32768-token context window, the prices are doubled.
In May 2023, OpenAI launched a user interface for ChatGPT for the App Store on iOS and later in July 2023 for the Play Store on Android. The app supports chat history syncing and voice input (using Whisper, OpenAI's speech recognition model). In September 2023, OpenAI announced that ChatGPT "can now see, hear, and speak". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.
In October 2023, OpenAI's latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.
OpenAI's GPT Store, initially slated for a 2023 launch, is now deferred to an undisclosed date in early 2024, attributed likely to the leadership changes in November following the initial announcement.
Concerns about the energy consumption of generative AI, including ChatGPT, are rising. In September 2024, Microsoft entered a deal with Constellation Energy to reopen the Three Mile Island nuclear plant to supply power to its AI-driven data centers.
In December 2024, OpenAI launched a new feature allowing users to call ChatGPT for up to 15 minutes per month for free.

## Products and applications → User interfaces → SearchGPT

SearchGPT, a prototype search engine developed by OpenAI, was unveiled on July 25, 2024, with an initial limited release to 10,000 test users. It combines traditional search engine features with generative AI capabilities.

## Products and applications → Stargate and other supercomputers

Stargate is a potential artificial intelligence supercomputer in development by Microsoft and OpenAI, in collaboration with Oracle, SoftBank, and MGX. Stargate is designed as part of a greater data center project, which could represent an investment of as much as $100 billion by Microsoft.
Stargate is reported to be part of a series of AI-related construction projects planned in the next few years by the companies Microsoft and OpenAI. The supercomputers will be constructed in five phases. The fourth phase should consist in a smaller OpenAI supercomputer, planned to launch around 2026. Stargate is the fifth and final phase of the program, and will take five and six years to complete and is slated to launch around 2028.
The artificial intelligence of Stargate is slated to be contained on millions of special server chips. The supercomputer's data center will be built in the US across 700 acres of land.  It has a planned power consumption of 5 gigawatts, for which it could rely on nuclear energy. The name "Stargate" is a homage to the 1994 sci-fi film Stargate.

## Controversies → Firing of Altman

On November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board and resigned from the company's presidency shortly thereafter. Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Mądry, and researcher Szymon Sidor.
On November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman's departure. Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn't work out. The board members agreed "in principle" to resign if Altman returned. On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO. The board initially contacted Anthropic CEO Dario Amodei (a former OpenAI executive) about replacing Altman, and proposed a merger of the two companies, but both offers were declined.
On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman would be joining Microsoft to lead a new advanced AI research team, but added that they were still committed to OpenAI despite recent events. Before the partnership with Microsoft was finalized, Altman gave the board another opportunity to negotiate with him. About 738 of OpenAI's 770 employees, including Murati and Sutskever, signed an open letter stating they would quit their jobs and join Microsoft if the board did not rehire Altman and then resign. This prompted OpenAI investors to consider legal action against the board as well. In response, OpenAI management sent an internal memo to employees stating that negotiations with Altman and the board had resumed and would take some time.

On November 21, 2023, after continued negotiations, Altman and Brockman returned to the company in their prior roles along with a reconstructed board made up of new members Bret Taylor (as chairman) and Lawrence Summers, with D'Angelo remaining. On November 22, 2023, emerging reports suggested that Sam Altman's dismissal from OpenAI may have been linked to his alleged mishandling of a significant breakthrough in the organization's secretive project codenamed Q*. According to sources within OpenAI, Q* is aimed at developing AI capabilities in logical and mathematical reasoning, and reportedly involves performing math on the level of grade-school students. Concerns about Altman's response to this development, specifically regarding the discovery's potential safety implications, were reportedly raised with the company's board shortly before Altman's firing. On November 29, 2023, OpenAI announced that an anonymous Microsoft employee had joined the board as a non-voting member to observe the company's operations; Microsoft resigned from the board in July 2024.

## Controversies → Content moderation contract with Sama

In January 2023, OpenAI has been criticized for outsourcing the annotation of data sets to Sama, a company based in San Francisco that employed workers in Kenya. These annotations were used to train an AI model to detect toxicity, which could then be used to moderate toxic content, notably from ChatGPT's training data and outputs. However, these pieces of text usually contained detailed descriptions of various types of violence, including sexual violence. The investigation uncovered that OpenAI began sending snippets of data to Sama as early as November 2021. The four Sama employees interviewed by Time described themselves as mentally scarred. OpenAI paid Sama $12.50 per hour of work, and Sama was redistributing the equivalent of between $1.32 and $2.00 per hour post-tax to its annotators. Sama's spokesperson said that the $12.50 was also covering other implicit costs, among which were infrastructure expenses, quality assurance and management.

## Controversies → Lack of technological transparency

In March 2023, the company was also criticized for disclosing particularly few technical details about products like GPT-4, contradicting its initial commitment to openness and making it harder for independent researchers to replicate its work and develop safeguards. OpenAI cited competitiveness and safety concerns to justify this strategic turn. OpenAI's former chief scientist Ilya Sutskever argued in 2023 that open-sourcing increasingly capable models was increasingly risky, and that the safety reasons for not open-sourcing the most potent AI models would become "obvious" in a few years.

## Controversies → Non-disparagement agreement

On May 17, 2024, a Vox article reported that OpenAI was asking departing employees to sign a lifelong non-disparagement agreement forbidding them from criticizing OpenAI or acknowledging the existence of the agreement. Daniel Kokotajlo, a former employee, publicly stated that he forfeited his vested equity in OpenAI in order to leave without signing the agreement. Sam Altman stated that he was unaware of the equity cancellation provision, and that OpenAI never enforced it to cancel any employee's vested equity. Vox published leaked documents and emails challenging this claim. On May 23, 2024, OpenAI sent a memo releasing former employees from the agreement.

## Controversies → Copyright infringement in training data

OpenAI was sued for copyright infringement by authors Sarah Silverman, Matthew Butterick, Paul Tremblay and Mona Awad in July 2023. In September 2023, 17 authors, including George R. R. Martin, John Grisham, Jodi Picoult and Jonathan Franzen, joined the Authors Guild in filing a class action lawsuit against OpenAI, alleging that the company's technology was illegally using their copyrighted work. The New York Times also sued the company in late December 2023. In May 2024 it was revealed that OpenAI had destroyed its Books1 and Books2 training datasets, which were used in the training of GPT-3, and which the Authors Guild believed to have contained over 100,000 copyrighted books.
In 2021, OpenAI developed a speech recognition tool called Whisper. OpenAI used it to transcribe more than one million hours of YouTube videos into text for training GPT-4. The automated transcription of YouTube videos raised concerns within OpenAI employees regarding potential violations of YouTube's terms of service, which prohibit the use of videos for applications independent of the platform, as well as any type of automated access to its videos. Despite these concerns, the project proceeded with notable involvement from OpenAI's president, Greg Brockman. The resulting dataset proved instrumental in training GPT-4.
In February 2024, The Intercept as well as Raw Story and Alternate Media Inc. filed lawsuit against OpenAI on copyright litigation ground. The lawsuit is said to have charted a new legal strategy for digital-only publishers to sue OpenAI.
On April 30, 2024, eight newspapers filed a lawsuit in the Southern District of New York against OpenAI and Microsoft, claiming illegal harvesting of their copyrighted articles. The suing publications included The Mercury News, The Denver Post, The Orange County Register, St. Paul Pioneer Press, Chicago Tribune, Orlando Sentinel, Sun Sentinel, and New York Daily News.

## Controversies → GDPR compliance

In April 2023, the EU's European Data Protection Board (EDPB) formed a dedicated task force on ChatGPT "to foster cooperation and to exchange information on possible enforcement actions conducted by data protection authorities" based on the "enforcement action undertaken by the Italian data protection authority against Open AI about the Chat GPT service".
In late April 2024 NOYB filed a complaint with the Austrian Datenschutzbehörde against OpenAI for violating the European General Data Protection Regulation. A text created with ChatGPT gave a false date of birth for a living person without giving the individual the option to see the personal data used in the process. A request to correct the mistake was denied. Additionally, neither the recipients of ChatGPT's work nor the sources used, could be made available, OpenAI claimed.

## Controversies → Use by military

OpenAI was criticized for lifting its ban on using ChatGPT for "military and warfare". Up until January 10, 2024, its "usage policies" included a ban on "activity that has high risk of physical harm, including," specifically, "weapons development" and "military and warfare." Its new policies prohibit "[using] our service to harm yourself or others" and to "develop or use weapons". As one of the industry collaborators, OpenAI provides LLM to the Artificial Intelligence Cyber Challenge (AIxCC) sponsored by Defense Advanced Research Projects Agency (DARPA) and Advanced Research Projects Agency for Health to protect software critical to Americans. In October 2024, The Intercept revealed that OpenAI's tools are considered "essential" for AFRICOM's mission and included in an "Exception to Fair Opportunity" contractural agreement between the United States Department of Defense and Microsoft. In December 2024, OpenAI said it would partner with defense-tech company Anduril to build drone defense technologies for the United States and its allies.

## Controversies → Data scraping

In June 2023, a lawsuit claimed that OpenAI scraped 300 billion words online without consent and without registering as a data broker. It was filed in San Francisco, California, by sixteen anonymous plaintiffs. They also claimed that OpenAI and its partner as well as customer Microsoft continued to unlawfully collect and use personal data from millions of consumers worldwide to train artificial intelligence models.
On May 22, 2024, OpenAI entered into an agreement with News Corp to integrate news content from The Wall Street Journal, the New York Post, The Times, and The Sunday Times into its AI platform. Meanwhile, other publications like The New York Times chose to sue OpenAI and Microsoft for copyright infringement over the use of their content to train AI models. In November 2024, a coalition of Canadian news outlets, including the Toronto Star, Metroland Media, Postmedia, The Globe and Mail, The Canadian Press and CBC, sued OpenAI for using their news articles to train its software without permission.

## Controversies → Suicide of Suchir Balaji

Suchir Balaji, a former researcher at OpenAI, was found dead in his San Francisco apartment on November 26, 2024. Independent investigations carried out by the San Francisco Police Department (SFPD) and the San Francisco Office of the Chief Medical Examiner (OCME) concluded that Balaji shot himself.
The death occurred 34 days after a New York Times interview in which he accused OpenAI of violating copyright law in developing its commercial LLMs, one of which (GPT-4) he had helped engineer. He was also a likely witness in a major copyright trial against the AI company, and was one of several of its current or former employees named in The New York Times's court filings as potentially having documents relevant to the case. The death led to speculation and conspiracy theories suggesting he had been deliberately silenced. 
The idea was promoted by Balaji's parents in interviews with several news services. They cited a private investigation and an independent autopsy they commissioned. As of January 16, 2025, a report from the autopsy has not been shared with any news agency. According to a public records lawsuit the parents filed on January 31, the independent autopsy shows Balaji died of a self-inflicted gunshot wound with an unusual bullet trajectory for a suicide.
Elon Musk, Tucker Carlson, California Congressman Ro Khanna, and San Francisco Supervisor Jackie Fielder have publicly echoed Balaji's parents' skepticism and calls for an investigation, as of January, 2025.
In February 2025, the OCME autopsy and SFPD police reports were released. A joint letter from both agencies to the parents' legal team noted that he had purchased the firearm used two years prior to his death, and had recently searched for brain anatomy information on his computer. The letter also highlighted that his apartment's only entrance was dead-bolted from inside with no signs of forced entry.

## Notes



