# GenTutor â€“ RAG Chatbot

**GenTutor** is a Streamlit-based Retrieval-Augmented Generation (RAG) chatbot that answers user questions by retrieving and combining context from a custom Wikipedia corpus. It uses:

- **LangChain** for RAG pipelines  
- **Hugging Face Hub** (via `HuggingFaceHub`) as the LLM  
- **sentence-transformers/all-MiniLM-L6-v2** for embeddings  
- **FAISS** for vector search  
- **Streamlit** for the UI  

---

## ğŸ“‚ Repository Layout

gen-tutor-project/ â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ raw/ # Wikipedia pages saved as CSV â”‚ â””â”€â”€ processed/ # combined_data.csv + combined_data.md â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ data_scraper.py # Fetch & process Wikipedia pages â”‚ â”œâ”€â”€ rag_chat.py # RAGChat class + CLI for building index & one-off queries â”‚ â””â”€â”€ app.py # Streamlit UI â”œâ”€â”€ vectorstore/ # FAISS index files â”œâ”€â”€ .streamlit/ â”‚ â””â”€â”€ secrets.toml # Your Hugging Face API token â”œâ”€â”€ requirements.txt â””â”€â”€ README.md # This file

yaml
Copy

---

## ğŸš€ Quickstart

### 1. Clone & create a venv

```bash
git clone <your-repo-url> gen-tutor-project
cd gen-tutor-project
python3 -m venv .venv
source .venv/bin/activate
2. Install dependencies
bash
Copy
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
macOS M1/M2 note:
If faiss-cpu fails to build, install via Homebrew:

bash
Copy
brew install faiss
then remove or loosen the faiss-cpu pin in requirements.txt.

3. Configure your Hugging Face token
Create or edit .streamlit/secrets.toml:

toml
Copy
HUGGINGFACEHUB_API_TOKEN = "hf_your_token_here"
Do not commit your token to version control.

4. Scrape & prepare the data
Edit pages.txt to list one Wikipedia page title per line, for example:

text
Copy
Massachusetts
History of Massachusetts
Massachusetts Bay Colony
Run the scraper:

bash
Copy
python src/data_scraper.py --pages pages.txt --out data
Youâ€™ll end up with:

data/raw/ â†’ one CSV per page

data/processed/combined_data.csv

data/processed/combined_data.md

5. Build the FAISS index
bash
Copy
python -m src.rag_chat --build-index data/processed/combined_data.md
This writes your vector store files into vectorstore/.

6. Launch the Streamlit app
bash
Copy
streamlit run src/app.py
Open your browser at http://localhost:8501 (or the URL printed in your terminal) and start chatting:

User:
massachusetts capital

GenTutor:
Boston

ğŸ› ï¸ Development Workflow
Rebuild index after updating data/processed/combined_data.md:

bash
Copy
python -m src.rag_chat --build-index data/processed/combined_data.md
One-off query without running Streamlit:

bash
Copy
python -m src.rag_chat --question "Who is the mayor of Boston?"
Adjust prompt in src/rag_chat.py under SYSTEM_PROMPT to customize behavior.

ğŸ³ Docker (optional)
dockerfile
Copy
FROM python:3.11-slim
WORKDIR /app
COPY . .
RUN pip install --no-cache-dir -r requirements.txt
EXPOSE 8501
CMD ["streamlit", "run", "src/app.py", "--server.address=0.0.0.0"]
Build and run:

bash
Copy
docker build -t gentutor .
docker run -p 8501:8501 -e HUGGINGFACEHUB_API_TOKEN=hf_your_token_here gentutor
ğŸ¤ Contributing
Fork the repo

Create a feature branch

Submit a PR

ğŸ“œ License
This project is licensed under the MIT License.
